{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10523408,"sourceType":"datasetVersion","datasetId":6142016}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# INSTALL REQUIREMENTS","metadata":{}},{"cell_type":"code","source":"!pip install -r /kaggle/input/requirements1.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:14:09.498442Z","iopub.execute_input":"2025-03-19T10:14:09.498695Z","iopub.status.idle":"2025-03-19T10:15:34.925158Z","shell.execute_reply.started":"2025-03-19T10:14:09.498667Z","shell.execute_reply":"2025-03-19T10:15:34.924139Z"}},"outputs":[{"name":"stdout","text":"Collecting appnope==0.1.4 (from -r /kaggle/input/requirements1.txt (line 1))\n  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\nRequirement already satisfied: asttokens==3.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 2)) (3.0.0)\nCollecting black==24.10.0 (from -r /kaggle/input/requirements1.txt (line 3))\n  Downloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: catboost==1.2.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 4)) (1.2.7)\nRequirement already satisfied: click==8.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 5)) (8.1.7)\nRequirement already satisfied: comm==0.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 6)) (0.2.2)\nRequirement already satisfied: contourpy==1.3.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 7)) (1.3.1)\nCollecting cramjam==2.9.0 (from -r /kaggle/input/requirements1.txt (line 8))\n  Downloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 9)) (0.12.1)\nCollecting debugpy==1.6.7 (from -r /kaggle/input/requirements1.txt (line 10))\n  Downloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting decorator==5.1.1 (from -r /kaggle/input/requirements1.txt (line 11))\n  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: exceptiongroup==1.2.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 12)) (1.2.2)\nCollecting executing==2.1.0 (from -r /kaggle/input/requirements1.txt (line 13))\n  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\nCollecting fastparquet==2024.11.0 (from -r /kaggle/input/requirements1.txt (line 14))\n  Downloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting filelock==3.16.1 (from -r /kaggle/input/requirements1.txt (line 15))\n  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting fonttools==4.55.1 (from -r /kaggle/input/requirements1.txt (line 16))\n  Downloading fonttools-4.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.7/164.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fsspec==2024.10.0 (from -r /kaggle/input/requirements1.txt (line 17))\n  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: graphviz==0.20.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 18)) (0.20.3)\nRequirement already satisfied: importlib_metadata==8.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 19)) (8.5.0)\nCollecting ipykernel==6.29.5 (from -r /kaggle/input/requirements1.txt (line 20))\n  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\nCollecting ipython==8.30.0 (from -r /kaggle/input/requirements1.txt (line 21))\n  Downloading ipython-8.30.0-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: jedi==0.19.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 22)) (0.19.2)\nRequirement already satisfied: Jinja2==3.1.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 23)) (3.1.4)\nRequirement already satisfied: joblib==1.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 24)) (1.4.2)\nRequirement already satisfied: jupyter_client==8.6.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 25)) (8.6.3)\nRequirement already satisfied: jupyter_core==5.7.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 26)) (5.7.2)\nRequirement already satisfied: kiwisolver==1.4.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 27)) (1.4.7)\nRequirement already satisfied: lightgbm==4.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 28)) (4.5.0)\nRequirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 29)) (3.0.2)\nCollecting matplotlib==3.9.3 (from -r /kaggle/input/requirements1.txt (line 30))\n  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: matplotlib-inline==0.1.7 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 31)) (0.1.7)\nRequirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 32)) (1.3.0)\nRequirement already satisfied: mypy-extensions==1.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 33)) (1.0.0)\nRequirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 34)) (1.6.0)\nRequirement already satisfied: networkx==3.4.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 35)) (3.4.2)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 36)) (1.26.4)\nRequirement already satisfied: packaging==24.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 37)) (24.2)\nRequirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 38)) (2.2.3)\nRequirement already satisfied: parso==0.8.4 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 39)) (0.8.4)\nCollecting pathspec==0.12.1 (from -r /kaggle/input/requirements1.txt (line 40))\n  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: pexpect==4.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 41)) (4.9.0)\nRequirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 42)) (0.7.5)\nRequirement already satisfied: pillow==11.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 43)) (11.0.0)\nCollecting pip==24.2 (from -r /kaggle/input/requirements1.txt (line 44))\n  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: platformdirs==4.3.6 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 45)) (4.3.6)\nRequirement already satisfied: plotly==5.24.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 46)) (5.24.1)\nCollecting polars==0.20.15 (from -r /kaggle/input/requirements1.txt (line 47))\n  Downloading polars-0.20.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\nRequirement already satisfied: prompt_toolkit==3.0.48 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 48)) (3.0.48)\nCollecting psutil==5.9.0 (from -r /kaggle/input/requirements1.txt (line 49))\n  Downloading psutil-5.9.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\nRequirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 50)) (0.7.0)\nCollecting pure_eval==0.2.3 (from -r /kaggle/input/requirements1.txt (line 51))\n  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting Pygments==2.18.0 (from -r /kaggle/input/requirements1.txt (line 52))\n  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pyparsing==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 53)) (3.2.0)\nRequirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 54)) (2.9.0.post0)\nCollecting pytz==2024.2 (from -r /kaggle/input/requirements1.txt (line 55))\n  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting pyzmq==25.1.2 (from -r /kaggle/input/requirements1.txt (line 56))\n  Downloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.9 kB)\nCollecting scikit-learn==1.5.2 (from -r /kaggle/input/requirements1.txt (line 57))\n  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting scipy==1.14.1 (from -r /kaggle/input/requirements1.txt (line 58))\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting seaborn==0.13.2 (from -r /kaggle/input/requirements1.txt (line 59))\n  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: setuptools==75.1.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 60)) (75.1.0)\nRequirement already satisfied: six==1.17.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 61)) (1.17.0)\nCollecting stack-data==0.6.2 (from -r /kaggle/input/requirements1.txt (line 62))\n  Downloading stack_data-0.6.2-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 63)) (1.13.1)\nRequirement already satisfied: tenacity==9.0.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 64)) (9.0.0)\nRequirement already satisfied: threadpoolctl==3.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 65)) (3.5.0)\nRequirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 66)) (2.5.1+cu121)\nCollecting tornado==6.4.1 (from -r /kaggle/input/requirements1.txt (line 67))\n  Downloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nRequirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 68)) (4.67.1)\nCollecting traitlets==5.14.3 (from -r /kaggle/input/requirements1.txt (line 69))\n  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing_extensions==4.12.2 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 70)) (4.12.2)\nCollecting tzdata==2024.2 (from -r /kaggle/input/requirements1.txt (line 71))\n  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: wcwidth==0.2.13 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 72)) (0.2.13)\nCollecting wheel==0.44.0 (from -r /kaggle/input/requirements1.txt (line 73))\n  Downloading wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting xgboost==2.1.3 (from -r /kaggle/input/requirements1.txt (line 74))\n  Downloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\nRequirement already satisfied: zipp==3.21.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/input/requirements1.txt (line 75)) (3.21.0)\nRequirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black==24.10.0->-r /kaggle/input/requirements1.txt (line 3)) (2.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2.4.1)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost==2.1.3->-r /kaggle/input/requirements1.txt (line 74)) (2.23.4)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->-r /kaggle/input/requirements1.txt (line 36)) (2024.2.0)\nDownloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\nDownloading black-24.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cramjam-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading debugpy-1.6.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\nDownloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\nDownloading fastparquet-2024.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\nDownloading fonttools-4.55.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.6/179.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ipython-8.30.0-py3-none-any.whl (820 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m820.8/820.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\nDownloading pip-24.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading polars-0.20.15-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading psutil-5.9.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\nDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading stack_data-0.6.2-py3-none-any.whl (24 kB)\nDownloading tornado-6.4.1-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wheel-0.44.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xgboost-2.1.3-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pytz, pure_eval, wheel, tzdata, traitlets, tornado, pyzmq, Pygments, psutil, polars, pip, pathspec, fsspec, fonttools, filelock, executing, decorator, debugpy, cramjam, appnope, stack-data, black, ipython, ipykernel, scipy, matplotlib, xgboost, seaborn, scikit-learn, fastparquet\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.1\n    Uninstalling pytz-2025.1:\n      Successfully uninstalled pytz-2025.1\n  Attempting uninstall: wheel\n    Found existing installation: wheel 0.45.1\n    Uninstalling wheel-0.45.1:\n      Successfully uninstalled wheel-0.45.1\n  Attempting uninstall: tzdata\n    Found existing installation: tzdata 2025.1\n    Uninstalling tzdata-2025.1:\n      Successfully uninstalled tzdata-2025.1\n  Attempting uninstall: traitlets\n    Found existing installation: traitlets 5.7.1\n    Uninstalling traitlets-5.7.1:\n      Successfully uninstalled traitlets-5.7.1\n  Attempting uninstall: tornado\n    Found existing installation: tornado 6.3.3\n    Uninstalling tornado-6.3.3:\n      Successfully uninstalled tornado-6.3.3\n  Attempting uninstall: pyzmq\n    Found existing installation: pyzmq 24.0.1\n    Uninstalling pyzmq-24.0.1:\n      Successfully uninstalled pyzmq-24.0.1\n  Attempting uninstall: Pygments\n    Found existing installation: Pygments 2.19.1\n    Uninstalling Pygments-2.19.1:\n      Successfully uninstalled Pygments-2.19.1\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.9.5\n    Uninstalling psutil-5.9.5:\n      Successfully uninstalled psutil-5.9.5\n  Attempting uninstall: polars\n    Found existing installation: polars 1.9.0\n    Uninstalling polars-1.9.0:\n      Successfully uninstalled polars-1.9.0\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.12.0\n    Uninstalling fsspec-2024.12.0:\n      Successfully uninstalled fsspec-2024.12.0\n  Attempting uninstall: fonttools\n    Found existing installation: fonttools 4.55.3\n    Uninstalling fonttools-4.55.3:\n      Successfully uninstalled fonttools-4.55.3\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.17.0\n    Uninstalling filelock-3.17.0:\n      Successfully uninstalled filelock-3.17.0\n  Attempting uninstall: decorator\n    Found existing installation: decorator 4.4.2\n    Uninstalling decorator-4.4.2:\n      Successfully uninstalled decorator-4.4.2\n  Attempting uninstall: debugpy\n    Found existing installation: debugpy 1.8.0\n    Uninstalling debugpy-1.8.0:\n      Successfully uninstalled debugpy-1.8.0\n  Attempting uninstall: ipython\n    Found existing installation: ipython 7.34.0\n    Uninstalling ipython-7.34.0:\n      Successfully uninstalled ipython-7.34.0\n  Attempting uninstall: ipykernel\n    Found existing installation: ipykernel 5.5.6\n    Uninstalling ipykernel-5.5.6:\n      Successfully uninstalled ipykernel-5.5.6\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.13.1\n    Uninstalling scipy-1.13.1:\n      Successfully uninstalled scipy-1.13.1\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.0.3\n    Uninstalling xgboost-2.0.3:\n      Successfully uninstalled xgboost-2.0.3\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.12.2\n    Uninstalling seaborn-0.12.2:\n      Successfully uninstalled seaborn-0.12.2\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\ngoogle-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.29.5 which is incompatible.\ngoogle-colab 1.0.0 requires ipython==7.34.0, but you have ipython 8.30.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.3.3, but you have tornado 6.4.1 which is incompatible.\nmoviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pygments-2.18.0 appnope-0.1.4 black-24.10.0 cramjam-2.9.0 debugpy-1.6.7 decorator-5.1.1 executing-2.1.0 fastparquet-2024.11.0 filelock-3.16.1 fonttools-4.55.1 fsspec-2024.10.0 ipykernel-6.29.5 ipython-8.30.0 matplotlib-3.9.3 pathspec-0.12.1 pip-24.2 polars-0.20.15 psutil-5.9.0 pure_eval-0.2.3 pytz-2024.2 pyzmq-25.1.2 scikit-learn-1.5.2 scipy-1.14.1 seaborn-0.13.2 stack-data-0.6.2 tornado-6.4.1 traitlets-5.14.3 tzdata-2024.2 wheel-0.44.0 xgboost-2.1.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# DATA PROCESSING UTILS","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\nclass StatelessRandomGenerator:\n    def __init__(self, seed=42):\n        self.seed = seed\n\n    def set_seed(self, new_seed):\n        self.seed = new_seed\n\n    def random(self, size=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.random(size)\n\n    def integers(self, low, high=None, size=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.integers(low, high, size)\n\n    def choice(self, a, size=None, replace=True, p=None):\n        rng = np.random.default_rng(self.seed)\n        return rng.choice(a, size, replace, p)\n\n\nglobal_rng = StatelessRandomGenerator(42)\n\n\ndef set_global_seed(new_seed):\n    global_rng.set_seed(new_seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:34.926240Z","iopub.execute_input":"2025-03-19T10:15:34.926496Z","iopub.status.idle":"2025-03-19T10:15:34.932229Z","shell.execute_reply.started":"2025-03-19T10:15:34.926464Z","shell.execute_reply":"2025-03-19T10:15:34.931560Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\n\n\ndef wmape_metric(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n    return torch.sum(torch.abs(pred - true), dim=0) / torch.sum(true, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:34.933031Z","iopub.execute_input":"2025-03-19T10:15:34.933344Z","iopub.status.idle":"2025-03-19T10:15:36.562512Z","shell.execute_reply.started":"2025-03-19T10:15:34.933314Z","shell.execute_reply":"2025-03-19T10:15:36.561645Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# DATA PROCESSING","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport json\nfrom pathlib import Path\nimport polars as pl\n#from data_processing.utils.stateless_rng import global_rng\n\ndef filter_purchases_purchases_per_month_pl(\n    df_pl: pl.DataFrame, train_end: datetime.date, group_by_channel_id: bool = False\n):\n    \"\"\"Filters extreme customers and groups purchases by date and optionally by sales channel.\n\n    This function:\n    1. Groups transactions by customer, date, and optionally sales channel\n    2. Identifies extreme customers based on the 99th percentile of total items purchased\n    3. Removes these customers from the dataset\n\n    Args:\n        df_pl (pl.DataFrame): Input transaction dataframe containing:\n            - customer_id: Customer identifier\n            - date: Transaction date\n            - article_id: Product identifier\n            - price: Transaction price\n            - sales_channel_id: Sales channel identifier\n        train_end (datetime.date): End date for training period.\n        group_by_channel_id (bool, optional): Whether to group transactions by sales channel. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - grouped_df: Grouped transaction data with columns:\n                - customer_id, date, [sales_channel_id], article_ids, total_price, prices, num_items\n            - extreme_customers: DataFrame of customers identified as outliers based on purchase behavior\n\n    Notes:\n        Extreme customers are identified using the 99th percentile of total items purchased\n        during the training period.\n    \"\"\"\n    # Used for multi variate time series\n    if group_by_channel_id:\n        grouped_df = (\n            df_pl.lazy()\n            .group_by([\"customer_id\", \"date\", \"sales_channel_id\"])\n            .agg(\n                [\n                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n                    pl.col(\"price\").explode().alias(\"prices\"),\n                ]\n            )\n            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n        )\n    else:\n        grouped_df = (\n            df_pl.lazy()\n            .group_by([\"customer_id\", \"date\"])\n            .agg(\n                [\n                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n                    pl.col(\"sales_channel_id\").explode().alias(\"sales_channel_ids\"),\n                    pl.col(\"price\").explode().alias(\"prices\"),\n                ]\n            )\n            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n        )\n\n    # Only remove customers with extreme purchases in train period\n    customers_summary = (\n        df_pl.lazy()\n        .filter(pl.col(\"date\") < train_end)\n        .group_by(\"customer_id\")\n        .agg(\n            [\n                pl.col(\"date\").n_unique().alias(\"total_purchases\"),\n                pl.col(\"price\").sum().round(2).alias(\"total_spent\"),\n                pl.col(\"article_id\").flatten().alias(\"flattened_ids\")\n            ]\n        )\n        .with_columns(pl.col(\"flattened_ids\").list.len().alias(\"total_items\"))\n    )\n\n    quantile = 0.99\n    total_purchases_99, total_spending_99, total_items_99 = (\n        customers_summary.select(\n            [\n                pl.col(\"total_purchases\").quantile(quantile),\n                pl.col(\"total_spent\").quantile(quantile),\n                pl.col(\"total_items\").quantile(quantile),\n            ]\n        )\n        .collect()\n        .to_numpy()\n        .flatten()\n    )\n\n    # Currently only remove customers with very large number of total items purchased\n    extreme_customers = customers_summary.filter(\n        (pl.col(\"total_items\") >= total_items_99)\n        # | (pl.col(\"total_purchases\") >= total_purchases_99)\n        # | (pl.col(\"total_spent\") >= total_spending_99)\n    )\n\n    extreme_customers = extreme_customers.select(\"customer_id\").unique()\n    extreme_customers = extreme_customers.collect()\n\n    print(\n        f\"\"\"\n        Cutoff Values for {quantile*100}th Percentiles:\n        -----------------------------------\n        Total items bought:    {total_items_99:.0f} items\n\n        -----------------------------------\n        Removed Customers:     {len(extreme_customers):,}\n        \"\"\"\n    )\n\n    return grouped_df.collect(), extreme_customers\n\ndef train_test_split(\n    train_df: pl.DataFrame,\n    test_df: pl.DataFrame,\n    subset: int = None,\n    train_subsample_percentage: float = None,\n) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Splits data into train, validation, and test sets with optional subsampling.\n\n    The function performs the following operations:\n    1. Optional subsampling of both train and test data\n    2. Optional percentage-based subsampling of training data\n    3. Creates a validation set from 10% of the training data\n\n    Args:\n        train_df (pl.DataFrame): Training dataset.\n        test_df (pl.DataFrame): Test dataset.\n        subset (int, optional): If provided, limits both train and test sets to first n rows. \n            Defaults to None.\n        train_subsample_percentage (float, optional): If provided, randomly samples this percentage \n            of training data. Defaults to None.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Final training dataset (90% of training data after subsampling)\n            - val_df: Validation dataset (10% of training data)\n            - test_df: Test dataset (potentially subsampled)\n\n    Notes:\n        If both subset and train_subsample_percentage are provided, subset is applied first.\n        The validation set is always 10% of the remaining training data after any subsampling.\n    \"\"\"\n\n    if subset is not None:\n        train_df = train_df[:subset]\n        test_df = test_df[:subset]\n    elif train_subsample_percentage is not None:\n        sampled_indices = global_rng.choice(\n            len(train_df),\n            size=int(train_subsample_percentage * len(train_df)),\n            replace=False,\n        )\n        train_df = train_df[sampled_indices]\n\n    # Train-val-split\n    # Calculate 10% of the length of the array\n    sampled_indices = global_rng.choice(\n        len(train_df), size=int(0.1 * len(train_df)), replace=False\n    )\n    val_df = train_df[sampled_indices]\n    train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n\n    return train_df, val_df, test_df\n\ndef map_article_ids(df: pl.DataFrame, data_path: Path) -> pl.DataFrame:\n    \"\"\"Maps article IDs to new running IDs using a mapping dictionary from JSON.\n\n    Args:\n        df (pl.DataFrame): DataFrame with 'article_id' column to be mapped.\n        data_path (Path): Path to directory with 'running_id_dict.json' containing ID mappings.\n\n    Returns:\n        pl.DataFrame: DataFrame with mapped article IDs, sorted by new IDs. Non-mapped articles are removed.\n    \"\"\"\n    with open(data_path / \"running_id_dict.json\", \"r\") as f:\n        data = json.load(f)\n    article_id_dict = data[\"combined\"]\n\n    mapping_df = pl.DataFrame(\n        {\n            \"old_id\": list(article_id_dict.keys()),\n            \"new_id\": list(article_id_dict.values()),\n        },\n        schema_overrides={\"old_id\": pl.Int32, \"new_id\": pl.Int32},\n    )\n\n    # Join and select\n    df = df.join(\n        mapping_df, left_on=\"article_id\", right_on=\"old_id\", how=\"inner\"\n    ).select(\n        pl.col(\"new_id\").alias(\"article_id\"),\n        pl.all().exclude([\"article_id\", \"old_id\", \"new_id\"]),\n    )\n    df = df.sort(\"article_id\")\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:36.564784Z","iopub.execute_input":"2025-03-19T10:15:36.565098Z","iopub.status.idle":"2025-03-19T10:15:36.750394Z","shell.execute_reply.started":"2025-03-19T10:15:36.565079Z","shell.execute_reply":"2025-03-19T10:15:36.748944Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#from pathlib import Path\n#from data_processing.customer_df.customer_df import get_customer_df_benchmarks\n#from data_processing.transaction_df.transaction_df import get_tx_article_dfs\nimport polars as pl\n\n\ndef expand_list_columns(\n    df: pl.DataFrame, date_col: str = \"days_before_lst\", num_col: str = \"num_items_lst\"\n) -> pl.DataFrame:\n    \"\"\"\n    Expand a Polars DataFrame by repeating each element in a list column according to\n    the counts specified in another list column.\n\n    Args:\n        df: Input Polars DataFrame with list columns\n        date_col: Name of the column containing the lists to be expanded\n        num_col: Name of the column containing lists of counts\n\n    Returns:\n        A new Polars DataFrame where the list elements in date_col have been expanded\n    \"\"\"\n    expanded = df.with_columns(\n        pl.struct([date_col, num_col])\n        .map_elements(\n            lambda x: [\n                date\n                for date, count in zip(x[date_col], x[num_col])\n                for _ in range(count)\n            ]\n        )\n        .alias(date_col)\n    )\n\n    return expanded\n\n\ndef add_benchmark_tx_features(df: pl.DataFrame) -> pl.DataFrame:\n    \"\"\"Creates benchmark transaction features from aggregated customer transaction data.\n\n    Args:\n        df: A Polars DataFrame containing aggregated transaction data with list columns\n            including total_price_lst, num_items_lst, days_before_lst, price_lst,\n            and CLV_label.\n\n    Returns:\n        pl.DataFrame: A DataFrame with derived features including:\n            - total_spent: Sum of all transaction amounts\n            - total_purchases: Count of transactions\n            - total_items: Sum of items purchased\n            - days_since_last_purchase: Days since most recent transaction\n            - days_since_first_purchase: Days since first transaction\n            - avg_spent_per_transaction: Mean transaction amount\n            - avg_items_per_transaction: Mean items per transaction\n            - avg_days_between: Mean days between transactions\n            - regression_label: CLV label for regression\n            - classification_label: Binary CLV label (>0)\n\n    Note:\n        The avg_days_between calculation may return None for customers with single\n        transactions, which is handled by tree-based algorithms.\n    \"\"\"\n    return df.select(\n        \"customer_id\",\n        pl.col(\"total_price_lst\").list.sum().alias(\"total_spent\"),\n        pl.col(\"total_price_lst\").list.len().alias(\"total_purchases\"),\n        pl.col(\"num_items_lst\").list.sum().alias(\"total_items\"),\n        pl.col(\"days_before_lst\").list.get(-1).alias(\"days_since_last_purchase\"),\n        pl.col(\"days_before_lst\").list.get(0).alias(\"days_since_first_purchase\"),\n        pl.col(\"price_lst\").list.mean().alias(\"avg_spent_per_transaction\"),\n        (\n            pl.col(\"num_items_lst\")\n            .list.mean()\n            .cast(pl.Float32)\n            .alias(\"avg_items_per_transaction\")\n        ),\n        # Code below returns None values for customers with single Tx\n        # Tree algos should be able to handle this\n        (\n            pl.col(\"days_before_lst\")\n            .list.diff(null_behavior=\"drop\")\n            .list.mean()\n            .mul(-1)\n            .cast(pl.Float32)\n            .alias(\"avg_days_between\")\n        ),\n        pl.col(\"CLV_label\").alias(\"regression_label\"),\n        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n    )\n\n\ndef process_dataframe(df: pl.DataFrame, max_length: int = 20) -> pl.DataFrame:\n    \"\"\"Processes a polars DataFrame by expanding list columns and selecting specific columns with transformations.\n\n    This function performs several operations on the input DataFrame:\n    1. Expands list columns using the expand_list_columns function\n    2. Selects and renames specific columns\n    3. Truncates list columns to a maximum length\n\n    Args:\n        df: A polars DataFrame containing customer transaction data\n        max_length: Maximum number of elements to keep in list columns (default: 20)\n\n    Returns:\n        A processed polars DataFrame with the following columns:\n            - customer_id: Customer identifier\n            - days_before_lst: Truncated list of days before some reference date\n            - articles_ids_lst: Truncated list of article identifiers\n            - regression_label: CLV label for regression tasks\n            - classification_label: Binary classification label derived from CLV\n    \"\"\"\n    df = expand_list_columns(df, date_col=\"days_before_lst\", num_col=\"num_items_lst\")\n    return df.select(\n        \"customer_id\",\n        \"days_before_lst\",\n        \"articles_ids_lst\",\n        pl.col(\"CLV_label\").alias(\"regression_label\"),\n        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n    ).with_columns(\n        pl.col(\"days_before_lst\").list.tail(max_length),\n        pl.col(\"articles_ids_lst\").list.tail(max_length),\n    )\n\n\ndef get_benchmark_dfs(\n    data_path: Path, config: dict\n) -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n    \"\"\"Creates benchmark train, validation, and test datasets with transaction and customer features.\n\n    Args:\n        data_path: Path object pointing to the data directory\n        config: Dictionary containing configuration parameters for data processing\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: A tuple containing:\n            - train_df: Training dataset with benchmark features\n            - val_df: Validation dataset with benchmark features\n            - test_df: Test dataset with benchmark features\n\n        Each DataFrame contains transaction-derived features joined with customer features.\n    \"\"\"\n    train_article, val_article, test_article = get_tx_article_dfs(\n        data_path=data_path,\n        config=config,\n        cols_to_aggregate=[\n            \"date\",\n            \"days_before\",\n            \"article_ids\",\n            \"sales_channel_ids\",\n            \"total_price\",\n            \"prices\",\n            \"num_items\",\n        ],\n        keep_customer_id=True,\n    )\n\n    customer_df = get_customer_df_benchmarks(data_path=data_path, config=config)\n\n    train_df = process_dataframe(\n        df=train_article, max_length=config[\"max_length\"]\n    ).join(customer_df, on=\"customer_id\", how=\"left\")\n    val_df = process_dataframe(df=val_article, max_length=config[\"max_length\"]).join(\n        customer_df, on=\"customer_id\", how=\"left\"\n    )\n    test_df = process_dataframe(df=test_article, max_length=config[\"max_length\"]).join(\n        customer_df, on=\"customer_id\", how=\"left\"\n    )\n\n    return train_df, val_df, test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:36.752205Z","iopub.execute_input":"2025-03-19T10:15:36.752507Z","iopub.status.idle":"2025-03-19T10:15:36.769276Z","shell.execute_reply.started":"2025-03-19T10:15:36.752477Z","shell.execute_reply":"2025-03-19T10:15:36.768498Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import polars as pl\n#from pathlib import Path\n\n\ndef get_customer_df_benchmarks(data_path: Path, config: dict):\n    \"\"\"Processes customer data with age grouping and zip code mapping.\n\n    Args:\n        data_path (Path): Path to directory containing 'customers.csv' and 'zip_code_count.csv'.\n        config (dict): Configuration with 'min_zip_code_count'. Updated with 'num_age_groups' and 'num_zip_codes'.\n\n    Returns:\n        pl.DataFrame: Processed DataFrame with customer_id, age_group (0-6), and mapped zip_code_id.\n    \"\"\"\n    file_path = data_path / \"customers.csv\"\n    df = pl.scan_csv(file_path).select(\n        (\n            \"customer_id\",\n            pl.col(\"age\").fill_null(strategy=\"mean\"),\n            \"postal_code\",\n        )\n    )\n\n    # df = df.with_columns(\n    #     [\n    #         pl.when(pl.col(\"age\").is_null())\n    #         .then(0)\n    #         .when(pl.col(\"age\") < 25)\n    #         .then(1)\n    #         .when(pl.col(\"age\").is_between(25, 34))\n    #         .then(2)\n    #         .when(pl.col(\"age\").is_between(35, 44))\n    #         .then(3)\n    #         .when(pl.col(\"age\").is_between(45, 54))\n    #         .then(4)\n    #         .when(pl.col(\"age\").is_between(55, 64))\n    #         .then(5)\n    #         .otherwise(6)\n    #         .alias(\"age_group\")\n    #     ]\n    # )\n    # config[\"num_age_groups\"] = 7\n\n    return df.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:36.770134Z","iopub.execute_input":"2025-03-19T10:15:36.770338Z","iopub.status.idle":"2025-03-19T10:15:36.794388Z","shell.execute_reply.started":"2025-03-19T10:15:36.770320Z","shell.execute_reply":"2025-03-19T10:15:36.793698Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#from datetime import datetime\n#from pathlib import Path\n#import polars as pl\n\n#from data_processing.utils.utils_transaction_df import (\n #   filter_purchases_purchases_per_month_pl,\n  #  map_article_ids,\n   # train_test_split,\n#)\n\n\ndef generate_clv_data_pl(\n    df: pl.DataFrame,\n    agg_df: pl.DataFrame,\n    label_threshold: datetime.date,\n    pred_end: datetime.date,\n    clv_periods: list,\n    log_clv: bool = False,\n):\n    \"\"\"Generates Customer Lifetime Value (CLV) data from transaction dataframe.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe containing customer purchases.\n        agg_df (pl.DataFrame): Aggregated dataframe containing customer data.\n        label_threshold (datetime.date): Start date for CLV calculation period.\n        pred_end (datetime.date): End date for CLV calculation period.\n        clv_periods (list): List of periods for CLV calculation (currently supports single period only).\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Aggregated dataframe with added CLV calculations.\n\n    Raises:\n        ValueError: If more than one CLV period is provided.\n    \"\"\"\n    if len(clv_periods) > 1:\n        raise ValueError(\"CLV periods should be a single number for now.\")\n\n    # Filter transactions between label_threshold and end_date for each period\n    filtered_df = df.filter(\n        (pl.col(\"date\") >= label_threshold) & (pl.col(\"date\") <= pred_end)\n    )\n\n    # Sum total_price for the filtered transactions by customer_id. This is the CLV\n    summed_period_df = filtered_df.group_by(\"customer_id\").agg(\n        pl.sum(\"total_price\").round(2).alias(f\"CLV_label\")\n    )\n    if log_clv:\n        summed_period_df = summed_period_df.with_columns(\n            pl.col(f\"CLV_label\").log1p().round(2).alias(f\"CLV_label\")\n        )\n\n    agg_df = agg_df.join(summed_period_df, on=\"customer_id\", how=\"left\")\n\n    agg_df = agg_df.fill_null(0)\n    return agg_df\n\n\ndef group_and_convert_df_pl(\n    df: pl.DataFrame,\n    label_start_date: datetime.date,\n    pred_end: datetime.date,\n    clv_periods: list,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"num_items\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n    ],\n    keep_customer_id: bool = True,\n    log_clv: bool = False,\n) -> pl.DataFrame:\n    \"\"\"Groups and converts transaction data into aggregated customer-level features.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe.\n        label_start_date (datetime.date): Start date for clv label period.\n        pred_end (datetime.date): End date for prediction period.\n        clv_periods (list): List of periods for CLV calculation.\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        pl.DataFrame: Aggregated customer-level dataframe.\n\n    Raises:\n        ValueError: If required columns (days_before, article_ids, num_items) are missing from cols_to_aggregate.\n    \"\"\"\n\n    if any(\n        col not in cols_to_aggregate\n        for col in [\"days_before\", \"article_ids\", \"num_items\"]\n    ):\n        raise ValueError(\n            \"The columns days_before, article_ids, and num_items are required \"\n            \"for the aggregation\"\n        )\n\n    mapping = {\n        \"date\": \"date_lst\",\n        \"days_before\": \"days_before_lst\",\n        \"article_ids\": \"articles_ids_lst\",\n        \"sales_channel_ids\": \"sales_channel_id_lst\",\n        \"total_price\": \"total_price_lst\",\n        \"prices\": \"price_lst\",\n        \"num_items\": \"num_items_lst\",\n    }\n\n    agg_df = (\n        df.filter(pl.col(\"date\") < label_start_date)\n        .with_columns(\n            (label_start_date - pl.col(\"date\"))\n            .dt.total_days()\n            .cast(pl.Int32)\n            .alias(\"days_before\"),\n            (\n                pl.col(\"sales_channel_ids\")\n                .cast(pl.List(pl.Int32))\n                .alias(\"sales_channel_ids\")\n            ),\n            pl.col(\"article_ids\").cast(pl.List(pl.Int32)).alias(\"article_ids\"),\n        )\n        .sort(\"customer_id\", \"date\")\n        .group_by(\"customer_id\")\n        .agg(\n            pl.col(\"date\").explode().alias(\"date_lst\"),\n            pl.col(\"days_before\").explode().alias(\"days_before_lst\"),\n            pl.col(\"article_ids\").explode().alias(\"articles_ids_lst\"),\n            pl.concat_list(pl.col(\"sales_channel_ids\")).alias(\"sales_channel_id_lst\"),\n            pl.col(\"total_price\").explode().alias(\"total_price_lst\"),\n            pl.col(\"prices\").explode().alias(\"price_lst\"),\n            pl.col(\"num_items\").explode().alias(\"num_items_lst\"),\n        )\n    )\n\n    if clv_periods is not None:\n        agg_df = generate_clv_data_pl(\n            df=df,\n            agg_df=agg_df,\n            label_threshold=label_start_date,\n            pred_end=pred_end,\n            clv_periods=clv_periods,\n            log_clv=log_clv,\n        )\n\n    # Drop columns which are not to be aggregated\n    cols_to_drop = [v for k, v in mapping.items() if k not in cols_to_aggregate]\n    if not keep_customer_id:\n        cols_to_drop.append(\"customer_id\")\n    agg_df = agg_df.drop(*cols_to_drop)\n\n    return agg_df\n\n\ndef split_df_and_group_pl(\n    df: pl.DataFrame,\n    clv_periods: list,\n    config: dict,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n    log_clv: bool = False,\n) -> tuple[pl.DataFrame, pl.DataFrame]:\n    \"\"\"Splits transaction data into training and test sets and performs aggregation.\n\n    Args:\n        df (pl.DataFrame): Input transaction dataframe.\n        clv_periods (list): List of periods for CLV calculation.\n        config (dict): Configuration dictionary containing:\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Aggregated training dataset\n            - test_df: Aggregated test dataset\n    \"\"\"\n\n    train_begin = datetime.strptime(config.get(\"train_begin\"), \"%Y-%m-%d\")\n    train_label_start = datetime.strptime(config.get(\"train_label_begin\"), \"%Y-%m-%d\")\n    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n    test_begin = datetime.strptime(config.get(\"test_begin\"), \"%Y-%m-%d\")\n    test_label_start = datetime.strptime(config.get(\"test_label_begin\"), \"%Y-%m-%d\")\n    test_end = datetime.strptime(config.get(\"test_end\"), \"%Y-%m-%d\")\n\n    # Creating the training DataFrame by filtering dates up to `train_end`\n    train_df = df.filter(\n        (pl.col(\"date\") <= train_end) & (pl.col(\"date\") >= train_begin)\n    )\n\n    train_df = group_and_convert_df_pl(\n        df=train_df,\n        label_start_date=train_label_start,\n        pred_end=train_end,\n        clv_periods=clv_periods,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n        log_clv=log_clv,\n    )\n\n    # Creating the test DataFrame by filtering dates after `test_begin`\n    test_df = df.filter((pl.col(\"date\") >= test_begin) & (pl.col(\"date\") <= test_end))\n\n    test_df = group_and_convert_df_pl(\n        df=test_df,\n        label_start_date=test_label_start,\n        pred_end=test_end,\n        clv_periods=clv_periods,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n        log_clv=log_clv,\n    )\n\n    return train_df, test_df\n\n\ndef load_data_rem_outlier_pl(\n    data_path: Path, train_end: datetime.date, group_by_channel_id: bool = False\n):\n    \"\"\"Loads transaction data, applies price scaling, and removes outliers.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data parquet file.\n        train_end (datetime.date): End date for training period.\n        group_by_channel_id (bool, optional): Whether to group data by sales channel ID. Defaults to False.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - grouped_df: Processed transaction dataframe\n            - extreme_customers: Dataframe of customers identified as outliers\n    \"\"\"\n    file_path = data_path / \"transactions_polars.parquet\"\n    df_pl = pl.read_parquet(file_path)\n\n    df_pl = df_pl.with_columns(\n        pl.col(\"t_dat\").alias(\"date\").cast(pl.Date), pl.col(\"article_id\").cast(pl.Int32)\n    )\n\n    df_pl = df_pl.with_columns(\n        pl.col(\"price\").mul(590).cast(pl.Float32).round(2).alias(\"price\")\n    )\n\n    # Map article ids to running ids so that they match with feature matrix\n    df_pl = map_article_ids(df=df_pl, data_path=data_path)\n\n    grouped_df, extreme_customers = filter_purchases_purchases_per_month_pl(\n        df_pl, train_end=train_end, group_by_channel_id=group_by_channel_id\n    )\n\n    return grouped_df, extreme_customers\n\n\ndef get_customer_train_test_articles_pl(\n    data_path: Path,\n    config: dict,\n    clv_periods: list = None,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n):\n    \"\"\"Processes customer transaction data into train and test sets with article information.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data.\n        config (dict): Configuration dictionary for data processing parameters.\n        clv_periods (list, optional): List of periods for CLV calculation. Defaults to None.\n        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Processed training dataset with article information\n            - test_df: Processed test dataset with article information\n    \"\"\"\n    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n    grouped_df, extreme_customers = load_data_rem_outlier_pl(\n        data_path=data_path, train_end=train_end\n    )\n\n    train_df, test_df = split_df_and_group_pl(\n        df=grouped_df,\n        clv_periods=clv_periods,\n        config=config,\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=True,\n        log_clv=config.get(\"log_clv\", False),\n    )\n\n    train_df = train_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n    test_df = test_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n\n    if not keep_customer_id:\n        train_df = train_df.drop(\"customer_id\")\n        test_df = test_df.drop(\"customer_id\")\n\n    return train_df, test_df\n\n\ndef get_tx_article_dfs(\n    data_path: Path,\n    config: dict,\n    cols_to_aggregate: list = [\n        \"date\",\n        \"days_before\",\n        \"article_ids\",\n        \"sales_channel_ids\",\n        \"total_price\",\n        \"prices\",\n        \"num_items\",\n    ],\n    keep_customer_id: bool = True,\n):\n    \"\"\"Creates train, validation, and test datasets with optional subsampling.\n\n    Args:\n        data_path (Path): Path to directory containing transaction data files.\n        config (dict): Configuration dictionary containing:\n        cols_to_aggregate (list, optional): Transaction columns to include in output.\n        keep_customer_id (bool, optional): Whether to retain customer_id column.\n\n    Returns:\n        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n            - train_df: Final training dataset (subset of original training data)\n            - val_df: Validation dataset (10% of original training data)\n            - test_df: Test dataset (optionally subsampled)\n    \"\"\"\n    \"\"\"\n    Columns of dfs:\n        - customer_id\n        - date_lst (list[date]): Dates of each transaction\n        - days_before_lst (list[int]): Number of days between start of prediction and date of transction\n        - articles_ids_lst (list[int]): Flattened list of all items a customer purchased \n        - sales_channel_id_lst (list[list[int]]): Sales channel of a transaction (repeated for each item within a transaction)\n        - total_price_lst (list[float]): Value of each transaction\n        - price_lst (list[float]): Flattened list of prices of all items customer purchased\n        - num_items_lst (list[int]): Number of items in each transaction\n        - CLV_label (float): Sales in prediction period (label to be used)\n    \"\"\"\n    train_df, test_df = get_customer_train_test_articles_pl(\n        data_path=data_path,\n        config=config,\n        clv_periods=config.get(\"clv_periods\", [6]),\n        cols_to_aggregate=cols_to_aggregate,\n        keep_customer_id=keep_customer_id,\n    )\n    train_df, val_df, test_df = train_test_split(\n        train_df=train_df,\n        test_df=test_df,\n        subset=config.get(\"subset\"),\n        train_subsample_percentage=config.get(\"train_subsample_percentage\"),\n    )\n    return train_df, val_df, test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:36.795291Z","iopub.execute_input":"2025-03-19T10:15:36.795576Z","iopub.status.idle":"2025-03-19T10:15:36.819759Z","shell.execute_reply.started":"2025-03-19T10:15:36.795548Z","shell.execute_reply":"2025-03-19T10:15:36.819046Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#from pathlib import Path\n#from data_processing.get_data import get_benchmark_dfs\n#import polars as pl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:36.820612Z","iopub.execute_input":"2025-03-19T10:15:36.820908Z","iopub.status.idle":"2025-03-19T10:15:36.845965Z","shell.execute_reply.started":"2025-03-19T10:15:36.820878Z","shell.execute_reply":"2025-03-19T10:15:36.845259Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"config = {\n    \"train_begin\": \"2018-09-20\",\n    \"train_label_begin\": \"2019-09-20\",\n    \"train_end\": \"2020-03-17\",\n    \"test_begin\": \"2019-03-19\",\n    \"test_label_begin\": \"2020-03-18\",\n    \"test_end\": \"2020-09-13\",\n    \"min_zip_code_count\": 3,\n    \"date_aggregation\": \"daily\",\n    \"group_by_channel_id\": False,\n    \"log_clv\": False,\n    \"clv_periods\": [6],\n    \"subset\": None,\n    \"train_subsample_percentage\": None,\n    \"max_length\":20, # DEFINE HOW MANY ITEMS ARE TO BE CONSIDERED IN TRANSFORMER SEQUENCE\n}\n# data_path = Path(\"/kaggle/input/hm-dataset/data/data\")\ndata_path = Path(\"/kaggle/input/data/data/\")\n\nprint(10 * \"#\", \" Loading data \", 10 * \"#\")\ntrain_df, val_df, test_df = get_benchmark_dfs(data_path, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:15:36.846833Z","iopub.execute_input":"2025-03-19T10:15:36.847110Z","iopub.status.idle":"2025-03-19T10:17:22.784598Z","shell.execute_reply.started":"2025-03-19T10:15:36.847058Z","shell.execute_reply":"2025-03-19T10:17:22.783718Z"}},"outputs":[{"name":"stdout","text":"##########  Loading data  ##########\n\n        Cutoff Values for 99.0th Percentiles:\n        -----------------------------------\n        Total items bought:    152 items\n\n        -----------------------------------\n        Removed Customers:     11,908\n        \n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-4-8bb703141d2d>:167: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n  train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:22.785551Z","iopub.execute_input":"2025-03-19T10:17:22.785808Z","iopub.status.idle":"2025-03-19T10:17:22.793523Z","shell.execute_reply.started":"2025-03-19T10:17:22.785787Z","shell.execute_reply":"2025-03-19T10:17:22.792672Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"shape: (951_705, 7)\n┌───────────────┬───────────────┬───────────────┬──────────────┬──────────────┬─────┬──────────────┐\n│ customer_id   ┆ days_before_l ┆ articles_ids_ ┆ regression_l ┆ classificati ┆ age ┆ postal_code  │\n│ ---           ┆ st            ┆ lst           ┆ abel         ┆ on_label     ┆ --- ┆ ---          │\n│ str           ┆ ---           ┆ ---           ┆ ---          ┆ ---          ┆ i64 ┆ str          │\n│               ┆ list[i64]     ┆ list[i32]     ┆ f32          ┆ i32          ┆     ┆              │\n╞═══════════════╪═══════════════╪═══════════════╪══════════════╪══════════════╪═════╪══════════════╡\n│ 76a9e3e769051 ┆ [68, 68, …    ┆ [9423, 9423,  ┆ 0.0          ┆ 0            ┆ 33  ┆ 2c29ae653a92 │\n│ 8dec67d775c3d ┆ 29]           ┆ … 80293]      ┆              ┆              ┆     ┆ 82cce4151bd8 │\n│ a579fb…       ┆               ┆               ┆              ┆              ┆     ┆ 7643c907…    │\n│ fd397bca991fb ┆ [183]         ┆ [15965]       ┆ 67.970001    ┆ 1            ┆ 27  ┆ 6955a641e0ba │\n│ 364c36252951b ┆               ┆               ┆              ┆              ┆     ┆ 9300efd9b2d4 │\n│ 50a767…       ┆               ┆               ┆              ┆              ┆     ┆ 2906c96f…    │\n│ 09a10086a549f ┆ [155, 110, …  ┆ [37963,       ┆ 0.0          ┆ 0            ┆ 24  ┆ 2c29ae653a92 │\n│ a6d34a8fb160e ┆ 110]          ┆ 33830, …      ┆              ┆              ┆     ┆ 82cce4151bd8 │\n│ bb3779…       ┆               ┆ 71079]        ┆              ┆              ┆     ┆ 7643c907…    │\n│ 2ee66d660b16e ┆ [289, 289, …  ┆ [2271, 24744, ┆ 0.0          ┆ 0            ┆ 44  ┆ 0fec0086f1e3 │\n│ 0c2dc27b8d656 ┆ 30]           ┆ … 83441]      ┆              ┆              ┆     ┆ 9e5284b6ec66 │\n│ b9b350…       ┆               ┆               ┆              ┆              ┆     ┆ 00a2df69…    │\n│ a6d717dde7780 ┆ [171, 171, …  ┆ [3455, 77717, ┆ 58.470001    ┆ 1            ┆ 47  ┆ bc38656f2a2b │\n│ 1e9d383dd52ad ┆ 110]          ┆ … 85852]      ┆              ┆              ┆     ┆ 191712151c32 │\n│ 9ced57…       ┆               ┆               ┆              ┆              ┆     ┆ 24687793…    │\n│ …             ┆ …             ┆ …             ┆ …            ┆ …            ┆ …   ┆ …            │\n│ a3b8ed7d0788e ┆ [97]          ┆ [68207]       ┆ 0.0          ┆ 0            ┆ 36  ┆ 83d355719a39 │\n│ d8e34bffab9f5 ┆               ┆               ┆              ┆              ┆     ┆ ccfc38e36339 │\n│ 13eee2…       ┆               ┆               ┆              ┆              ┆     ┆ e1dcfcd7…    │\n│ c30cc75277dac ┆ [320, 309, …  ┆ [55901, 8, …  ┆ 0.0          ┆ 0            ┆ 26  ┆ 120c8f1333d6 │\n│ 24c66da85a0ab ┆ 185]          ┆ 75628]        ┆              ┆              ┆     ┆ 24ab09433ffd │\n│ 5f2a8f…       ┆               ┆               ┆              ┆              ┆     ┆ 5147011a…    │\n│ 31473511ded7a ┆ [168, 168, …  ┆ [44984,       ┆ 197.100006   ┆ 1            ┆ 50  ┆ 2faae2f724d7 │\n│ 7e5065449a3c5 ┆ 66]           ┆ 44987, …      ┆              ┆              ┆     ┆ a0705c894177 │\n│ 1565e4…       ┆               ┆ 88701]        ┆              ┆              ┆     ┆ 38a7b886…    │\n│ 66fa41f4d5f33 ┆ [228, 228, …  ┆ [15674,       ┆ 148.949997   ┆ 1            ┆ 45  ┆ c8217776877a │\n│ 81faeeff57148 ┆ 139]          ┆ 15689, …      ┆              ┆              ┆     ┆ 67ccac699acf │\n│ 58d81e…       ┆               ┆ 2247]         ┆              ┆              ┆     ┆ fd1ecbd3…    │\n│ 7f3c43999e3b2 ┆ [294, 237, …  ┆ [53356,       ┆ 98.900002    ┆ 1            ┆ 26  ┆ 2ccca0c0f059 │\n│ ab0d342151574 ┆ 22]           ┆ 56612, …      ┆              ┆              ┆     ┆ a5cd5469abf2 │\n│ 6108e1…       ┆               ┆ 84724]        ┆              ┆              ┆     ┆ 8895a82c…    │\n└───────────────┴───────────────┴───────────────┴──────────────┴──────────────┴─────┴──────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (951_705, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>customer_id</th><th>days_before_lst</th><th>articles_ids_lst</th><th>regression_label</th><th>classification_label</th><th>age</th><th>postal_code</th></tr><tr><td>str</td><td>list[i64]</td><td>list[i32]</td><td>f32</td><td>i32</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;76a9e3e7690518…</td><td>[68, 68, … 29]</td><td>[9423, 9423, … 80293]</td><td>0.0</td><td>0</td><td>33</td><td>&quot;2c29ae653a9282…</td></tr><tr><td>&quot;fd397bca991fb3…</td><td>[183]</td><td>[15965]</td><td>67.970001</td><td>1</td><td>27</td><td>&quot;6955a641e0ba93…</td></tr><tr><td>&quot;09a10086a549fa…</td><td>[155, 110, … 110]</td><td>[37963, 33830, … 71079]</td><td>0.0</td><td>0</td><td>24</td><td>&quot;2c29ae653a9282…</td></tr><tr><td>&quot;2ee66d660b16e0…</td><td>[289, 289, … 30]</td><td>[2271, 24744, … 83441]</td><td>0.0</td><td>0</td><td>44</td><td>&quot;0fec0086f1e39e…</td></tr><tr><td>&quot;a6d717dde77801…</td><td>[171, 171, … 110]</td><td>[3455, 77717, … 85852]</td><td>58.470001</td><td>1</td><td>47</td><td>&quot;bc38656f2a2b19…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;a3b8ed7d0788ed…</td><td>[97]</td><td>[68207]</td><td>0.0</td><td>0</td><td>36</td><td>&quot;83d355719a39cc…</td></tr><tr><td>&quot;c30cc75277dac2…</td><td>[320, 309, … 185]</td><td>[55901, 8, … 75628]</td><td>0.0</td><td>0</td><td>26</td><td>&quot;120c8f1333d624…</td></tr><tr><td>&quot;31473511ded7a7…</td><td>[168, 168, … 66]</td><td>[44984, 44987, … 88701]</td><td>197.100006</td><td>1</td><td>50</td><td>&quot;2faae2f724d7a0…</td></tr><tr><td>&quot;66fa41f4d5f338…</td><td>[228, 228, … 139]</td><td>[15674, 15689, … 2247]</td><td>148.949997</td><td>1</td><td>45</td><td>&quot;c8217776877a67…</td></tr><tr><td>&quot;7f3c43999e3b2a…</td><td>[294, 237, … 22]</td><td>[53356, 56612, … 84724]</td><td>98.900002</td><td>1</td><td>26</td><td>&quot;2ccca0c0f059a5…</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# BST TRAINING AND TESTING (FINAL VERSION)","metadata":{}},{"cell_type":"code","source":"import math\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict, Optional\n\n\n# Custom Transformer classes\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8) -> None:\n        super().__init__()\n        self.eps = eps\n        self.gain = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n        return norm * self.gain\n\nclass CustomMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.dropout = dropout\n      \n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        B, T, _ = query.size()\n        qkv = self.in_proj(query)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for multi-head attention\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        attn_mask = self.merge_masks(attn_mask, key_padding_mask, query)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v,\n            attn_mask=attn_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)\n        output = self.out_proj(attn_output)\n        return output\n\n    def merge_masks(self,\n                    attn_mask: Optional[torch.Tensor],\n                    key_padding_mask: Optional[torch.Tensor],\n                    query: torch.Tensor) -> Optional[torch.Tensor]:\n        merged_mask = None\n        batch_size, seq_len, _ = query.shape\n\n        def convert_to_float_mask(mask):\n            if mask.dtype == torch.bool:\n                return mask.float().masked_fill(mask, float(\"-inf\"))\n            return mask\n\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(\n                -1, self.num_heads, -1, -1)\n            merged_mask = convert_to_float_mask(key_padding_mask)\n\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                correct_2d_size = (seq_len, seq_len)\n                if attn_mask.shape != correct_2d_size:\n                    raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, \"\n                                       f\"but should be {correct_2d_size}.\")\n                attn_mask = attn_mask.unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n            elif attn_mask.dim() == 3:\n                correct_3d_size = (batch_size * self.num_heads, seq_len, seq_len)\n                if attn_mask.shape != correct_3d_size:\n                    raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, \"\n                                       f\"but should be {correct_3d_size}.\")\n                attn_mask = attn_mask.view(batch_size, self.num_heads, seq_len, seq_len)\n            else:\n                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n            attn_mask = convert_to_float_mask(attn_mask)\n            if merged_mask is None:\n                merged_mask = attn_mask\n            else:\n                merged_mask = merged_mask + attn_mask\n        return merged_mask\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        embed_dim = config[\"embedding_dim\"]\n        num_heads = config.get(\"heads\", 8)\n        dropout = config[\"transformer_dropout\"]\n        dim_feedforward = config[\"dim_feedforward\"]\n        self.norm_first = config.get(\"norm_first\", False)\n\n        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm1 = RMSNorm(embed_dim)\n        self.norm2 = RMSNorm(embed_dim)\n        self.activation = nn.GELU()\n\n    def _sa_block(self, src, attn_mask=None, key_padding_mask=None):\n        src2 = self.self_attn(src, src, src,\n                              key_padding_mask=key_padding_mask,\n                              attn_mask=attn_mask)\n        return self.dropout1(src2)\n\n    def _ff_block(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        return self.dropout2(src2)\n\n    def forward(self,\n                src: torch.Tensor,\n                src_key_padding_mask: torch.Tensor = None,\n                src_mask: torch.Tensor = None):\n        if self.norm_first:\n            src = src + self._sa_block(self.norm1(src),\n                                        attn_mask=src_mask,\n                                        key_padding_mask=src_key_padding_mask)\n            src = src + self._ff_block(self.norm2(src))\n        else:\n            src = self.norm1(src + self._sa_block(src,\n                                                  attn_mask=src_mask,\n                                                  key_padding_mask=src_key_padding_mask))\n            src = self.norm2(src + self._ff_block(src))\n        return src\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = nn.ModuleList([\n            TransformerEncoderLayer(config)\n            for _ in range(config[\"num_transformer_layers\"])\n        ])\n\n    def forward(self,\n                src,\n                src_key_padding_mask=None,\n                src_mask=None):\n        for layer in self.encoder:\n            src = layer(src,\n                        src_key_padding_mask=src_key_padding_mask,\n                        src_mask=src_mask)\n        return src\n\n\n# Preparing vocabularies and the dataset (without customer_id)\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    1) Ensure each df is a pandas DataFrame.\n    2) Builds dictionary for postal codes.\n    3) Finds max article ID, max day, and max age.\n    \"\"\"\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1\n\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return postal2idx, num_postal, num_articles, max_day, num_age\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n      - postal_code (str)\n      - days_before_lst (list[int])\n      - articles_ids_lst (list[int])\n      - regression_label (float)\n      - classification_label (int) (0 means churn, 1 means not churn)\n      - age (int)\n    Note: customer_id is no longer used.\n    \"\"\"\n    def __init__(self, df, postal2idx: Dict[str, int]):\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        postal_id = self.postal2idx[row['postal_code']]\n        age = int(row['age'])\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n        classification_label = torch.tensor(int(row['classification_label']), dtype=torch.long)\n        return (articles, days, age, postal_id, regression_label, classification_label)\n\n# Custom collate function for variable-length sequences\ndef fixed_length_collate_fn(batch: list[tuple[torch.Tensor, torch.Tensor, int, int, torch.Tensor, torch.Tensor]], \n                           sequence_length: int = 8, padding_value: int = 0) -> tuple[torch.Tensor, ...]:\n    \"\"\"\n    Efficiently pads sequences using PyTorch's pad_sequence and then truncates them to a fixed length.\n    \n    Args:\n        batch: List of tuples where each tuple contains \n              (articles, days, age, postal_id, regression_label, classification_label)\n        sequence_length: Desired length of the sequences\n        padding_value: Value to use for padding sequences\n    \n    Returns:\n        Tuple of tensors: \n          (article_seqs_tensor, day_seqs_tensor, ages_tensor, postal_ids_tensor, reg_labels_tensor, class_labels_tensor)\n    \"\"\"\n    # Unpack the batch\n    article_seqs, day_seqs, ages, postal_ids, reg_labels, class_labels = zip(*batch)\n    \n    # Use pad_sequence for efficient padding (batch_first gives shape: [B, L, ...])\n    article_seqs_tensor = pad_sequence(article_seqs, batch_first=True, padding_value=padding_value)\n    day_seqs_tensor = pad_sequence(day_seqs, batch_first=True, padding_value=padding_value)\n    \n    # Truncate padded tensors to the desired sequence length\n    article_seqs_tensor = article_seqs_tensor[:, :sequence_length]\n    day_seqs_tensor = day_seqs_tensor[:, :sequence_length]\n    \n    # Convert scalar values to tensors\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    reg_labels_tensor = torch.stack(reg_labels, dim=0)\n    class_labels_tensor = torch.stack(class_labels, dim=0)\n    \n    return (\n        article_seqs_tensor,\n        day_seqs_tensor,\n        ages_tensor,\n        postal_ids_tensor,\n        reg_labels_tensor,\n        class_labels_tensor\n    )\n\n# BST model WITHOUT customer_id input\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        postal2idx,\n        # embedding dims\n        article_emb_dim=16,\n        day_emb_dim=8,\n        age_emb_dim=4,\n        postal_emb_dim=4,\n        # transformer config\n        transformer_nhead=2,\n        transformer_ff_dim=64,\n        num_transformer_layers=1,\n        # multi-task config\n        predict_churn=False,  # Flag to enable/disable churn prediction\n        # training\n        learning_rate=0.0005\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','postal2idx'])\n        self.learning_rate = learning_rate\n        self.predict_churn = predict_churn\n\n        # DataFrames and mapping\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.postal2idx = postal2idx\n\n        # Embeddings (customer embedding removed)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # Sequence features: concatenation of article and day embeddings\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n\n        # Custom Transformer setup\n        config = {\n            \"embedding_dim\": self.seq_feature_dim,\n            \"heads\": transformer_nhead,\n            \"transformer_dropout\": 0.2,\n            \"dim_feedforward\": transformer_ff_dim,\n            \"norm_first\": False,\n            \"num_transformer_layers\": num_transformer_layers,\n        }\n        self.transformer = TransformerEncoder(config)\n        self.transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features: only age and postal embeddings are used\n        user_feature_dim = age_emb_dim + postal_emb_dim\n\n        combined_dim = self.transformer_output_dim + user_feature_dim\n\n        # Separate heads for regression and (optional) classification\n        self.regressor_head = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n\n        if self.predict_churn:\n            self.classifier_head = nn.Sequential(\n                nn.Linear(combined_dim, 128),\n                nn.LeakyReLU(),\n                nn.Linear(128, 1)  # single logit for binary classification\n            )\n            self.classification_criterion = nn.BCEWithLogitsLoss()\n\n        self.regression_criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        # Expected tuple: (articles, days, age, postal_id, regression_label, classification_label)\n        articles, days, age, postal_id, regression_label, classification_label = batch\n\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n        # Concatenate to form the sequence features; day_embeds serve as the time/position signal.\n        transformer_input = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n\n        transformer_output = self.transformer(transformer_input)  # (B, L, seq_feature_dim)\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([age_embed, postal_embed], dim=1)\n\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n        return combined_features, regression_label, classification_label\n\n    def forward(self, batch):\n        features, reg_label, class_label = self.encode_input(batch)\n        reg_output = self.regressor_head(features).squeeze(dim=-1)\n        if self.predict_churn:\n            class_output = self.classifier_head(features).squeeze(dim=-1)\n        else:\n            class_output = None\n        return reg_output, class_output, reg_label, class_label\n\n    def training_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        loss = reg_loss\n        self.log(\"train_reg_loss\", reg_loss)\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"train_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"val_reg_loss\", reg_loss)\n        loss = reg_loss\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"val_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"test_reg_loss\", reg_loss)\n        loss = reg_loss\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"test_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df, self.postal2idx)\n\n    def train_dataloader(self):\n     return DataLoader(\n        self.train_dataset,\n        batch_size=128,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=lambda batch: fixed_length_collate_fn(batch, sequence_length=self.hparams.sequence_length)\n     )\n\n    def val_dataloader(self):\n     return DataLoader(\n        self.val_dataset,\n        batch_size=128,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=lambda batch: fixed_length_collate_fn(batch, sequence_length=self.hparams.sequence_length)\n     )\n\n    def test_dataloader(self):\n     return DataLoader(\n        self.test_dataset,\n        batch_size=128,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=lambda batch: fixed_length_collate_fn(batch, sequence_length=self.hparams.sequence_length)\n     )\n\n\n\n# TRAIN AND TEST\n\n\n#\n# 1. for regression only\n# model = BST(\n#     num_articles=num_articles,\n#     max_day=max_day,\n#     num_age=num_age,\n#     num_postal=num_postal,\n#     sequence_length=sequence_length,\n#     train_df=train_df,\n#     val_df=val_df,\n#     test_df=test_df,\n#     postal2idx=postal2idx,\n#     article_emb_dim=16,\n#     day_emb_dim=8,\n#     age_emb_dim=4,\n#     postal_emb_dim=4,\n#     transformer_nhead=2,\n#     transformer_ff_dim=64,\n#     num_transformer_layers=1,\n#     predict_churn=False,\n#     learning_rate=0.0005\n# )\n\n\n# 2. with churn\nmodel = BST(\n    #num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    #user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    #customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    predict_churn=True,   # <--- Enable churn\n    learning_rate=0.0005\n)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:31:41.037745Z","iopub.execute_input":"2025-03-19T10:31:41.038074Z","iopub.status.idle":"2025-03-19T10:35:28.857754Z","shell.execute_reply.started":"2025-03-19T10:31:41.038049Z","shell.execute_reply":"2025-03-19T10:35:28.856716Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee66624adcb24ba6a3560de58a414e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60999682ddbd4712afa0e424b9918c37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m     test_class_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5889449119567871    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     29195.490234375     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m      test_reg_loss      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     29194.90234375      \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">      test_class_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5889449119567871     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      29195.490234375      </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">       test_reg_loss       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      29194.90234375       </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{'test_reg_loss': 29194.90234375,\n  'test_class_loss': 0.5889449119567871,\n  'test_loss': 29195.490234375}]"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"# BASE VERSION","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    1) Ensures each df is a pandas DataFrame (for easy indexing).\n    2) Builds dictionaries to map string IDs (customer_id, postal_code) to integer indices.\n    3) Finds max article ID, max day, and max age so we can define embedding sizes.\n    \"\"\"\n\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n\n    # Combine for global vocabularies\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    # Map string-based customer_id -> integer\n    unique_users = combined['customer_id'].unique()\n    user2idx = {u: i for i, u in enumerate(unique_users)}\n    num_customers = len(user2idx)\n\n    # Map string-based postal_code -> integer\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    # Determine max article ID\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)  # 'lst' is a list of ints\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1  # for embedding\n\n    # Determine max day\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    # Determine max age if we treat age as discrete\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return user2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n    - customer_id (str)\n    - days_before_lst (list[int])\n    - articles_ids_lst (list[int])\n    - regression_label (float)\n    - classification_label (int)  (not used here)\n    - age (int)\n    - postal_code (str)\n    \"\"\"\n    def __init__(self, df, user2idx: Dict[str,int], postal2idx: Dict[str,int]):\n        # Convert to Pandas if Polars\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n\n        # Convert string-based IDs to integer indices\n        user_id = self.user2idx[row['customer_id']]\n        postal_id = self.postal2idx[row['postal_code']]\n\n        age = int(row['age'])  # embedding or numeric\n\n        # articles_ids_lst and days_before_lst are lists of ints\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n\n        return (\n            user_id,\n            articles,\n            days,\n            age,\n            postal_id,\n            regression_label\n        )\n\n\n# CUSTOM COLLATE FUNCTION FOR VARIABLE-LENGTH SEQUENCES\n\n\ndef fixed_length_collate_fn(batch, sequence_length=8):\n    \"\"\"\n    Pads or truncates the 'articles' and 'days' sequences to 'sequence_length'.\n    Each item in the batch is a tuple:\n      (user_id, articles, days, age, postal_id, regression_label)\n    \"\"\"\n    user_ids      = []\n    article_seqs  = []\n    day_seqs      = []\n    ages          = []\n    postal_ids    = []\n    labels        = []\n\n    # 1) Unpack\n    for item in batch:\n        (user_id, articles, days, age, postal_id, label) = item\n        user_ids.append(user_id)\n        article_seqs.append(articles)\n        day_seqs.append(days)\n        ages.append(age)\n        postal_ids.append(postal_id)\n        labels.append(label)\n\n    # 2) Pad or truncate each sequence\n    def pad_or_trunc(seq, desired_length):\n        length = seq.size(0)\n        if length > desired_length:\n            return seq[:desired_length]\n        elif length < desired_length:\n            pad_size = desired_length - length\n            pad = torch.zeros(pad_size, dtype=seq.dtype)\n            return torch.cat([seq, pad], dim=0)\n        else:\n            return seq\n\n    for i in range(len(article_seqs)):\n        article_seqs[i] = pad_or_trunc(article_seqs[i], sequence_length)\n        day_seqs[i] = pad_or_trunc(day_seqs[i], sequence_length)\n\n    # 3) Stack everything\n    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n    article_seqs_tensor = torch.stack(article_seqs, dim=0)  # shape: (batch_size, sequence_length)\n    day_seqs_tensor = torch.stack(day_seqs, dim=0)         # shape: (batch_size, sequence_length)\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    labels_tensor = torch.stack(labels, dim=0)  # shape: (batch_size,)\n\n    return (\n        user_ids_tensor,\n        article_seqs_tensor,\n        day_seqs_tensor,\n        ages_tensor,\n        postal_ids_tensor,\n        labels_tensor\n    )\n\n\n# BST MODEL\nclass PositionalEmbedding(nn.Module):\n    \"\"\"\n    Simple positional embedding that learns a unique embedding per position (0..max_len-1).\n    \"\"\"\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.pe = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, d_model)\n        batch_size, seq_length, _ = x.size()\n        positions = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n        return self.pe(positions)  # (batch_size, seq_length, d_model)\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_customers,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        user2idx,\n        postal2idx,\n        article_emb_dim=16,\n        day_emb_dim=8,\n        customer_emb_dim=16,\n        age_emb_dim=4,\n        postal_emb_dim=4,\n        transformer_nhead=2,\n        learning_rate=0.0005\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','user2idx','postal2idx'])\n        self.learning_rate = learning_rate\n\n        # DataFrames + Mappings\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n        # Embeddings\n        self.embeddings_customer = nn.Embedding(num_customers, customer_emb_dim)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # Sequence dimension\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n        self.positional_embedding = PositionalEmbedding(sequence_length, self.seq_feature_dim)\n\n        # Transformer\n        self.transformer_layer = nn.TransformerEncoderLayer(\n            d_model=self.seq_feature_dim,\n            nhead=transformer_nhead,\n            dropout=0.2\n        )\n\n        # Flattened dimension after transformer\n        transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features dimension\n        user_feature_dim = customer_emb_dim + age_emb_dim + postal_emb_dim\n\n        # Combined dimension\n        combined_dim = transformer_output_dim + user_feature_dim\n\n        # Final regressor\n        self.linear = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n\n        self.criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        user_id, articles, days, age, postal_id, regression_label = batch\n\n        # Sequence embeddings\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n        sequence_features = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n\n        # Positional embeddings\n        pos_embeds = self.positional_embedding(sequence_features)\n        transformer_input = sequence_features + pos_embeds\n\n        # Transformer expects (L, B, d_model)\n        transformer_input = transformer_input.transpose(0, 1)  # (L, B, seq_feature_dim)\n        transformer_output = self.transformer_layer(transformer_input)\n        transformer_output = transformer_output.transpose(0, 1)  # (B, L, seq_feature_dim)\n\n        # Flatten\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n\n        # User features\n        customer_embed = self.embeddings_customer(user_id)\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([customer_embed, age_embed, postal_embed], dim=1)\n\n        # Combine\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n        return combined_features, regression_label\n\n    def forward(self, batch):\n        features, target = self.encode_input(batch)\n        output = self.linear(features)\n        return output.squeeze(), target\n\n    def training_step(self, batch, batch_idx):\n        output, target = self(batch)\n        loss = self.criterion(output, target)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        output, target = self(batch)\n        loss = self.criterion(output, target)\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        output, target = self(batch)\n        loss = self.criterion(output, target)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.user2idx, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df,   self.user2idx, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df,  self.user2idx, self.postal2idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=128,\n            shuffle=True,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n\n# TRAIN AND TEST\n\n\nuser2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age = prepare_vocabularies(\n     train_df, val_df, test_df\n )\n\n\nsequence_length = 8\n\n\nmodel = BST(\n  num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    customer_emb_dim=16,\n    age_emb_dim=4,\n   postal_emb_dim=4,\n    transformer_nhead=2,\n    learning_rate=0.0005\n)\n\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:30.351326Z","iopub.status.idle":"2025-03-19T10:17:30.351722Z","shell.execute_reply":"2025-03-19T10:17:30.351504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# USING ALTERNATIVE TRANSFORMER","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict, Optional\n\n\n# Custom Transformer classes\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8) -> None:\n        super().__init__()\n        self.eps = eps\n        self.gain = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n        return norm * self.gain\n\n\nclass CustomMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.dropout = dropout\n      \n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        B, T, _ = query.size()\n        qkv = self.in_proj(query)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for multi-head\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Merge masks if present\n        attn_mask = self.merge_masks(attn_mask, key_padding_mask, query)\n\n        # Use PyTorch's scaled dot-product attention\n        attn_output = F.scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attn_mask=attn_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,\n        )\n\n        # Reshape back\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)\n        output = self.out_proj(attn_output)\n        return output\n\n    def merge_masks(\n        self,\n        attn_mask: Optional[torch.Tensor],\n        key_padding_mask: Optional[torch.Tensor],\n        query: torch.Tensor,\n    ) -> Optional[torch.Tensor]:\n        merged_mask = None\n        batch_size, seq_len, _ = query.shape\n\n        def convert_to_float_mask(mask):\n            if mask.dtype == torch.bool:\n                return mask.float().masked_fill(mask, float(\"-inf\"))\n            return mask\n\n        # key_padding_mask -> float mask\n        if key_padding_mask is not None:\n            # shape (B, T) -> (B, 1, 1, T) -> expand to (B, num_heads, 1, T)\n            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(\n                -1, self.num_heads, -1, -1\n            )\n            merged_mask = convert_to_float_mask(key_padding_mask)\n\n        # attn_mask -> float mask\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                # shape (T, T) -> (B, num_heads, T, T)\n                correct_2d_size = (seq_len, seq_len)\n                if attn_mask.shape != correct_2d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 2D attn_mask is {attn_mask.shape}, \"\n                        f\"but should be {correct_2d_size}.\"\n                    )\n                attn_mask = attn_mask.unsqueeze(0).expand(\n                    batch_size, self.num_heads, -1, -1\n                )\n            elif attn_mask.dim() == 3:\n                # shape (B*num_heads, T, T) -> (B, num_heads, T, T)\n                correct_3d_size = (batch_size * self.num_heads, seq_len, seq_len)\n                if attn_mask.shape != correct_3d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 3D attn_mask is {attn_mask.shape}, \"\n                        f\"but should be {correct_3d_size}.\"\n                    )\n                attn_mask = attn_mask.view(batch_size, self.num_heads, seq_len, seq_len)\n            else:\n                raise RuntimeError(\n                    f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n                )\n\n            attn_mask = convert_to_float_mask(attn_mask)\n\n            if merged_mask is None:\n                merged_mask = attn_mask\n            else:\n                merged_mask = merged_mask + attn_mask\n\n        return merged_mask\n\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        embed_dim = config[\"embedding_dim\"]\n        num_heads = config.get(\"heads\", 8)\n        dropout = config[\"transformer_dropout\"]\n        dim_feedforward = config[\"dim_feedforward\"]\n        self.norm_first = config.get(\"norm_first\", False)\n\n        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.norm1 = RMSNorm(embed_dim)\n        self.norm2 = RMSNorm(embed_dim)\n        self.activation = nn.GELU()\n\n    def _sa_block(self, src, attn_mask=None, key_padding_mask=None):\n        src2 = self.self_attn(\n            src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n        )\n        return self.dropout1(src2)\n\n    def _ff_block(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        return self.dropout2(src2)\n\n    def forward(\n        self,\n        src: torch.Tensor,\n        src_key_padding_mask: torch.Tensor = None,\n        src_mask: torch.Tensor = None,\n    ):\n        if self.norm_first:\n            # Pre-norm\n            src = src + self._sa_block(\n                self.norm1(src),\n                attn_mask=src_mask,\n                key_padding_mask=src_key_padding_mask,\n            )\n            src = src + self._ff_block(self.norm2(src))\n        else:\n            # Post-norm\n            src = self.norm1(\n                src\n                + self._sa_block(\n                    src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n                )\n            )\n            src = self.norm2(src + self._ff_block(src))\n        return src\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = nn.ModuleList(\n            [\n                TransformerEncoderLayer(config)\n                for _ in range(config[\"num_transformer_layers\"])\n            ]\n        )\n\n    def forward(\n        self,\n        src,\n        src_key_padding_mask=None,\n        src_mask=None,\n    ):\n        \"\"\"\n        src: shape (B, T, E)\n        \"\"\"\n        for layer in self.encoder:\n            src = layer(\n                src, src_key_padding_mask=src_key_padding_mask, src_mask=src_mask\n            )\n        return src\n\n\n\n#  Preparing vocabularies and the dataset classes\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    1) Ensures each df is a pandas DataFrame (for easy indexing).\n    2) Builds dictionaries to map string IDs (customer_id, postal_code) to integer indices.\n    3) Finds max article ID, max day, and max age so we can define embedding sizes.\n    \"\"\"\n\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n\n    # Combine for global vocabularies\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    # Map string-based customer_id -> integer\n    unique_users = combined['customer_id'].unique()\n    user2idx = {u: i for i, u in enumerate(unique_users)}\n    num_customers = len(user2idx)\n\n    # Map string-based postal_code -> integer\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    # Determine max article ID\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)  # 'lst' is a list of ints\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1  # for embedding\n\n    # Determine max day\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    # Determine max age if we treat age as discrete\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return user2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age\n\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n    - customer_id (str)\n    - days_before_lst (list[int])\n    - articles_ids_lst (list[int])\n    - regression_label (float)\n    - classification_label (int)  (not used here)\n    - age (int)\n    - postal_code (str)\n    \"\"\"\n    def __init__(self, df, user2idx: Dict[str,int], postal2idx: Dict[str,int]):\n        # Convert to Pandas if Polars\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n\n        # Convert string-based IDs to integer indices\n        user_id = self.user2idx[row['customer_id']]\n        postal_id = self.postal2idx[row['postal_code']]\n\n        age = int(row['age'])  # embedding or numeric\n\n        # articles_ids_lst and days_before_lst are lists of ints\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n\n        return (\n            user_id,\n            articles,\n            days,\n            age,\n            postal_id,\n            regression_label\n        )\n\n\n# Custom collate function for variable-length sequences\ndef fixed_length_collate_fn(batch, sequence_length=8):\n    \"\"\"\n    Pads or truncates the 'articles' and 'days' sequences to 'sequence_length'.\n    Each item in the batch is a tuple:\n      (user_id, articles, days, age, postal_id, regression_label)\n    \"\"\"\n    user_ids      = []\n    article_seqs  = []\n    day_seqs      = []\n    ages          = []\n    postal_ids    = []\n    labels        = []\n\n    # 1) Unpack\n    for item in batch:\n        (user_id, articles, days, age, postal_id, label) = item\n        user_ids.append(user_id)\n        article_seqs.append(articles)\n        day_seqs.append(days)\n        ages.append(age)\n        postal_ids.append(postal_id)\n        labels.append(label)\n\n    # 2) Pad or truncate each sequence\n    def pad_or_trunc(seq, desired_length):\n        length = seq.size(0)\n        if length > desired_length:\n            return seq[:desired_length]\n        elif length < desired_length:\n            pad_size = desired_length - length\n            pad = torch.zeros(pad_size, dtype=seq.dtype)\n            return torch.cat([seq, pad], dim=0)\n        else:\n            return seq\n\n    for i in range(len(article_seqs)):\n        article_seqs[i] = pad_or_trunc(article_seqs[i], sequence_length)\n        day_seqs[i] = pad_or_trunc(day_seqs[i], sequence_length)\n\n    # 3) Stack everything\n    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n    article_seqs_tensor = torch.stack(article_seqs, dim=0)  # shape: (batch_size, sequence_length)\n    day_seqs_tensor = torch.stack(day_seqs, dim=0)         # shape: (batch_size, sequence_length)\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    labels_tensor = torch.stack(labels, dim=0)  # shape: (batch_size,)\n\n    return (\n        user_ids_tensor,\n        article_seqs_tensor,\n        day_seqs_tensor,\n        ages_tensor,\n        postal_ids_tensor,\n        labels_tensor\n    )\n\n\n\n# BST model with the custom Transformer\n\n\nclass PositionalEmbedding(nn.Module):\n    \"\"\"\n    Simple positional embedding that learns a unique embedding per position (0..max_len-1).\n    \"\"\"\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.pe = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, d_model)\n        batch_size, seq_length, _ = x.size()\n        positions = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n        return self.pe(positions)  # (batch_size, seq_length, d_model)\n\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_customers,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        user2idx,\n        postal2idx,\n        article_emb_dim=16,\n        day_emb_dim=8,\n        customer_emb_dim=16,\n        age_emb_dim=4,\n        postal_emb_dim=4,\n        transformer_nhead=2,\n        transformer_ff_dim=64,     # <-- new hyperparam for the feed-forward layer\n        num_transformer_layers=1,  # <-- how many layers in the custom transformer\n        learning_rate=0.0005\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','user2idx','postal2idx'])\n        self.learning_rate = learning_rate\n\n        # DataFrames + Mappings\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n        # Embeddings\n        self.embeddings_customer = nn.Embedding(num_customers, customer_emb_dim)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # Sequence dimension\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n        self.positional_embedding = PositionalEmbedding(sequence_length, self.seq_feature_dim)\n\n        # -------------------------\n        # Custom Transformer setup\n        # -------------------------\n        config = {\n            \"embedding_dim\": self.seq_feature_dim,\n            \"heads\": transformer_nhead,\n            \"transformer_dropout\": 0.2,\n            \"dim_feedforward\": transformer_ff_dim,\n            \"norm_first\": False,\n            \"num_transformer_layers\": num_transformer_layers,\n        }\n        self.transformer = TransformerEncoder(config)\n\n        # Flattened dimension after transformer\n        self.transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features dimension\n        user_feature_dim = customer_emb_dim + age_emb_dim + postal_emb_dim\n\n        # Combined dimension\n        combined_dim = self.transformer_output_dim + user_feature_dim\n\n        # Final regressor\n        self.linear = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n\n        self.criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        user_id, articles, days, age, postal_id, regression_label = batch\n\n        # Sequence embeddings\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n        sequence_features = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n\n        # Positional embeddings\n        pos_embeds = self.positional_embedding(sequence_features)  # (B, L, seq_feature_dim)\n        transformer_input = sequence_features + pos_embeds         # (B, L, seq_feature_dim)\n\n        # Pass through our custom Transformer (B, L, d_model)\n        transformer_output = self.transformer(transformer_input)   # (B, L, seq_feature_dim)\n\n        # Flatten\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n\n        # User features\n        customer_embed = self.embeddings_customer(user_id)\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([customer_embed, age_embed, postal_embed], dim=1)\n\n        # Combine\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n        return combined_features, regression_label\n\n    def forward(self, batch):\n        features, target = self.encode_input(batch)\n        output = self.linear(features)\n        return output.squeeze(), target\n\n    def training_step(self, batch, batch_idx):\n        output, target = self(batch)\n        loss = self.criterion(output, target)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        output, target = self(batch)\n        loss = self.criterion(output, target)\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        output, target = self(batch)\n        loss = self.criterion(output, target)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.user2idx, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df,   self.user2idx, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df,  self.user2idx, self.postal2idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=128,\n            shuffle=True,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n\n\n# Example usage (Train and Test)\n\n\nsequence_length = 8\nmodel = BST(\n    num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    learning_rate=0.0005\n )\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:30.352672Z","iopub.status.idle":"2025-03-19T10:17:30.353030Z","shell.execute_reply":"2025-03-19T10:17:30.352852Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ADDING CHURN PREDICTION","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict, Optional\n\n\n# Custom Transformer classes \n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8) -> None:\n        super().__init__()\n        self.eps = eps\n        self.gain = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n        return norm * self.gain\n\nclass CustomMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.dropout = dropout\n      \n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        B, T, _ = query.size()\n        qkv = self.in_proj(query)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for multi-head\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Merge masks if present\n        attn_mask = self.merge_masks(attn_mask, key_padding_mask, query)\n\n        # Use PyTorch's scaled dot-product attention\n        attn_output = F.scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attn_mask=attn_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,\n        )\n\n        # Reshape back\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)\n        output = self.out_proj(attn_output)\n        return output\n\n    def merge_masks(\n        self,\n        attn_mask: Optional[torch.Tensor],\n        key_padding_mask: Optional[torch.Tensor],\n        query: torch.Tensor,\n    ) -> Optional[torch.Tensor]:\n        merged_mask = None\n        batch_size, seq_len, _ = query.shape\n\n        def convert_to_float_mask(mask):\n            if mask.dtype == torch.bool:\n                return mask.float().masked_fill(mask, float(\"-inf\"))\n            return mask\n\n        # key_padding_mask -> float mask\n        if key_padding_mask is not None:\n            # shape (B, T) -> (B, 1, 1, T) -> expand to (B, num_heads, 1, T)\n            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(\n                -1, self.num_heads, -1, -1\n            )\n            merged_mask = convert_to_float_mask(key_padding_mask)\n\n        # attn_mask -> float mask\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                # shape (T, T) -> (B, num_heads, T, T)\n                correct_2d_size = (seq_len, seq_len)\n                if attn_mask.shape != correct_2d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 2D attn_mask is {attn_mask.shape}, \"\n                        f\"but should be {correct_2d_size}.\"\n                    )\n                attn_mask = attn_mask.unsqueeze(0).expand(\n                    batch_size, self.num_heads, -1, -1\n                )\n            elif attn_mask.dim() == 3:\n                # shape (B*num_heads, T, T) -> (B, num_heads, T, T)\n                correct_3d_size = (batch_size * self.num_heads, seq_len, seq_len)\n                if attn_mask.shape != correct_3d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 3D attn_mask is {attn_mask.shape}, \"\n                        f\"but should be {correct_3d_size}.\"\n                    )\n                attn_mask = attn_mask.view(batch_size, self.num_heads, seq_len, seq_len)\n            else:\n                raise RuntimeError(\n                    f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n                )\n\n            attn_mask = convert_to_float_mask(attn_mask)\n\n            if merged_mask is None:\n                merged_mask = attn_mask\n            else:\n                merged_mask = merged_mask + attn_mask\n\n        return merged_mask\n\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        embed_dim = config[\"embedding_dim\"]\n        num_heads = config.get(\"heads\", 8)\n        dropout = config[\"transformer_dropout\"]\n        dim_feedforward = config[\"dim_feedforward\"]\n        self.norm_first = config.get(\"norm_first\", False)\n\n        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.norm1 = RMSNorm(embed_dim)\n        self.norm2 = RMSNorm(embed_dim)\n        self.activation = nn.GELU()\n\n    def _sa_block(self, src, attn_mask=None, key_padding_mask=None):\n        src2 = self.self_attn(\n            src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n        )\n        return self.dropout1(src2)\n\n    def _ff_block(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        return self.dropout2(src2)\n\n    def forward(\n        self,\n        src: torch.Tensor,\n        src_key_padding_mask: torch.Tensor = None,\n        src_mask: torch.Tensor = None,\n    ):\n        if self.norm_first:\n            # Pre-norm\n            src = src + self._sa_block(\n                self.norm1(src),\n                attn_mask=src_mask,\n                key_padding_mask=src_key_padding_mask,\n            )\n            src = src + self._ff_block(self.norm2(src))\n        else:\n            # Post-norm\n            src = self.norm1(\n                src\n                + self._sa_block(\n                    src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n                )\n            )\n            src = self.norm2(src + self._ff_block(src))\n        return src\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = nn.ModuleList(\n            [\n                TransformerEncoderLayer(config)\n                for _ in range(config[\"num_transformer_layers\"])\n            ]\n        )\n\n    def forward(\n        self,\n        src,\n        src_key_padding_mask=None,\n        src_mask=None,\n    ):\n        \"\"\"\n        src: shape (B, T, E)\n        \"\"\"\n        for layer in self.encoder:\n            src = layer(\n                src, src_key_padding_mask=src_key_padding_mask, src_mask=src_mask\n            )\n        return src\n\n\n# Preparing vocabularies and the Dataset (with churn)\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    1) Ensures each df is a pandas DataFrame (for easy indexing).\n    2) Builds dictionaries to map string IDs (customer_id, postal_code) to integer indices.\n    3) Finds max article ID, max day, and max age so we can define embedding sizes.\n    \"\"\"\n\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n\n    # Combine for global vocabularies\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    # Map string-based customer_id -> integer\n    unique_users = combined['customer_id'].unique()\n    user2idx = {u: i for i, u in enumerate(unique_users)}\n    num_customers = len(user2idx)\n\n    # Map string-based postal_code -> integer\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    # Determine max article ID\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)  # 'lst' is a list of ints\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1  # for embedding\n\n    # Determine max day\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_df]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    # Determine max age if we treat age as discrete\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return user2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age\n\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n    - customer_id (str)\n    - days_before_lst (list[int])\n    - articles_ids_lst (list[int])\n    - regression_label (float)\n    - classification_label (int)  (0 means churn, 1 means not churn)\n    - age (int)\n    - postal_code (str)\n    \"\"\"\n    def __init__(self, df, user2idx: Dict[str,int], postal2idx: Dict[str,int]):\n        # Convert to Pandas if Polars\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n\n        # Convert string-based IDs to integer indices\n        user_id = self.user2idx[row['customer_id']]\n        postal_id = self.postal2idx[row['postal_code']]\n\n        age = int(row['age'])  # embedding or numeric\n\n        # articles_ids_lst and days_before_lst are lists of ints\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n        classification_label = torch.tensor(int(row['classification_label']), dtype=torch.long)\n\n        # Return 7 items now\n        return (\n            user_id,\n            articles,\n            days,\n            age,\n            postal_id,\n            regression_label,\n            classification_label\n        )\n\n\n# Custom collate function for variable-length sequences\ndef fixed_length_collate_fn(batch, sequence_length=8):\n    \"\"\"\n    Pads or truncates the 'articles' and 'days' sequences to 'sequence_length'.\n    Each item in the batch is a tuple:\n      (user_id, articles, days, age, postal_id, regression_label, classification_label)\n    \"\"\"\n    user_ids      = []\n    article_seqs  = []\n    day_seqs      = []\n    ages          = []\n    postal_ids    = []\n    reg_labels    = []\n    class_labels  = []\n\n    # 1) Unpack\n    for item in batch:\n        (user_id, articles, days, age, postal_id, reg_label, cls_label) = item\n        user_ids.append(user_id)\n        article_seqs.append(articles)\n        day_seqs.append(days)\n        ages.append(age)\n        postal_ids.append(postal_id)\n        reg_labels.append(reg_label)\n        class_labels.append(cls_label)\n\n    # 2) Pad or truncate each sequence\n    def pad_or_trunc(seq, desired_length):\n        length = seq.size(0)\n        if length > desired_length:\n            return seq[:desired_length]\n        elif length < desired_length:\n            pad_size = desired_length - length\n            pad = torch.zeros(pad_size, dtype=seq.dtype)\n            return torch.cat([seq, pad], dim=0)\n        else:\n            return seq\n\n    for i in range(len(article_seqs)):\n        article_seqs[i] = pad_or_trunc(article_seqs[i], sequence_length)\n        day_seqs[i] = pad_or_trunc(day_seqs[i], sequence_length)\n\n    # 3) Stack everything\n    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n    article_seqs_tensor = torch.stack(article_seqs, dim=0)  # shape: (batch_size, sequence_length)\n    day_seqs_tensor = torch.stack(day_seqs, dim=0)         # shape: (batch_size, sequence_length)\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    reg_labels_tensor = torch.stack(reg_labels, dim=0)  # shape: (batch_size,)\n    class_labels_tensor = torch.stack(class_labels, dim=0)  # shape: (batch_size,)\n\n    return (\n        user_ids_tensor,\n        article_seqs_tensor,\n        day_seqs_tensor,\n        ages_tensor,\n        postal_ids_tensor,\n        reg_labels_tensor,\n        class_labels_tensor\n    )\n\n\n\n# BST model with optional Churn Head\n\n\nclass PositionalEmbedding(nn.Module):\n    \"\"\"\n    Simple positional embedding that learns a unique embedding per position (0..max_len-1).\n    \"\"\"\n    def __init__(self, max_len, d_model):\n        super().__init__()\n        self.pe = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        # x: (batch_size, seq_length, d_model)\n        batch_size, seq_length, _ = x.size()\n        positions = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n        return self.pe(positions)  # (batch_size, seq_length, d_model)\n\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_customers,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        user2idx,\n        postal2idx,\n        # embedding dims\n        article_emb_dim=16,\n        day_emb_dim=8,\n        customer_emb_dim=16,\n        age_emb_dim=4,\n        postal_emb_dim=4,\n        # transformer config\n        transformer_nhead=2,\n        transformer_ff_dim=64,\n        num_transformer_layers=1,\n        # multi-task config\n        predict_churn=False,  # <-- Flag to enable or disable churn prediction\n        # training\n        learning_rate=0.0005\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','user2idx','postal2idx'])\n        self.learning_rate = learning_rate\n        self.predict_churn = predict_churn  # store the flag\n\n        # DataFrames + Mappings\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n        # Embeddings\n        self.embeddings_customer = nn.Embedding(num_customers, customer_emb_dim)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # Sequence dimension\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n        self.positional_embedding = PositionalEmbedding(sequence_length, self.seq_feature_dim)\n\n        # -------------------------\n        # Custom Transformer setup\n        # -------------------------\n        config = {\n            \"embedding_dim\": self.seq_feature_dim,\n            \"heads\": transformer_nhead,\n            \"transformer_dropout\": 0.2,\n            \"dim_feedforward\": transformer_ff_dim,\n            \"norm_first\": False,\n            \"num_transformer_layers\": num_transformer_layers,\n        }\n        self.transformer = TransformerEncoder(config)\n\n        # Flattened dimension after transformer\n        self.transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features dimension\n        user_feature_dim = customer_emb_dim + age_emb_dim + postal_emb_dim\n\n        # Combined dimension\n        combined_dim = self.transformer_output_dim + user_feature_dim\n\n        # -------------------------\n        # Separate heads:\n        #   1) Regression (always)\n        #   2) Classification (optional)\n        # -------------------------\n        self.regressor_head = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n\n        if self.predict_churn:\n            # Binary classification head (churn=0, not-churn=1)\n            self.classifier_head = nn.Sequential(\n                nn.Linear(combined_dim, 128),\n                nn.LeakyReLU(),\n                nn.Linear(128, 1)  # single logit for BCE\n            )\n            self.classification_criterion = nn.BCEWithLogitsLoss()\n\n        # MSE for regression\n        self.regression_criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        # We now receive 7 items instead of 6\n        (user_id, articles, days, age, postal_id, regression_label, classification_label) = batch\n\n        # Sequence embeddings\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n        sequence_features = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n\n        # Positional embeddings\n        pos_embeds = self.positional_embedding(sequence_features)  # (B, L, seq_feature_dim)\n        transformer_input = sequence_features + pos_embeds         # (B, L, seq_feature_dim)\n\n        # Pass through our custom Transformer (B, L, d_model)\n        transformer_output = self.transformer(transformer_input)   # (B, L, seq_feature_dim)\n\n        # Flatten\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n\n        # User features\n        customer_embed = self.embeddings_customer(user_id)\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([customer_embed, age_embed, postal_embed], dim=1)\n\n        # Combine\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n        return combined_features, regression_label, classification_label\n\n    def forward(self, batch):\n        features, reg_label, class_label = self.encode_input(batch)\n\n        # 1) Regression output\n        reg_output = self.regressor_head(features).squeeze(dim=-1)\n\n        # 2) Classification output (only if predict_churn=True)\n        if self.predict_churn:\n            class_output = self.classifier_head(features).squeeze(dim=-1)  # logit\n        else:\n            class_output = None\n\n        return reg_output, class_output, reg_label, class_label\n\n    def training_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n\n        # Always compute regression loss\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        loss = reg_loss  # default if churn is off\n\n        self.log(\"train_reg_loss\", reg_loss)\n\n        # If churn is enabled, compute classification loss\n        if self.predict_churn:\n            # BCEWithLogitsLoss expects float targets of 0 or 1\n            class_label = class_label.float()\n            class_loss = self.classification_criterion(class_output, class_label)\n            loss = reg_loss + class_loss  # simple combined loss\n            self.log(\"train_class_loss\", class_loss)\n\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n\n        # Regression\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"val_reg_loss\", reg_loss)\n\n        loss = reg_loss\n\n        # Classification\n        if self.predict_churn:\n            class_label = class_label.float()\n            class_loss = self.classification_criterion(class_output, class_label)\n            self.log(\"val_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n\n        # Regression\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"test_reg_loss\", reg_loss)\n\n        loss = reg_loss\n\n        # Classification\n        if self.predict_churn:\n            class_label = class_label.float()\n            class_loss = self.classification_criterion(class_output, class_label)\n            self.log(\"test_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.user2idx, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df,   self.user2idx, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df,  self.user2idx, self.postal2idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=128,\n            shuffle=True,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n\n# train and test\n\n# do regression only\nmodel = BST(\n    num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    predict_churn=False,  # <--- Disable churn\n    learning_rate=0.0005\n)\n\n\n\n# also predict churn (multi-task: regression + classification):\nmodel = BST(\n    num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    predict_churn=True,   # <--- Enable churn\n    learning_rate=0.0005\n)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:30.354118Z","iopub.status.idle":"2025-03-19T10:17:30.354604Z","shell.execute_reply":"2025-03-19T10:17:30.354460Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# REMOVED ADDITIVE POSITIONAL ENCODING","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict, Optional\n\n\n# custom Transformer class\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8) -> None:\n        super().__init__()\n        self.eps = eps\n        self.gain = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n        return norm * self.gain\n\nclass CustomMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.dropout = dropout\n      \n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        B, T, _ = query.size()\n        qkv = self.in_proj(query)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for multi-head\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        # Merge masks if present\n        attn_mask = self.merge_masks(attn_mask, key_padding_mask, query)\n\n        # Use PyTorch's scaled dot-product attention\n        attn_output = F.scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attn_mask=attn_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,\n        )\n\n        # Reshape back\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)\n        output = self.out_proj(attn_output)\n        return output\n\n    def merge_masks(\n        self,\n        attn_mask: Optional[torch.Tensor],\n        key_padding_mask: Optional[torch.Tensor],\n        query: torch.Tensor,\n    ) -> Optional[torch.Tensor]:\n        merged_mask = None\n        batch_size, seq_len, _ = query.shape\n\n        def convert_to_float_mask(mask):\n            if mask.dtype == torch.bool:\n                return mask.float().masked_fill(mask, float(\"-inf\"))\n            return mask\n\n        # key_padding_mask -> float mask\n        if key_padding_mask is not None:\n            # shape (B, T) -> (B, 1, 1, T) -> expand to (B, num_heads, 1, T)\n            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(\n                -1, self.num_heads, -1, -1\n            )\n            merged_mask = convert_to_float_mask(key_padding_mask)\n\n        # attn_mask -> float mask\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                # shape (T, T) -> (B, num_heads, T, T)\n                correct_2d_size = (seq_len, seq_len)\n                if attn_mask.shape != correct_2d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 2D attn_mask is {attn_mask.shape}, \"\n                        f\"but should be {correct_2d_size}.\"\n                    )\n                attn_mask = attn_mask.unsqueeze(0).expand(\n                    batch_size, self.num_heads, -1, -1\n                )\n            elif attn_mask.dim() == 3:\n                # shape (B*num_heads, T, T) -> (B, num_heads, T, T)\n                correct_3d_size = (batch_size * self.num_heads, seq_len, seq_len)\n                if attn_mask.shape != correct_3d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 3D attn_mask is {attn_mask.shape}, \"\n                        f\"but should be {correct_3d_size}.\"\n                    )\n                attn_mask = attn_mask.view(batch_size, self.num_heads, seq_len, seq_len)\n            else:\n                raise RuntimeError(\n                    f\"attn_mask's dimension {attn_mask.dim()} is not supported\"\n                )\n\n            attn_mask = convert_to_float_mask(attn_mask)\n\n            if merged_mask is None:\n                merged_mask = attn_mask\n            else:\n                merged_mask = merged_mask + attn_mask\n\n        return merged_mask\n\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        embed_dim = config[\"embedding_dim\"]\n        num_heads = config.get(\"heads\", 8)\n        dropout = config[\"transformer_dropout\"]\n        dim_feedforward = config[\"dim_feedforward\"]\n        self.norm_first = config.get(\"norm_first\", False)\n\n        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.norm1 = RMSNorm(embed_dim)\n        self.norm2 = RMSNorm(embed_dim)\n        self.activation = nn.GELU()\n\n    def _sa_block(self, src, attn_mask=None, key_padding_mask=None):\n        src2 = self.self_attn(\n            src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n        )\n        return self.dropout1(src2)\n\n    def _ff_block(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        return self.dropout2(src2)\n\n    def forward(\n        self,\n        src: torch.Tensor,\n        src_key_padding_mask: torch.Tensor = None,\n        src_mask: torch.Tensor = None,\n    ):\n        if self.norm_first:\n            # Pre-norm\n            src = src + self._sa_block(\n                self.norm1(src),\n                attn_mask=src_mask,\n                key_padding_mask=src_key_padding_mask,\n            )\n            src = src + self._ff_block(self.norm2(src))\n        else:\n            # Post-norm\n            src = self.norm1(\n                src\n                + self._sa_block(\n                    src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask\n                )\n            )\n            src = self.norm2(src + self._ff_block(src))\n        return src\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = nn.ModuleList(\n            [\n                TransformerEncoderLayer(config)\n                for _ in range(config[\"num_transformer_layers\"])\n            ]\n        )\n\n    def forward(\n        self,\n        src,\n        src_key_padding_mask=None,\n        src_mask=None,\n    ):\n        \"\"\"\n        src: shape (B, T, E)\n        \"\"\"\n        for layer in self.encoder:\n            src = layer(\n                src, src_key_padding_mask=src_key_padding_mask, src_mask=src_mask\n            )\n        return src\n\n\n\n# Preparing vocabularies and the Dataset (with churn)\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    1) Ensures each df is a pandas DataFrame (for easy indexing).\n    2) Builds dictionaries to map string IDs (customer_id, postal_code) to integer indices.\n    3) Finds max article ID, max day, and max age so we can define embedding sizes.\n    \"\"\"\n\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n\n    # Combine for global vocabularies\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    # Map string-based customer_id -> integer\n    unique_users = combined['customer_id'].unique()\n    user2idx = {u: i for i, u in enumerate(unique_users)}\n    num_customers = len(user2idx)\n\n    # Map string-based postal_code -> integer\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    # Determine max article ID\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)  # 'lst' is a list of ints\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1  # for embedding\n\n    # Determine max day\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    # Determine max age if we treat age as discrete\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return user2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age\n\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n    - customer_id (str)\n    - days_before_lst (list[int])\n    - articles_ids_lst (list[int])\n    - regression_label (float)\n    - classification_label (int)  (0 means churn, 1 means not churn)\n    - age (int)\n    - postal_code (str)\n    \"\"\"\n    def __init__(self, df, user2idx: Dict[str,int], postal2idx: Dict[str,int]):\n        # Convert to Pandas if Polars\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n\n        # Convert string-based IDs to integer indices\n        user_id = self.user2idx[row['customer_id']]\n        postal_id = self.postal2idx[row['postal_code']]\n\n        age = int(row['age'])  # embedding or numeric\n\n        # articles_ids_lst and days_before_lst are lists of ints\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n        classification_label = torch.tensor(int(row['classification_label']), dtype=torch.long)\n\n        return (\n            user_id,\n            articles,\n            days,\n            age,\n            postal_id,\n            regression_label,\n            classification_label\n        )\n\n\n# Custom collate function for variable-length sequences\ndef fixed_length_collate_fn(batch, sequence_length=8):\n    \"\"\"\n    Pads or truncates the 'articles' and 'days' sequences to 'sequence_length'.\n    Each item in the batch is a tuple:\n      (user_id, articles, days, age, postal_id, regression_label, classification_label)\n    \"\"\"\n    user_ids      = []\n    article_seqs  = []\n    day_seqs      = []\n    ages          = []\n    postal_ids    = []\n    reg_labels    = []\n    class_labels  = []\n\n    # 1) Unpack\n    for item in batch:\n        (user_id, articles, days, age, postal_id, reg_label, cls_label) = item\n        user_ids.append(user_id)\n        article_seqs.append(articles)\n        day_seqs.append(days)\n        ages.append(age)\n        postal_ids.append(postal_id)\n        reg_labels.append(reg_label)\n        class_labels.append(cls_label)\n\n    # 2) Pad or truncate each sequence\n    def pad_or_trunc(seq, desired_length):\n        length = seq.size(0)\n        if length > desired_length:\n            return seq[:desired_length]\n        elif length < desired_length:\n            pad_size = desired_length - length\n            pad = torch.zeros(pad_size, dtype=seq.dtype)\n            return torch.cat([seq, pad], dim=0)\n        else:\n            return seq\n\n    for i in range(len(article_seqs)):\n        article_seqs[i] = pad_or_trunc(article_seqs[i], sequence_length)\n        day_seqs[i] = pad_or_trunc(day_seqs[i], sequence_length)\n\n    # 3) Stack everything\n    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n    article_seqs_tensor = torch.stack(article_seqs, dim=0)  # shape: (batch_size, sequence_length)\n    day_seqs_tensor = torch.stack(day_seqs, dim=0)         # shape: (batch_size, sequence_length)\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    reg_labels_tensor = torch.stack(reg_labels, dim=0)  # shape: (batch_size,)\n    class_labels_tensor = torch.stack(class_labels, dim=0)  # shape: (batch_size,)\n\n    return (\n        user_ids_tensor,\n        article_seqs_tensor,\n        day_seqs_tensor,\n        ages_tensor,\n        postal_ids_tensor,\n        reg_labels_tensor,\n        class_labels_tensor\n    )\n\n\n\n# BST model WITHOUT separate pos. embed\n\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_customers,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        user2idx,\n        postal2idx,\n        # embedding dims\n        article_emb_dim=16,\n        day_emb_dim=8,\n        customer_emb_dim=16,\n        age_emb_dim=4,\n        postal_emb_dim=4,\n        # transformer config\n        transformer_nhead=2,\n        transformer_ff_dim=64,\n        num_transformer_layers=1,\n        # multi-task config\n        predict_churn=False,  # <-- Flag to enable or disable churn prediction\n        # training\n        learning_rate=0.0005\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','user2idx','postal2idx'])\n        self.learning_rate = learning_rate\n        self.predict_churn = predict_churn  # store the flag\n\n        # DataFrames + Mappings\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.user2idx = user2idx\n        self.postal2idx = postal2idx\n\n        # Embeddings\n        self.embeddings_customer = nn.Embedding(num_customers, customer_emb_dim)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # We treat \"day_embeds\" as the positional/time feature. \n        # So final dimension of each timestep = article_emb_dim + day_emb_dim\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n\n        # -------------------------\n        # Custom Transformer setup\n        # -------------------------\n        config = {\n            \"embedding_dim\": self.seq_feature_dim,\n            \"heads\": transformer_nhead,\n            \"transformer_dropout\": 0.2,\n            \"dim_feedforward\": transformer_ff_dim,\n            \"norm_first\": False,\n            \"num_transformer_layers\": num_transformer_layers,\n        }\n        self.transformer = TransformerEncoder(config)\n\n        # Flattened dimension after transformer\n        self.transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features dimension\n        user_feature_dim = customer_emb_dim + age_emb_dim + postal_emb_dim\n\n        # Combined dimension\n        combined_dim = self.transformer_output_dim + user_feature_dim\n\n        # -------------------------\n        # Separate heads:\n        #   1) Regression (always)\n        #   2) Classification (optional)\n        # -------------------------\n        self.regressor_head = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n\n        if self.predict_churn:\n            # Binary classification head (churn=0, not-churn=1)\n            self.classifier_head = nn.Sequential(\n                nn.Linear(combined_dim, 128),\n                nn.LeakyReLU(),\n                nn.Linear(128, 1)  # single logit for BCE\n            )\n            self.classification_criterion = nn.BCEWithLogitsLoss()\n\n        # MSE for regression\n        self.regression_criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        \"\"\"\n        Returns:\n          combined_features: (B, combined_dim)\n          regression_label:  (B,)\n          classification_label: (B,)\n        \"\"\"\n        (user_id, articles, days, age, postal_id, regression_label, classification_label) = batch\n\n        # Sequence embeddings\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n\n        # No separate positional embedding. \n        # We treat 'days' as our time/pos feature.\n        transformer_input = torch.cat([article_embeds, day_embeds], dim=-1)  \n        # shape: (B, L, seq_feature_dim)\n\n        # Pass through our custom Transformer\n        transformer_output = self.transformer(transformer_input)   \n        # shape: (B, L, seq_feature_dim)\n\n        # Flatten the sequence dimension\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n\n        # User-level features\n        customer_embed = self.embeddings_customer(user_id)\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([customer_embed, age_embed, postal_embed], dim=1)\n\n        # Combine sequence output + user features\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n\n        return combined_features, regression_label, classification_label\n\n    def forward(self, batch):\n        features, reg_label, class_label = self.encode_input(batch)\n\n        # 1) Regression output\n        reg_output = self.regressor_head(features).squeeze(dim=-1)\n\n        # 2) Classification output (only if predict_churn=True)\n        if self.predict_churn:\n            class_output = self.classifier_head(features).squeeze(dim=-1)  # logit\n        else:\n            class_output = None\n\n        return reg_output, class_output, reg_label, class_label\n\n    def training_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n\n        # Always compute regression loss\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        loss = reg_loss  # default if churn is off\n        self.log(\"train_reg_loss\", reg_loss)\n\n        # If churn is enabled, compute classification loss\n        if self.predict_churn:\n            class_label = class_label.float()  # BCE expects float\n            class_loss = self.classification_criterion(class_output, class_label)\n            loss = reg_loss + class_loss  # combine them\n            self.log(\"train_class_loss\", class_loss)\n\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n\n        # Regression\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"val_reg_loss\", reg_loss)\n        loss = reg_loss\n\n        # Classification\n        if self.predict_churn:\n            class_label = class_label.float()\n            class_loss = self.classification_criterion(class_output, class_label)\n            self.log(\"val_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n\n        # Regression\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"test_reg_loss\", reg_loss)\n        loss = reg_loss\n\n        # Classification\n        if self.predict_churn:\n            class_label = class_label.float()\n            class_loss = self.classification_criterion(class_output, class_label)\n            self.log(\"test_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.user2idx, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df,   self.user2idx, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df,  self.user2idx, self.postal2idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=128,\n            shuffle=True,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n\n\n# Train and Test\n\n\nmodel = BST(\n    num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    predict_churn=True,   # <--- Enable churn\n    learning_rate=0.0005\n)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:30.358056Z","iopub.status.idle":"2025-03-19T10:17:30.358379Z","shell.execute_reply":"2025-03-19T10:17:30.358214Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# REMOVED customer_id","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict, Optional\n\n\n# Custom Transformer Classes\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8) -> None:\n        super().__init__()\n        self.eps = eps\n        self.gain = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n        return norm * self.gain\n\nclass CustomMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.dropout = dropout\n      \n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        B, T, _ = query.size()\n        qkv = self.in_proj(query)\n        q, k, v = qkv.chunk(3, dim=-1)\n\n        # Reshape for multi-head attention\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n\n        attn_mask = self.merge_masks(attn_mask, key_padding_mask, query)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v,\n            attn_mask=attn_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)\n        output = self.out_proj(attn_output)\n        return output\n\n    def merge_masks(self,\n                    attn_mask: Optional[torch.Tensor],\n                    key_padding_mask: Optional[torch.Tensor],\n                    query: torch.Tensor) -> Optional[torch.Tensor]:\n        merged_mask = None\n        batch_size, seq_len, _ = query.shape\n\n        def convert_to_float_mask(mask):\n            if mask.dtype == torch.bool:\n                return mask.float().masked_fill(mask, float(\"-inf\"))\n            return mask\n\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(\n                -1, self.num_heads, -1, -1)\n            merged_mask = convert_to_float_mask(key_padding_mask)\n\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                correct_2d_size = (seq_len, seq_len)\n                if attn_mask.shape != correct_2d_size:\n                    raise RuntimeError(f\"The shape of the 2D attn_mask is {attn_mask.shape}, \"\n                                       f\"but should be {correct_2d_size}.\")\n                attn_mask = attn_mask.unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n            elif attn_mask.dim() == 3:\n                correct_3d_size = (batch_size * self.num_heads, seq_len, seq_len)\n                if attn_mask.shape != correct_3d_size:\n                    raise RuntimeError(f\"The shape of the 3D attn_mask is {attn_mask.shape}, \"\n                                       f\"but should be {correct_3d_size}.\")\n                attn_mask = attn_mask.view(batch_size, self.num_heads, seq_len, seq_len)\n            else:\n                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n            attn_mask = convert_to_float_mask(attn_mask)\n            if merged_mask is None:\n                merged_mask = attn_mask\n            else:\n                merged_mask = merged_mask + attn_mask\n        return merged_mask\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        embed_dim = config[\"embedding_dim\"]\n        num_heads = config.get(\"heads\", 8)\n        dropout = config[\"transformer_dropout\"]\n        dim_feedforward = config[\"dim_feedforward\"]\n        self.norm_first = config.get(\"norm_first\", False)\n\n        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm1 = RMSNorm(embed_dim)\n        self.norm2 = RMSNorm(embed_dim)\n        self.activation = nn.GELU()\n\n    def _sa_block(self, src, attn_mask=None, key_padding_mask=None):\n        src2 = self.self_attn(src, src, src,\n                              key_padding_mask=key_padding_mask,\n                              attn_mask=attn_mask)\n        return self.dropout1(src2)\n\n    def _ff_block(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        return self.dropout2(src2)\n\n    def forward(self,\n                src: torch.Tensor,\n                src_key_padding_mask: torch.Tensor = None,\n                src_mask: torch.Tensor = None):\n        if self.norm_first:\n            src = src + self._sa_block(self.norm1(src),\n                                        attn_mask=src_mask,\n                                        key_padding_mask=src_key_padding_mask)\n            src = src + self._ff_block(self.norm2(src))\n        else:\n            src = self.norm1(src + self._sa_block(src,\n                                                  attn_mask=src_mask,\n                                                  key_padding_mask=src_key_padding_mask))\n            src = self.norm2(src + self._ff_block(src))\n        return src\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = nn.ModuleList([\n            TransformerEncoderLayer(config)\n            for _ in range(config[\"num_transformer_layers\"])\n        ])\n\n    def forward(self,\n                src,\n                src_key_padding_mask=None,\n                src_mask=None):\n        for layer in self.encoder:\n            src = layer(src,\n                        src_key_padding_mask=src_key_padding_mask,\n                        src_mask=src_mask)\n        return src\n\n\n# Preparing vocabularies and the dataset (without customer_id)\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    1) Ensure each df is a pandas DataFrame.\n    2) Builds dictionary for postal codes.\n    3) Finds max article ID, max day, and max age.\n    \"\"\"\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1\n\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return postal2idx, num_postal, num_articles, max_day, num_age\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n      - postal_code (str)\n      - days_before_lst (list[int])\n      - articles_ids_lst (list[int])\n      - regression_label (float)\n      - classification_label (int) (0 means churn, 1 means not churn)\n      - age (int)\n    Note: customer_id is no longer used.\n    \"\"\"\n    def __init__(self, df, postal2idx: Dict[str, int]):\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        postal_id = self.postal2idx[row['postal_code']]\n        age = int(row['age'])\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n        classification_label = torch.tensor(int(row['classification_label']), dtype=torch.long)\n        return (articles, days, age, postal_id, regression_label, classification_label)\n\n# Custom collate function for variable-length sequences (without customer_id)\ndef fixed_length_collate_fn(batch, sequence_length=8):\n    articles_list, days_list, ages, postal_ids, reg_labels, class_labels = [], [], [], [], [], []\n    for item in batch:\n        articles, days, age, postal_id, reg_label, cls_label = item\n        articles_list.append(articles)\n        days_list.append(days)\n        ages.append(age)\n        postal_ids.append(postal_id)\n        reg_labels.append(reg_label)\n        class_labels.append(cls_label)\n\n    def pad_or_trunc(seq, desired_length):\n        length = seq.size(0)\n        if length > desired_length:\n            return seq[:desired_length]\n        elif length < desired_length:\n            pad = torch.zeros(desired_length - length, dtype=seq.dtype)\n            return torch.cat([seq, pad], dim=0)\n        else:\n            return seq\n\n    for i in range(len(articles_list)):\n        articles_list[i] = pad_or_trunc(articles_list[i], sequence_length)\n        days_list[i] = pad_or_trunc(days_list[i], sequence_length)\n\n    articles_tensor = torch.stack(articles_list, dim=0)\n    days_tensor = torch.stack(days_list, dim=0)\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    reg_labels_tensor = torch.stack(reg_labels, dim=0)\n    class_labels_tensor = torch.stack(class_labels, dim=0)\n\n    return (articles_tensor, days_tensor, ages_tensor, postal_ids_tensor, reg_labels_tensor, class_labels_tensor)\n\n\n# BST model WITHOUT customer_id input\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        postal2idx,\n        # embedding dims\n        article_emb_dim=16,\n        day_emb_dim=8,\n        age_emb_dim=4,\n        postal_emb_dim=4,\n        # transformer config\n        transformer_nhead=2,\n        transformer_ff_dim=64,\n        num_transformer_layers=1,\n        # multi-task config\n        predict_churn=False,  # Flag to enable/disable churn prediction\n        # training\n        learning_rate=0.0005\n    ):\n        super().__init__()\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','postal2idx'])\n        self.learning_rate = learning_rate\n        self.predict_churn = predict_churn\n\n        # DataFrames and mapping\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.postal2idx = postal2idx\n\n        # Embeddings (customer embedding removed)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # Sequence features: concatenation of article and day embeddings\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n\n        # Custom Transformer setup\n        config = {\n            \"embedding_dim\": self.seq_feature_dim,\n            \"heads\": transformer_nhead,\n            \"transformer_dropout\": 0.2,\n            \"dim_feedforward\": transformer_ff_dim,\n            \"norm_first\": False,\n            \"num_transformer_layers\": num_transformer_layers,\n        }\n        self.transformer = TransformerEncoder(config)\n        self.transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features: only age and postal embeddings are used\n        user_feature_dim = age_emb_dim + postal_emb_dim\n\n        combined_dim = self.transformer_output_dim + user_feature_dim\n\n        # Separate heads for regression and (optional) classification\n        self.regressor_head = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n\n        if self.predict_churn:\n            self.classifier_head = nn.Sequential(\n                nn.Linear(combined_dim, 128),\n                nn.LeakyReLU(),\n                nn.Linear(128, 1)  # single logit for binary classification\n            )\n            self.classification_criterion = nn.BCEWithLogitsLoss()\n\n        self.regression_criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        # Expected tuple: (articles, days, age, postal_id, regression_label, classification_label)\n        articles, days, age, postal_id, regression_label, classification_label = batch\n\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n        # Concatenate to form the sequence features; day_embeds serve as the time/position signal.\n        transformer_input = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n\n        transformer_output = self.transformer(transformer_input)  # (B, L, seq_feature_dim)\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([age_embed, postal_embed], dim=1)\n\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n        return combined_features, regression_label, classification_label\n\n    def forward(self, batch):\n        features, reg_label, class_label = self.encode_input(batch)\n        reg_output = self.regressor_head(features).squeeze(dim=-1)\n        if self.predict_churn:\n            class_output = self.classifier_head(features).squeeze(dim=-1)\n        else:\n            class_output = None\n        return reg_output, class_output, reg_label, class_label\n\n    def training_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        loss = reg_loss\n        self.log(\"train_reg_loss\", reg_loss)\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"train_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"val_reg_loss\", reg_loss)\n        loss = reg_loss\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"val_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"test_reg_loss\", reg_loss)\n        loss = reg_loss\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"test_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df, self.postal2idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=128,\n            shuffle=True,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n        )\n\n\n# train and test\n\nmodel = BST(\n    #num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    #user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    #customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    predict_churn=True,   # <--- Enable churn\n    learning_rate=0.0005\n)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:17:30.360894Z","iopub.status.idle":"2025-03-19T10:17:30.361166Z","shell.execute_reply":"2025-03-19T10:17:30.361049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ALTERNATIVE VERSION (WITH CUSTOM COLLATOR)","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom typing import List, Dict, Optional\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-8) -> None:\n        super().__init__()\n        self.eps = eps\n        self.gain = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n        return norm * self.gain\n\nclass CustomMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.in_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.dropout = dropout\n      \n    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n        B, T, _ = query.size()\n        qkv = self.in_proj(query)\n        q, k, v = qkv.chunk(3, dim=-1)\n        # Reshape for multi-head attention\n        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n        attn_mask = self.merge_masks(attn_mask, key_padding_mask, query)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v,\n            attn_mask=attn_mask,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=False,\n        )\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, self.embed_dim)\n        output = self.out_proj(attn_output)\n        return output\n\n    def merge_masks(\n        self,\n        attn_mask: Optional[torch.Tensor],\n        key_padding_mask: Optional[torch.Tensor],\n        query: torch.Tensor,\n    ) -> Optional[torch.Tensor]:\n        merged_mask = None\n        batch_size, seq_len, _ = query.shape\n\n        def convert_to_float_mask(mask):\n            if mask.dtype == torch.bool:\n                return mask.float().masked_fill(mask, float(\"-inf\"))\n            return mask\n\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.view(batch_size, 1, 1, seq_len).expand(\n                -1, self.num_heads, -1, -1)\n            merged_mask = convert_to_float_mask(key_padding_mask)\n\n        if attn_mask is not None:\n            if attn_mask.dim() == 2:\n                correct_2d_size = (seq_len, seq_len)\n                if attn_mask.shape != correct_2d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.\"\n                    )\n                attn_mask = attn_mask.unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)\n            elif attn_mask.dim() == 3:\n                correct_3d_size = (batch_size * self.num_heads, seq_len, seq_len)\n                if attn_mask.shape != correct_3d_size:\n                    raise RuntimeError(\n                        f\"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.\"\n                    )\n                attn_mask = attn_mask.view(batch_size, self.num_heads, seq_len, seq_len)\n            else:\n                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n            attn_mask = convert_to_float_mask(attn_mask)\n            if merged_mask is None:\n                merged_mask = attn_mask\n            else:\n                merged_mask = merged_mask + attn_mask\n\n        return merged_mask\n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, config: dict):\n        super().__init__()\n        embed_dim = config[\"embedding_dim\"]\n        num_heads = config.get(\"heads\", 8)\n        dropout = config[\"transformer_dropout\"]\n        dim_feedforward = config[\"dim_feedforward\"]\n        self.norm_first = config.get(\"norm_first\", False)\n        self.self_attn = CustomMultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout1 = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.dropout2 = nn.Dropout(dropout)\n        self.norm1 = RMSNorm(embed_dim)\n        self.norm2 = RMSNorm(embed_dim)\n        self.activation = nn.GELU()\n\n    def _sa_block(self, src, attn_mask=None, key_padding_mask=None):\n        src2 = self.self_attn(src, src, src,\n                              key_padding_mask=key_padding_mask,\n                              attn_mask=attn_mask)\n        return self.dropout1(src2)\n\n    def _ff_block(self, src):\n        src2 = self.linear2(self.dropout2(self.activation(self.linear1(src))))\n        return self.dropout2(src2)\n\n    def forward(self,\n                src: torch.Tensor,\n                src_key_padding_mask: torch.Tensor = None,\n                src_mask: torch.Tensor = None):\n        if self.norm_first:\n            src = src + self._sa_block(self.norm1(src),\n                                        attn_mask=src_mask,\n                                        key_padding_mask=src_key_padding_mask)\n            src = src + self._ff_block(self.norm2(src))\n        else:\n            src = self.norm1(src + self._sa_block(src,\n                                                  attn_mask=src_mask,\n                                                  key_padding_mask=src_key_padding_mask))\n            src = self.norm2(src + self._ff_block(src))\n        return src\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = nn.ModuleList([\n            TransformerEncoderLayer(config)\n            for _ in range(config[\"num_transformer_layers\"])\n        ])\n\n    def forward(self,\n                src,\n                src_key_padding_mask=None,\n                src_mask=None):\n        for layer in self.encoder:\n            src = layer(src,\n                        src_key_padding_mask=src_key_padding_mask,\n                        src_mask=src_mask)\n        return src\n\n\ndef prepare_vocabularies(train_df, val_df, test_df):\n    \"\"\"\n    Ensures each df is a pandas DataFrame, builds a dictionary for postal codes,\n    and finds max article ID, max day, and max age.\n    \"\"\"\n    def to_pandas_if_polars(df):\n        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n\n    train_pd = to_pandas_if_polars(train_df)\n    val_pd   = to_pandas_if_polars(val_df)\n    test_pd  = to_pandas_if_polars(test_df)\n    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n\n    unique_postals = combined['postal_code'].unique()\n    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n    num_postal = len(postal2idx)\n\n    all_articles = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['articles_ids_lst']:\n            all_articles.extend(lst)\n    max_article_id = max(all_articles)\n    num_articles = max_article_id + 1\n\n    all_days = []\n    for df_pd in [train_pd, val_pd, test_pd]:\n        for lst in df_pd['days_before_lst']:\n            all_days.extend(lst)\n    max_day = max(all_days)\n\n    max_age = combined['age'].max()\n    num_age = max_age + 1\n\n    return postal2idx, num_postal, num_articles, max_day, num_age\n\nclass CustomerDataset(Dataset):\n    \"\"\"\n    Expects columns:\n      - postal_code (str)\n      - days_before_lst (list[int])\n      - articles_ids_lst (list[int])\n      - regression_label (float)\n      - classification_label (int) (0 means churn, 1 means not churn)\n      - age (int)\n    Note: customer_id is not used.\n    \"\"\"\n    def __init__(self, df, postal2idx: Dict[str, int]):\n        if not hasattr(df, \"iloc\"):\n            df = df.to_pandas()\n        self.data = df\n        self.postal2idx = postal2idx\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        postal_id = self.postal2idx[row['postal_code']]\n        age = int(row['age'])\n        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n        classification_label = torch.tensor(int(row['classification_label']), dtype=torch.long)\n        return (articles, days, age, postal_id, regression_label, classification_label)\n\n\ndef fixed_length_collate_fn(batch: list[tuple[torch.Tensor, torch.Tensor, int, int, torch.Tensor, torch.Tensor]],\n                           sequence_length: int = 8, padding_value: int = 0) -> tuple[torch.Tensor, ...]:\n    \"\"\"\n    Efficiently pads sequences using PyTorch's pad_sequence and then truncates them.\n    \n    Args:\n        batch: List of tuples where each tuple contains \n              (articles, days, age, postal_id, regression_label, classification_label)\n        sequence_length: Desired length of the sequences\n        padding_value: Value to use for padding sequences\n    \n    Returns:\n        Tuple of tensors: \n          (article_seqs_tensor, day_seqs_tensor, ages_tensor, postal_ids_tensor, reg_labels_tensor, class_labels_tensor)\n    \"\"\"\n    article_seqs, day_seqs, ages, postal_ids, reg_labels, class_labels = zip(*batch)\n    article_seqs_tensor = pad_sequence(article_seqs, batch_first=True, padding_value=padding_value)\n    day_seqs_tensor = pad_sequence(day_seqs, batch_first=True, padding_value=padding_value)\n    # Truncate to the desired sequence length\n    article_seqs_tensor = article_seqs_tensor[:, :sequence_length]\n    day_seqs_tensor = day_seqs_tensor[:, :sequence_length]\n    ages_tensor = torch.tensor(ages, dtype=torch.long)\n    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n    reg_labels_tensor = torch.stack(reg_labels, dim=0)\n    class_labels_tensor = torch.stack(class_labels, dim=0)\n    return (\n        article_seqs_tensor,\n        day_seqs_tensor,\n        ages_tensor,\n        postal_ids_tensor,\n        reg_labels_tensor,\n        class_labels_tensor\n    )\n\n\nclass BST(pl.LightningModule):\n    def __init__(\n        self,\n        num_articles,\n        max_day,\n        num_age,\n        num_postal,\n        sequence_length,\n        train_df,\n        val_df,\n        test_df,\n        postal2idx,\n        # Embedding dimensions\n        article_emb_dim: int = 16,\n        day_emb_dim: int = 8,\n        age_emb_dim: int = 4,\n        postal_emb_dim: int = 4,\n        # Transformer config\n        transformer_nhead: int = 2,\n        transformer_ff_dim: int = 64,\n        num_transformer_layers: int = 1,\n        # Multi-task config\n        predict_churn: bool = False,\n        # Training\n        learning_rate: float = 0.0005\n    ):\n        super().__init__()\n        # Save hyperparameters (including sequence_length)\n        self.save_hyperparameters(ignore=['train_df','val_df','test_df','postal2idx'])\n        self.learning_rate = learning_rate\n        self.predict_churn = predict_churn\n\n        # DataFrames and mapping\n        self.train_df = train_df\n        self.val_df   = val_df\n        self.test_df  = test_df\n        self.postal2idx = postal2idx\n\n        # Embeddings (customer_id removed)\n        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n\n        # Sequence features: concatenation of article and day embeddings\n        self.seq_feature_dim = article_emb_dim + day_emb_dim\n\n        # Custom Transformer setup\n        config = {\n            \"embedding_dim\": self.seq_feature_dim,\n            \"heads\": transformer_nhead,\n            \"transformer_dropout\": 0.2,\n            \"dim_feedforward\": transformer_ff_dim,\n            \"norm_first\": False,\n            \"num_transformer_layers\": num_transformer_layers,\n        }\n        self.transformer = TransformerEncoder(config)\n        self.transformer_output_dim = sequence_length * self.seq_feature_dim\n\n        # User features: only age and postal embeddings are used\n        user_feature_dim = age_emb_dim + postal_emb_dim\n        combined_dim = self.transformer_output_dim + user_feature_dim\n\n        # Separate heads for regression and optional classification\n        self.regressor_head = nn.Sequential(\n            nn.Linear(combined_dim, 512),\n            nn.LeakyReLU(),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(),\n            nn.Linear(256, 1)\n        )\n        if self.predict_churn:\n            self.classifier_head = nn.Sequential(\n                nn.Linear(combined_dim, 128),\n                nn.LeakyReLU(),\n                nn.Linear(128, 1)  # single logit for binary classification\n            )\n            self.classification_criterion = nn.BCEWithLogitsLoss()\n        self.regression_criterion = nn.MSELoss()\n\n    def encode_input(self, batch):\n        # Expecting: (articles, days, age, postal_id, regression_label, classification_label)\n        articles, days, age, postal_id, regression_label, classification_label = batch\n        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n        # Concatenate article and day embeddings as the sequence features\n        transformer_input = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n        transformer_output = self.transformer(transformer_input)  # (B, L, seq_feature_dim)\n        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n        age_embed = self.embeddings_age(age)\n        postal_embed = self.embeddings_postal(postal_id)\n        user_features = torch.cat([age_embed, postal_embed], dim=1)\n        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n        return combined_features, regression_label, classification_label\n\n    def forward(self, batch):\n        features, reg_label, class_label = self.encode_input(batch)\n        reg_output = self.regressor_head(features).squeeze(dim=-1)\n        if self.predict_churn:\n            class_output = self.classifier_head(features).squeeze(dim=-1)\n        else:\n            class_output = None\n        return reg_output, class_output, reg_label, class_label\n\n    def training_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        loss = reg_loss\n        self.log(\"train_reg_loss\", reg_loss)\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"train_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"val_reg_loss\", reg_loss)\n        loss = reg_loss\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"val_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        reg_output, class_output, reg_label, class_label = self(batch)\n        reg_loss = self.regression_criterion(reg_output, reg_label)\n        self.log(\"test_reg_loss\", reg_loss)\n        loss = reg_loss\n        if self.predict_churn:\n            class_loss = self.classification_criterion(class_output, class_label.float())\n            self.log(\"test_class_loss\", class_loss)\n            loss = reg_loss + class_loss\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n\n    def setup(self, stage=None):\n        if stage == \"fit\" or stage is None:\n            self.train_dataset = CustomerDataset(self.train_df, self.postal2idx)\n            self.val_dataset   = CustomerDataset(self.val_df, self.postal2idx)\n        if stage == \"test\" or stage is None:\n            self.test_dataset  = CustomerDataset(self.test_df, self.postal2idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size=128,\n            shuffle=True,\n            num_workers=4,\n            collate_fn=lambda batch: fixed_length_collate_fn(batch, sequence_length=self.hparams.sequence_length)\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda batch: fixed_length_collate_fn(batch, sequence_length=self.hparams.sequence_length)\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=128,\n            shuffle=False,\n            num_workers=4,\n            collate_fn=lambda batch: fixed_length_collate_fn(batch, sequence_length=self.hparams.sequence_length)\n        )\npostal2idx, num_postal, num_articles, max_day, num_age = prepare_vocabularies(train_df, val_df, test_df)\n\n    # Define the desired sequence length\nsequence_length = 8        \nmodel = BST(\n    #num_customers=num_customers,\n    num_articles=num_articles,\n    max_day=max_day,\n    num_age=num_age,\n    num_postal=num_postal,\n    sequence_length=sequence_length,\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    #user2idx=user2idx,\n    postal2idx=postal2idx,\n    article_emb_dim=16,\n    day_emb_dim=8,\n    #customer_emb_dim=16,\n    age_emb_dim=4,\n    postal_emb_dim=4,\n    transformer_nhead=2,\n    transformer_ff_dim=64,\n    num_transformer_layers=1,\n    predict_churn=True,   # <--- Enable churn\n    learning_rate=0.0005\n)\n\ntrainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\ntrainer.fit(model)\ntrainer.test(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GET ENVIRONMENT ","metadata":{}},{"cell_type":"code","source":"!pip freeze > requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:36:31.565379Z","iopub.execute_input":"2025-03-19T10:36:31.565814Z","iopub.status.idle":"2025-03-19T10:36:33.155288Z","shell.execute_reply.started":"2025-03-19T10:36:31.565776Z","shell.execute_reply":"2025-03-19T10:36:33.154293Z"}},"outputs":[],"execution_count":18}]}