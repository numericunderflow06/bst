{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:05:02.285962Z",
     "iopub.status.busy": "2025-03-06T11:05:02.285757Z",
     "iopub.status.idle": "2025-03-06T11:06:00.776627Z",
     "shell.execute_reply": "2025-03-06T11:06:00.775569Z",
     "shell.execute_reply.started": "2025-03-06T11:05:02.285943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '/kaggle/input/hm-dataset/requirements1.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r /kaggle/input/hm-dataset/requirements1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:00.777979Z",
     "iopub.status.busy": "2025-03-06T11:06:00.777666Z",
     "iopub.status.idle": "2025-03-06T11:06:00.784241Z",
     "shell.execute_reply": "2025-03-06T11:06:00.783245Z",
     "shell.execute_reply.started": "2025-03-06T11:06:00.777955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class StatelessRandomGenerator:\n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "\n",
    "    def set_seed(self, new_seed):\n",
    "        self.seed = new_seed\n",
    "\n",
    "    def random(self, size=None):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        return rng.random(size)\n",
    "\n",
    "    def integers(self, low, high=None, size=None):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        return rng.integers(low, high, size)\n",
    "\n",
    "    def choice(self, a, size=None, replace=True, p=None):\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        return rng.choice(a, size, replace, p)\n",
    "\n",
    "\n",
    "global_rng = StatelessRandomGenerator(42)\n",
    "\n",
    "\n",
    "def set_global_seed(new_seed):\n",
    "    global_rng.set_seed(new_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:00.785359Z",
     "iopub.status.busy": "2025-03-06T11:06:00.785017Z",
     "iopub.status.idle": "2025-03-06T11:06:04.048755Z",
     "shell.execute_reply": "2025-03-06T11:06:04.047801Z",
     "shell.execute_reply.started": "2025-03-06T11:06:00.785323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def wmape_metric(pred: torch.Tensor, true: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sum(torch.abs(pred - true), dim=0) / torch.sum(true, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:04.050713Z",
     "iopub.status.busy": "2025-03-06T11:06:04.050294Z",
     "iopub.status.idle": "2025-03-06T11:06:04.193799Z",
     "shell.execute_reply": "2025-03-06T11:06:04.193154Z",
     "shell.execute_reply.started": "2025-03-06T11:06:04.050686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "#from data_processing.utils.stateless_rng import global_rng\n",
    "\n",
    "def filter_purchases_purchases_per_month_pl(\n",
    "    df_pl: pl.DataFrame, train_end: datetime.date, group_by_channel_id: bool = False\n",
    "):\n",
    "    \"\"\"Filters extreme customers and groups purchases by date and optionally by sales channel.\n",
    "\n",
    "    This function:\n",
    "    1. Groups transactions by customer, date, and optionally sales channel\n",
    "    2. Identifies extreme customers based on the 99th percentile of total items purchased\n",
    "    3. Removes these customers from the dataset\n",
    "\n",
    "    Args:\n",
    "        df_pl (pl.DataFrame): Input transaction dataframe containing:\n",
    "            - customer_id: Customer identifier\n",
    "            - date: Transaction date\n",
    "            - article_id: Product identifier\n",
    "            - price: Transaction price\n",
    "            - sales_channel_id: Sales channel identifier\n",
    "        train_end (datetime.date): End date for training period.\n",
    "        group_by_channel_id (bool, optional): Whether to group transactions by sales channel. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n",
    "            - grouped_df: Grouped transaction data with columns:\n",
    "                - customer_id, date, [sales_channel_id], article_ids, total_price, prices, num_items\n",
    "            - extreme_customers: DataFrame of customers identified as outliers based on purchase behavior\n",
    "\n",
    "    Notes:\n",
    "        Extreme customers are identified using the 99th percentile of total items purchased\n",
    "        during the training period.\n",
    "    \"\"\"\n",
    "    # Used for multi variate time series\n",
    "    if group_by_channel_id:\n",
    "        grouped_df = (\n",
    "            df_pl.lazy()\n",
    "            .group_by([\"customer_id\", \"date\", \"sales_channel_id\"])\n",
    "            .agg(\n",
    "                [\n",
    "                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n",
    "                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n",
    "                    pl.col(\"price\").explode().alias(\"prices\"),\n",
    "                ]\n",
    "            )\n",
    "            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n",
    "        )\n",
    "    else:\n",
    "        grouped_df = (\n",
    "            df_pl.lazy()\n",
    "            .group_by([\"customer_id\", \"date\"])\n",
    "            .agg(\n",
    "                [\n",
    "                    pl.col(\"article_id\").explode().alias(\"article_ids\"),\n",
    "                    pl.col(\"price\").sum().round(2).alias(\"total_price\"),\n",
    "                    pl.col(\"sales_channel_id\").explode().alias(\"sales_channel_ids\"),\n",
    "                    pl.col(\"price\").explode().alias(\"prices\"),\n",
    "                ]\n",
    "            )\n",
    "            .with_columns(pl.col(\"article_ids\").list.len().alias(\"num_items\"))\n",
    "        )\n",
    "\n",
    "    # Only remove customers with extreme purchases in train period\n",
    "    customers_summary = (\n",
    "        df_pl.lazy()\n",
    "        .filter(pl.col(\"date\") < train_end)\n",
    "        .group_by(\"customer_id\")\n",
    "        .agg(\n",
    "            [\n",
    "                pl.col(\"date\").n_unique().alias(\"total_purchases\"),\n",
    "                pl.col(\"price\").sum().round(2).alias(\"total_spent\"),\n",
    "                pl.col(\"article_id\").flatten().alias(\"flattened_ids\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(pl.col(\"flattened_ids\").list.len().alias(\"total_items\"))\n",
    "    )\n",
    "\n",
    "    quantile = 0.99\n",
    "    total_purchases_99, total_spending_99, total_items_99 = (\n",
    "        customers_summary.select(\n",
    "            [\n",
    "                pl.col(\"total_purchases\").quantile(quantile),\n",
    "                pl.col(\"total_spent\").quantile(quantile),\n",
    "                pl.col(\"total_items\").quantile(quantile),\n",
    "            ]\n",
    "        )\n",
    "        .collect()\n",
    "        .to_numpy()\n",
    "        .flatten()\n",
    "    )\n",
    "\n",
    "    # Currently only remove customers with very large number of total items purchased\n",
    "    extreme_customers = customers_summary.filter(\n",
    "        (pl.col(\"total_items\") >= total_items_99)\n",
    "        # | (pl.col(\"total_purchases\") >= total_purchases_99)\n",
    "        # | (pl.col(\"total_spent\") >= total_spending_99)\n",
    "    )\n",
    "\n",
    "    extreme_customers = extreme_customers.select(\"customer_id\").unique()\n",
    "    extreme_customers = extreme_customers.collect()\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        Cutoff Values for {quantile*100}th Percentiles:\n",
    "        -----------------------------------\n",
    "        Total items bought:    {total_items_99:.0f} items\n",
    "\n",
    "        -----------------------------------\n",
    "        Removed Customers:     {len(extreme_customers):,}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    return grouped_df.collect(), extreme_customers\n",
    "\n",
    "def train_test_split(\n",
    "    train_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    "    subset: int = None,\n",
    "    train_subsample_percentage: float = None,\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"Splits data into train, validation, and test sets with optional subsampling.\n",
    "\n",
    "    The function performs the following operations:\n",
    "    1. Optional subsampling of both train and test data\n",
    "    2. Optional percentage-based subsampling of training data\n",
    "    3. Creates a validation set from 10% of the training data\n",
    "\n",
    "    Args:\n",
    "        train_df (pl.DataFrame): Training dataset.\n",
    "        test_df (pl.DataFrame): Test dataset.\n",
    "        subset (int, optional): If provided, limits both train and test sets to first n rows. \n",
    "            Defaults to None.\n",
    "        train_subsample_percentage (float, optional): If provided, randomly samples this percentage \n",
    "            of training data. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n",
    "            - train_df: Final training dataset (90% of training data after subsampling)\n",
    "            - val_df: Validation dataset (10% of training data)\n",
    "            - test_df: Test dataset (potentially subsampled)\n",
    "\n",
    "    Notes:\n",
    "        If both subset and train_subsample_percentage are provided, subset is applied first.\n",
    "        The validation set is always 10% of the remaining training data after any subsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    if subset is not None:\n",
    "        train_df = train_df[:subset]\n",
    "        test_df = test_df[:subset]\n",
    "    elif train_subsample_percentage is not None:\n",
    "        sampled_indices = global_rng.choice(\n",
    "            len(train_df),\n",
    "            size=int(train_subsample_percentage * len(train_df)),\n",
    "            replace=False,\n",
    "        )\n",
    "        train_df = train_df[sampled_indices]\n",
    "\n",
    "    # Train-val-split\n",
    "    # Calculate 10% of the length of the array\n",
    "    sampled_indices = global_rng.choice(\n",
    "        len(train_df), size=int(0.1 * len(train_df)), replace=False\n",
    "    )\n",
    "    val_df = train_df[sampled_indices]\n",
    "    train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def map_article_ids(df: pl.DataFrame, data_path: Path) -> pl.DataFrame:\n",
    "    \"\"\"Maps article IDs to new running IDs using a mapping dictionary from JSON.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): DataFrame with 'article_id' column to be mapped.\n",
    "        data_path (Path): Path to directory with 'running_id_dict.json' containing ID mappings.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with mapped article IDs, sorted by new IDs. Non-mapped articles are removed.\n",
    "    \"\"\"\n",
    "    with open(data_path / \"running_id_dict.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    article_id_dict = data[\"combined\"]\n",
    "\n",
    "    mapping_df = pl.DataFrame(\n",
    "        {\n",
    "            \"old_id\": list(article_id_dict.keys()),\n",
    "            \"new_id\": list(article_id_dict.values()),\n",
    "        },\n",
    "        schema_overrides={\"old_id\": pl.Int32, \"new_id\": pl.Int32},\n",
    "    )\n",
    "\n",
    "    # Join and select\n",
    "    df = df.join(\n",
    "        mapping_df, left_on=\"article_id\", right_on=\"old_id\", how=\"inner\"\n",
    "    ).select(\n",
    "        pl.col(\"new_id\").alias(\"article_id\"),\n",
    "        pl.all().exclude([\"article_id\", \"old_id\", \"new_id\"]),\n",
    "    )\n",
    "    df = df.sort(\"article_id\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:04.195137Z",
     "iopub.status.busy": "2025-03-06T11:06:04.194917Z",
     "iopub.status.idle": "2025-03-06T11:06:04.206050Z",
     "shell.execute_reply": "2025-03-06T11:06:04.205396Z",
     "shell.execute_reply.started": "2025-03-06T11:06:04.195118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from pathlib import Path\n",
    "#from data_processing.customer_df.customer_df import get_customer_df_benchmarks\n",
    "#from data_processing.transaction_df.transaction_df import get_tx_article_dfs\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def expand_list_columns(\n",
    "    df: pl.DataFrame, date_col: str = \"days_before_lst\", num_col: str = \"num_items_lst\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand a Polars DataFrame by repeating each element in a list column according to\n",
    "    the counts specified in another list column.\n",
    "\n",
    "    Args:\n",
    "        df: Input Polars DataFrame with list columns\n",
    "        date_col: Name of the column containing the lists to be expanded\n",
    "        num_col: Name of the column containing lists of counts\n",
    "\n",
    "    Returns:\n",
    "        A new Polars DataFrame where the list elements in date_col have been expanded\n",
    "    \"\"\"\n",
    "    expanded = df.with_columns(\n",
    "        pl.struct([date_col, num_col])\n",
    "        .map_elements(\n",
    "            lambda x: [\n",
    "                date\n",
    "                for date, count in zip(x[date_col], x[num_col])\n",
    "                for _ in range(count)\n",
    "            ]\n",
    "        )\n",
    "        .alias(date_col)\n",
    "    )\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "def add_benchmark_tx_features(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Creates benchmark transaction features from aggregated customer transaction data.\n",
    "\n",
    "    Args:\n",
    "        df: A Polars DataFrame containing aggregated transaction data with list columns\n",
    "            including total_price_lst, num_items_lst, days_before_lst, price_lst,\n",
    "            and CLV_label.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A DataFrame with derived features including:\n",
    "            - total_spent: Sum of all transaction amounts\n",
    "            - total_purchases: Count of transactions\n",
    "            - total_items: Sum of items purchased\n",
    "            - days_since_last_purchase: Days since most recent transaction\n",
    "            - days_since_first_purchase: Days since first transaction\n",
    "            - avg_spent_per_transaction: Mean transaction amount\n",
    "            - avg_items_per_transaction: Mean items per transaction\n",
    "            - avg_days_between: Mean days between transactions\n",
    "            - regression_label: CLV label for regression\n",
    "            - classification_label: Binary CLV label (>0)\n",
    "\n",
    "    Note:\n",
    "        The avg_days_between calculation may return None for customers with single\n",
    "        transactions, which is handled by tree-based algorithms.\n",
    "    \"\"\"\n",
    "    return df.select(\n",
    "        \"customer_id\",\n",
    "        pl.col(\"total_price_lst\").list.sum().alias(\"total_spent\"),\n",
    "        pl.col(\"total_price_lst\").list.len().alias(\"total_purchases\"),\n",
    "        pl.col(\"num_items_lst\").list.sum().alias(\"total_items\"),\n",
    "        pl.col(\"days_before_lst\").list.get(-1).alias(\"days_since_last_purchase\"),\n",
    "        pl.col(\"days_before_lst\").list.get(0).alias(\"days_since_first_purchase\"),\n",
    "        pl.col(\"price_lst\").list.mean().alias(\"avg_spent_per_transaction\"),\n",
    "        (\n",
    "            pl.col(\"num_items_lst\")\n",
    "            .list.mean()\n",
    "            .cast(pl.Float32)\n",
    "            .alias(\"avg_items_per_transaction\")\n",
    "        ),\n",
    "        # Code below returns None values for customers with single Tx\n",
    "        # Tree algos should be able to handle this\n",
    "        (\n",
    "            pl.col(\"days_before_lst\")\n",
    "            .list.diff(null_behavior=\"drop\")\n",
    "            .list.mean()\n",
    "            .mul(-1)\n",
    "            .cast(pl.Float32)\n",
    "            .alias(\"avg_days_between\")\n",
    "        ),\n",
    "        pl.col(\"CLV_label\").alias(\"regression_label\"),\n",
    "        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def process_dataframe(df: pl.DataFrame, max_length: int = 20) -> pl.DataFrame:\n",
    "    \"\"\"Processes a polars DataFrame by expanding list columns and selecting specific columns with transformations.\n",
    "\n",
    "    This function performs several operations on the input DataFrame:\n",
    "    1. Expands list columns using the expand_list_columns function\n",
    "    2. Selects and renames specific columns\n",
    "    3. Truncates list columns to a maximum length\n",
    "\n",
    "    Args:\n",
    "        df: A polars DataFrame containing customer transaction data\n",
    "        max_length: Maximum number of elements to keep in list columns (default: 20)\n",
    "\n",
    "    Returns:\n",
    "        A processed polars DataFrame with the following columns:\n",
    "            - customer_id: Customer identifier\n",
    "            - days_before_lst: Truncated list of days before some reference date\n",
    "            - articles_ids_lst: Truncated list of article identifiers\n",
    "            - regression_label: CLV label for regression tasks\n",
    "            - classification_label: Binary classification label derived from CLV\n",
    "    \"\"\"\n",
    "    df = expand_list_columns(df, date_col=\"days_before_lst\", num_col=\"num_items_lst\")\n",
    "    return df.select(\n",
    "        \"customer_id\",\n",
    "        \"days_before_lst\",\n",
    "        \"articles_ids_lst\",\n",
    "        pl.col(\"CLV_label\").alias(\"regression_label\"),\n",
    "        pl.col(\"CLV_label\").gt(0).cast(pl.Int32).alias(\"classification_label\"),\n",
    "    ).with_columns(\n",
    "        pl.col(\"days_before_lst\").list.tail(max_length),\n",
    "        pl.col(\"articles_ids_lst\").list.tail(max_length),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_benchmark_dfs(\n",
    "    data_path: Path, config: dict\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"Creates benchmark train, validation, and test datasets with transaction and customer features.\n",
    "\n",
    "    Args:\n",
    "        data_path: Path object pointing to the data directory\n",
    "        config: Dictionary containing configuration parameters for data processing\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: A tuple containing:\n",
    "            - train_df: Training dataset with benchmark features\n",
    "            - val_df: Validation dataset with benchmark features\n",
    "            - test_df: Test dataset with benchmark features\n",
    "\n",
    "        Each DataFrame contains transaction-derived features joined with customer features.\n",
    "    \"\"\"\n",
    "    train_article, val_article, test_article = get_tx_article_dfs(\n",
    "        data_path=data_path,\n",
    "        config=config,\n",
    "        cols_to_aggregate=[\n",
    "            \"date\",\n",
    "            \"days_before\",\n",
    "            \"article_ids\",\n",
    "            \"sales_channel_ids\",\n",
    "            \"total_price\",\n",
    "            \"prices\",\n",
    "            \"num_items\",\n",
    "        ],\n",
    "        keep_customer_id=True,\n",
    "    )\n",
    "\n",
    "    customer_df = get_customer_df_benchmarks(data_path=data_path, config=config)\n",
    "\n",
    "    train_df = process_dataframe(\n",
    "        df=train_article, max_length=config[\"max_length\"]\n",
    "    ).join(customer_df, on=\"customer_id\", how=\"left\")\n",
    "    val_df = process_dataframe(df=val_article, max_length=config[\"max_length\"]).join(\n",
    "        customer_df, on=\"customer_id\", how=\"left\"\n",
    "    )\n",
    "    test_df = process_dataframe(df=test_article, max_length=config[\"max_length\"]).join(\n",
    "        customer_df, on=\"customer_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:04.206870Z",
     "iopub.status.busy": "2025-03-06T11:06:04.206674Z",
     "iopub.status.idle": "2025-03-06T11:06:04.226923Z",
     "shell.execute_reply": "2025-03-06T11:06:04.226115Z",
     "shell.execute_reply.started": "2025-03-06T11:06:04.206844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "#from pathlib import Path\n",
    "\n",
    "\n",
    "def get_customer_df_benchmarks(data_path: Path, config: dict):\n",
    "    \"\"\"Processes customer data with age grouping and zip code mapping.\n",
    "\n",
    "    Args:\n",
    "        data_path (Path): Path to directory containing 'customers.csv' and 'zip_code_count.csv'.\n",
    "        config (dict): Configuration with 'min_zip_code_count'. Updated with 'num_age_groups' and 'num_zip_codes'.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Processed DataFrame with customer_id, age_group (0-6), and mapped zip_code_id.\n",
    "    \"\"\"\n",
    "    file_path = data_path / \"customers.csv\"\n",
    "    df = pl.scan_csv(file_path).select(\n",
    "        (\n",
    "            \"customer_id\",\n",
    "            pl.col(\"age\").fill_null(strategy=\"mean\"),\n",
    "            \"postal_code\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # df = df.with_columns(\n",
    "    #     [\n",
    "    #         pl.when(pl.col(\"age\").is_null())\n",
    "    #         .then(0)\n",
    "    #         .when(pl.col(\"age\") < 25)\n",
    "    #         .then(1)\n",
    "    #         .when(pl.col(\"age\").is_between(25, 34))\n",
    "    #         .then(2)\n",
    "    #         .when(pl.col(\"age\").is_between(35, 44))\n",
    "    #         .then(3)\n",
    "    #         .when(pl.col(\"age\").is_between(45, 54))\n",
    "    #         .then(4)\n",
    "    #         .when(pl.col(\"age\").is_between(55, 64))\n",
    "    #         .then(5)\n",
    "    #         .otherwise(6)\n",
    "    #         .alias(\"age_group\")\n",
    "    #     ]\n",
    "    # )\n",
    "    # config[\"num_age_groups\"] = 7\n",
    "\n",
    "    return df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:04.228008Z",
     "iopub.status.busy": "2025-03-06T11:06:04.227730Z",
     "iopub.status.idle": "2025-03-06T11:06:04.248754Z",
     "shell.execute_reply": "2025-03-06T11:06:04.248077Z",
     "shell.execute_reply.started": "2025-03-06T11:06:04.227979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from datetime import datetime\n",
    "#from pathlib import Path\n",
    "#import polars as pl\n",
    "\n",
    "#from data_processing.utils.utils_transaction_df import (\n",
    " #   filter_purchases_purchases_per_month_pl,\n",
    "  #  map_article_ids,\n",
    "   # train_test_split,\n",
    "#)\n",
    "\n",
    "\n",
    "def generate_clv_data_pl(\n",
    "    df: pl.DataFrame,\n",
    "    agg_df: pl.DataFrame,\n",
    "    label_threshold: datetime.date,\n",
    "    pred_end: datetime.date,\n",
    "    clv_periods: list,\n",
    "    log_clv: bool = False,\n",
    "):\n",
    "    \"\"\"Generates Customer Lifetime Value (CLV) data from transaction dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input transaction dataframe containing customer purchases.\n",
    "        agg_df (pl.DataFrame): Aggregated dataframe containing customer data.\n",
    "        label_threshold (datetime.date): Start date for CLV calculation period.\n",
    "        pred_end (datetime.date): End date for CLV calculation period.\n",
    "        clv_periods (list): List of periods for CLV calculation (currently supports single period only).\n",
    "        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Aggregated dataframe with added CLV calculations.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If more than one CLV period is provided.\n",
    "    \"\"\"\n",
    "    if len(clv_periods) > 1:\n",
    "        raise ValueError(\"CLV periods should be a single number for now.\")\n",
    "\n",
    "    # Filter transactions between label_threshold and end_date for each period\n",
    "    filtered_df = df.filter(\n",
    "        (pl.col(\"date\") >= label_threshold) & (pl.col(\"date\") <= pred_end)\n",
    "    )\n",
    "\n",
    "    # Sum total_price for the filtered transactions by customer_id. This is the CLV\n",
    "    summed_period_df = filtered_df.group_by(\"customer_id\").agg(\n",
    "        pl.sum(\"total_price\").round(2).alias(f\"CLV_label\")\n",
    "    )\n",
    "    if log_clv:\n",
    "        summed_period_df = summed_period_df.with_columns(\n",
    "            pl.col(f\"CLV_label\").log1p().round(2).alias(f\"CLV_label\")\n",
    "        )\n",
    "\n",
    "    agg_df = agg_df.join(summed_period_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "    agg_df = agg_df.fill_null(0)\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def group_and_convert_df_pl(\n",
    "    df: pl.DataFrame,\n",
    "    label_start_date: datetime.date,\n",
    "    pred_end: datetime.date,\n",
    "    clv_periods: list,\n",
    "    cols_to_aggregate: list = [\n",
    "        \"date\",\n",
    "        \"days_before\",\n",
    "        \"num_items\",\n",
    "        \"article_ids\",\n",
    "        \"sales_channel_ids\",\n",
    "        \"total_price\",\n",
    "        \"prices\",\n",
    "    ],\n",
    "    keep_customer_id: bool = True,\n",
    "    log_clv: bool = False,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Groups and converts transaction data into aggregated customer-level features.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input transaction dataframe.\n",
    "        label_start_date (datetime.date): Start date for clv label period.\n",
    "        pred_end (datetime.date): End date for prediction period.\n",
    "        clv_periods (list): List of periods for CLV calculation.\n",
    "        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n",
    "        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n",
    "        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: Aggregated customer-level dataframe.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns (days_before, article_ids, num_items) are missing from cols_to_aggregate.\n",
    "    \"\"\"\n",
    "\n",
    "    if any(\n",
    "        col not in cols_to_aggregate\n",
    "        for col in [\"days_before\", \"article_ids\", \"num_items\"]\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"The columns days_before, article_ids, and num_items are required \"\n",
    "            \"for the aggregation\"\n",
    "        )\n",
    "\n",
    "    mapping = {\n",
    "        \"date\": \"date_lst\",\n",
    "        \"days_before\": \"days_before_lst\",\n",
    "        \"article_ids\": \"articles_ids_lst\",\n",
    "        \"sales_channel_ids\": \"sales_channel_id_lst\",\n",
    "        \"total_price\": \"total_price_lst\",\n",
    "        \"prices\": \"price_lst\",\n",
    "        \"num_items\": \"num_items_lst\",\n",
    "    }\n",
    "\n",
    "    agg_df = (\n",
    "        df.filter(pl.col(\"date\") < label_start_date)\n",
    "        .with_columns(\n",
    "            (label_start_date - pl.col(\"date\"))\n",
    "            .dt.total_days()\n",
    "            .cast(pl.Int32)\n",
    "            .alias(\"days_before\"),\n",
    "            (\n",
    "                pl.col(\"sales_channel_ids\")\n",
    "                .cast(pl.List(pl.Int32))\n",
    "                .alias(\"sales_channel_ids\")\n",
    "            ),\n",
    "            pl.col(\"article_ids\").cast(pl.List(pl.Int32)).alias(\"article_ids\"),\n",
    "        )\n",
    "        .sort(\"customer_id\", \"date\")\n",
    "        .group_by(\"customer_id\")\n",
    "        .agg(\n",
    "            pl.col(\"date\").explode().alias(\"date_lst\"),\n",
    "            pl.col(\"days_before\").explode().alias(\"days_before_lst\"),\n",
    "            pl.col(\"article_ids\").explode().alias(\"articles_ids_lst\"),\n",
    "            pl.concat_list(pl.col(\"sales_channel_ids\")).alias(\"sales_channel_id_lst\"),\n",
    "            pl.col(\"total_price\").explode().alias(\"total_price_lst\"),\n",
    "            pl.col(\"prices\").explode().alias(\"price_lst\"),\n",
    "            pl.col(\"num_items\").explode().alias(\"num_items_lst\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if clv_periods is not None:\n",
    "        agg_df = generate_clv_data_pl(\n",
    "            df=df,\n",
    "            agg_df=agg_df,\n",
    "            label_threshold=label_start_date,\n",
    "            pred_end=pred_end,\n",
    "            clv_periods=clv_periods,\n",
    "            log_clv=log_clv,\n",
    "        )\n",
    "\n",
    "    # Drop columns which are not to be aggregated\n",
    "    cols_to_drop = [v for k, v in mapping.items() if k not in cols_to_aggregate]\n",
    "    if not keep_customer_id:\n",
    "        cols_to_drop.append(\"customer_id\")\n",
    "    agg_df = agg_df.drop(*cols_to_drop)\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "def split_df_and_group_pl(\n",
    "    df: pl.DataFrame,\n",
    "    clv_periods: list,\n",
    "    config: dict,\n",
    "    cols_to_aggregate: list = [\n",
    "        \"date\",\n",
    "        \"days_before\",\n",
    "        \"article_ids\",\n",
    "        \"sales_channel_ids\",\n",
    "        \"total_price\",\n",
    "        \"prices\",\n",
    "        \"num_items\",\n",
    "    ],\n",
    "    keep_customer_id: bool = True,\n",
    "    log_clv: bool = False,\n",
    ") -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"Splits transaction data into training and test sets and performs aggregation.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input transaction dataframe.\n",
    "        clv_periods (list): List of periods for CLV calculation.\n",
    "        config (dict): Configuration dictionary containing:\n",
    "        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n",
    "        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n",
    "        log_clv (bool, optional): Whether to apply log1p transformation to CLV values. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n",
    "            - train_df: Aggregated training dataset\n",
    "            - test_df: Aggregated test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    train_begin = datetime.strptime(config.get(\"train_begin\"), \"%Y-%m-%d\")\n",
    "    train_label_start = datetime.strptime(config.get(\"train_label_begin\"), \"%Y-%m-%d\")\n",
    "    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n",
    "    test_begin = datetime.strptime(config.get(\"test_begin\"), \"%Y-%m-%d\")\n",
    "    test_label_start = datetime.strptime(config.get(\"test_label_begin\"), \"%Y-%m-%d\")\n",
    "    test_end = datetime.strptime(config.get(\"test_end\"), \"%Y-%m-%d\")\n",
    "\n",
    "    # Creating the training DataFrame by filtering dates up to `train_end`\n",
    "    train_df = df.filter(\n",
    "        (pl.col(\"date\") <= train_end) & (pl.col(\"date\") >= train_begin)\n",
    "    )\n",
    "\n",
    "    train_df = group_and_convert_df_pl(\n",
    "        df=train_df,\n",
    "        label_start_date=train_label_start,\n",
    "        pred_end=train_end,\n",
    "        clv_periods=clv_periods,\n",
    "        cols_to_aggregate=cols_to_aggregate,\n",
    "        keep_customer_id=keep_customer_id,\n",
    "        log_clv=log_clv,\n",
    "    )\n",
    "\n",
    "    # Creating the test DataFrame by filtering dates after `test_begin`\n",
    "    test_df = df.filter((pl.col(\"date\") >= test_begin) & (pl.col(\"date\") <= test_end))\n",
    "\n",
    "    test_df = group_and_convert_df_pl(\n",
    "        df=test_df,\n",
    "        label_start_date=test_label_start,\n",
    "        pred_end=test_end,\n",
    "        clv_periods=clv_periods,\n",
    "        cols_to_aggregate=cols_to_aggregate,\n",
    "        keep_customer_id=keep_customer_id,\n",
    "        log_clv=log_clv,\n",
    "    )\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def load_data_rem_outlier_pl(\n",
    "    data_path: Path, train_end: datetime.date, group_by_channel_id: bool = False\n",
    "):\n",
    "    \"\"\"Loads transaction data, applies price scaling, and removes outliers.\n",
    "\n",
    "    Args:\n",
    "        data_path (Path): Path to directory containing transaction data parquet file.\n",
    "        train_end (datetime.date): End date for training period.\n",
    "        group_by_channel_id (bool, optional): Whether to group data by sales channel ID. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n",
    "            - grouped_df: Processed transaction dataframe\n",
    "            - extreme_customers: Dataframe of customers identified as outliers\n",
    "    \"\"\"\n",
    "    file_path = data_path / \"transactions_polars.parquet\"\n",
    "    df_pl = pl.read_parquet(file_path)\n",
    "\n",
    "    df_pl = df_pl.with_columns(\n",
    "        pl.col(\"t_dat\").alias(\"date\").cast(pl.Date), pl.col(\"article_id\").cast(pl.Int32)\n",
    "    )\n",
    "\n",
    "    df_pl = df_pl.with_columns(\n",
    "        pl.col(\"price\").mul(590).cast(pl.Float32).round(2).alias(\"price\")\n",
    "    )\n",
    "\n",
    "    # Map article ids to running ids so that they match with feature matrix\n",
    "    df_pl = map_article_ids(df=df_pl, data_path=data_path)\n",
    "\n",
    "    grouped_df, extreme_customers = filter_purchases_purchases_per_month_pl(\n",
    "        df_pl, train_end=train_end, group_by_channel_id=group_by_channel_id\n",
    "    )\n",
    "\n",
    "    return grouped_df, extreme_customers\n",
    "\n",
    "\n",
    "def get_customer_train_test_articles_pl(\n",
    "    data_path: Path,\n",
    "    config: dict,\n",
    "    clv_periods: list = None,\n",
    "    cols_to_aggregate: list = [\n",
    "        \"date\",\n",
    "        \"days_before\",\n",
    "        \"article_ids\",\n",
    "        \"sales_channel_ids\",\n",
    "        \"total_price\",\n",
    "        \"prices\",\n",
    "        \"num_items\",\n",
    "    ],\n",
    "    keep_customer_id: bool = True,\n",
    "):\n",
    "    \"\"\"Processes customer transaction data into train and test sets with article information.\n",
    "\n",
    "    Args:\n",
    "        data_path (Path): Path to directory containing transaction data.\n",
    "        config (dict): Configuration dictionary for data processing parameters.\n",
    "        clv_periods (list, optional): List of periods for CLV calculation. Defaults to None.\n",
    "        cols_to_aggregate (list, optional): Columns to include in aggregation. Defaults to standard transaction columns.\n",
    "        keep_customer_id (bool, optional): Whether to retain customer_id in output. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame]: Tuple containing:\n",
    "            - train_df: Processed training dataset with article information\n",
    "            - test_df: Processed test dataset with article information\n",
    "    \"\"\"\n",
    "    train_end = datetime.strptime(config.get(\"train_end\"), \"%Y-%m-%d\")\n",
    "    grouped_df, extreme_customers = load_data_rem_outlier_pl(\n",
    "        data_path=data_path, train_end=train_end\n",
    "    )\n",
    "\n",
    "    train_df, test_df = split_df_and_group_pl(\n",
    "        df=grouped_df,\n",
    "        clv_periods=clv_periods,\n",
    "        config=config,\n",
    "        cols_to_aggregate=cols_to_aggregate,\n",
    "        keep_customer_id=True,\n",
    "        log_clv=config.get(\"log_clv\", False),\n",
    "    )\n",
    "\n",
    "    train_df = train_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n",
    "    test_df = test_df.join(extreme_customers, on=\"customer_id\", how=\"anti\")\n",
    "\n",
    "    if not keep_customer_id:\n",
    "        train_df = train_df.drop(\"customer_id\")\n",
    "        test_df = test_df.drop(\"customer_id\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def get_tx_article_dfs(\n",
    "    data_path: Path,\n",
    "    config: dict,\n",
    "    cols_to_aggregate: list = [\n",
    "        \"date\",\n",
    "        \"days_before\",\n",
    "        \"article_ids\",\n",
    "        \"sales_channel_ids\",\n",
    "        \"total_price\",\n",
    "        \"prices\",\n",
    "        \"num_items\",\n",
    "    ],\n",
    "    keep_customer_id: bool = True,\n",
    "):\n",
    "    \"\"\"Creates train, validation, and test datasets with optional subsampling.\n",
    "\n",
    "    Args:\n",
    "        data_path (Path): Path to directory containing transaction data files.\n",
    "        config (dict): Configuration dictionary containing:\n",
    "        cols_to_aggregate (list, optional): Transaction columns to include in output.\n",
    "        keep_customer_id (bool, optional): Whether to retain customer_id column.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame]: Tuple containing:\n",
    "            - train_df: Final training dataset (subset of original training data)\n",
    "            - val_df: Validation dataset (10% of original training data)\n",
    "            - test_df: Test dataset (optionally subsampled)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Columns of dfs:\n",
    "        - customer_id\n",
    "        - date_lst (list[date]): Dates of each transaction\n",
    "        - days_before_lst (list[int]): Number of days between start of prediction and date of transction\n",
    "        - articles_ids_lst (list[int]): Flattened list of all items a customer purchased \n",
    "        - sales_channel_id_lst (list[list[int]]): Sales channel of a transaction (repeated for each item within a transaction)\n",
    "        - total_price_lst (list[float]): Value of each transaction\n",
    "        - price_lst (list[float]): Flattened list of prices of all items customer purchased\n",
    "        - num_items_lst (list[int]): Number of items in each transaction\n",
    "        - CLV_label (float): Sales in prediction period (label to be used)\n",
    "    \"\"\"\n",
    "    train_df, test_df = get_customer_train_test_articles_pl(\n",
    "        data_path=data_path,\n",
    "        config=config,\n",
    "        clv_periods=config.get(\"clv_periods\", [6]),\n",
    "        cols_to_aggregate=cols_to_aggregate,\n",
    "        keep_customer_id=keep_customer_id,\n",
    "    )\n",
    "    train_df, val_df, test_df = train_test_split(\n",
    "        train_df=train_df,\n",
    "        test_df=test_df,\n",
    "        subset=config.get(\"subset\"),\n",
    "        train_subsample_percentage=config.get(\"train_subsample_percentage\"),\n",
    "    )\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:04.249701Z",
     "iopub.status.busy": "2025-03-06T11:06:04.249435Z",
     "iopub.status.idle": "2025-03-06T11:06:04.268501Z",
     "shell.execute_reply": "2025-03-06T11:06:04.267865Z",
     "shell.execute_reply.started": "2025-03-06T11:06:04.249682Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from pathlib import Path\n",
    "#from data_processing.get_data import get_benchmark_dfs\n",
    "#import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:06:04.269654Z",
     "iopub.status.busy": "2025-03-06T11:06:04.269349Z",
     "iopub.status.idle": "2025-03-06T11:07:48.575002Z",
     "shell.execute_reply": "2025-03-06T11:07:48.574291Z",
     "shell.execute_reply.started": "2025-03-06T11:06:04.269626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########  Loading data  ##########\n",
      "\n",
      "        Cutoff Values for 99.0th Percentiles:\n",
      "        -----------------------------------\n",
      "        Total items bought:    152 items\n",
      "\n",
      "        -----------------------------------\n",
      "        Removed Customers:     11,908\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5_/08c48wz500d22z98_52v8hq80000gr/T/ipykernel_14510/233034741.py:167: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  train_df = train_df.filter(~pl.arange(0, pl.count()).is_in(sampled_indices))\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"train_begin\": \"2018-09-20\",\n",
    "    \"train_label_begin\": \"2019-09-20\",\n",
    "    \"train_end\": \"2020-03-17\",\n",
    "    \"test_begin\": \"2019-03-19\",\n",
    "    \"test_label_begin\": \"2020-03-18\",\n",
    "    \"test_end\": \"2020-09-13\",\n",
    "    \"min_zip_code_count\": 3,\n",
    "    \"date_aggregation\": \"daily\",\n",
    "    \"group_by_channel_id\": False,\n",
    "    \"log_clv\": False,\n",
    "    \"clv_periods\": [6],\n",
    "    \"subset\": None,\n",
    "    \"train_subsample_percentage\": None,\n",
    "    \"max_length\":20, # DEFINE HOW MANY ITEMS ARE TO BE CONSIDERED IN TRANSFORMER SEQUENCE\n",
    "}\n",
    "# data_path = Path(\"/kaggle/input/hm-dataset/data/data\")\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "print(10 * \"#\", \" Loading data \", 10 * \"#\")\n",
    "train_df, val_df, test_df = get_benchmark_dfs(data_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:07:48.576101Z",
     "iopub.status.busy": "2025-03-06T11:07:48.575821Z",
     "iopub.status.idle": "2025-03-06T11:07:48.583688Z",
     "shell.execute_reply": "2025-03-06T11:07:48.582931Z",
     "shell.execute_reply.started": "2025-03-06T11:07:48.576074Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (951_705, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>customer_id</th><th>days_before_lst</th><th>articles_ids_lst</th><th>regression_label</th><th>classification_label</th><th>age</th><th>postal_code</th></tr><tr><td>str</td><td>list[i64]</td><td>list[i32]</td><td>f32</td><td>i32</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>&quot;125eb73ce9758e…</td><td>[297, 297, … 11]</td><td>[16421, 33829, … 100706]</td><td>114.919998</td><td>1</td><td>44</td><td>&quot;2c29ae653a9282…</td></tr><tr><td>&quot;24e68f570456f9…</td><td>[288]</td><td>[56940]</td><td>0.0</td><td>0</td><td>22</td><td>&quot;b65e38fc25a775…</td></tr><tr><td>&quot;439076932c3424…</td><td>[172, 172, … 40]</td><td>[170, 1460, … 80172]</td><td>63.970001</td><td>1</td><td>29</td><td>&quot;66a848a945d5a2…</td></tr><tr><td>&quot;15840793f615a0…</td><td>[347, 347, … 308]</td><td>[1696, 1697, … 61711]</td><td>0.0</td><td>0</td><td>27</td><td>&quot;d39bb9bb60a731…</td></tr><tr><td>&quot;64ca5270549117…</td><td>[335, 335, … 62]</td><td>[26185, 26188, … 67178]</td><td>365.869995</td><td>1</td><td>30</td><td>&quot;2e004e9cc8a99b…</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;84a725fdaf9f24…</td><td>[303, 303, … 114]</td><td>[4917, 17299, … 82554]</td><td>0.0</td><td>0</td><td>53</td><td>&quot;39bf565d0f8485…</td></tr><tr><td>&quot;92e75ab280dca1…</td><td>[132, 132, … 107]</td><td>[49169, 49169, … 80327]</td><td>213.889999</td><td>1</td><td>44</td><td>&quot;c0b2c8425b045c…</td></tr><tr><td>&quot;dde792dba77729…</td><td>[338, 338, … 6]</td><td>[613, 68368, … 92577]</td><td>17.98</td><td>1</td><td>20</td><td>&quot;1c7322990209f2…</td></tr><tr><td>&quot;4c22cb4038b4c8…</td><td>[297, 297, … 22]</td><td>[7859, 45080, … 89674]</td><td>0.0</td><td>0</td><td>24</td><td>&quot;0e5493dc37a814…</td></tr><tr><td>&quot;5673794d1ed335…</td><td>[277, 277, … 83]</td><td>[12611, 56425, … 30555]</td><td>239.889999</td><td>1</td><td>28</td><td>&quot;dd9ccbdd49a88c…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (951_705, 7)\n",
       "┌───────────────┬───────────────┬───────────────┬──────────────┬──────────────┬─────┬──────────────┐\n",
       "│ customer_id   ┆ days_before_l ┆ articles_ids_ ┆ regression_l ┆ classificati ┆ age ┆ postal_code  │\n",
       "│ ---           ┆ st            ┆ lst           ┆ abel         ┆ on_label     ┆ --- ┆ ---          │\n",
       "│ str           ┆ ---           ┆ ---           ┆ ---          ┆ ---          ┆ i64 ┆ str          │\n",
       "│               ┆ list[i64]     ┆ list[i32]     ┆ f32          ┆ i32          ┆     ┆              │\n",
       "╞═══════════════╪═══════════════╪═══════════════╪══════════════╪══════════════╪═════╪══════════════╡\n",
       "│ 125eb73ce9758 ┆ [297, 297, …  ┆ [16421,       ┆ 114.919998   ┆ 1            ┆ 44  ┆ 2c29ae653a92 │\n",
       "│ e76ea75306ef1 ┆ 11]           ┆ 33829, …      ┆              ┆              ┆     ┆ 82cce4151bd8 │\n",
       "│ ee0512…       ┆               ┆ 100706]       ┆              ┆              ┆     ┆ 7643c907…    │\n",
       "│ 24e68f570456f ┆ [288]         ┆ [56940]       ┆ 0.0          ┆ 0            ┆ 22  ┆ b65e38fc25a7 │\n",
       "│ 9cc1f457520f4 ┆               ┆               ┆              ┆              ┆     ┆ 752ced8b9c26 │\n",
       "│ c450d2…       ┆               ┆               ┆              ┆              ┆     ┆ e30717ed…    │\n",
       "│ 439076932c342 ┆ [172, 172, …  ┆ [170, 1460, … ┆ 63.970001    ┆ 1            ┆ 29  ┆ 66a848a945d5 │\n",
       "│ 4c07242334102 ┆ 40]           ┆ 80172]        ┆              ┆              ┆     ┆ a2cc1a5a7326 │\n",
       "│ 0c06d3…       ┆               ┆               ┆              ┆              ┆     ┆ a5784dea…    │\n",
       "│ 15840793f615a ┆ [347, 347, …  ┆ [1696, 1697,  ┆ 0.0          ┆ 0            ┆ 27  ┆ d39bb9bb60a7 │\n",
       "│ 0224ead127e2e ┆ 308]          ┆ … 61711]      ┆              ┆              ┆     ┆ 31b4e481259a │\n",
       "│ 71428e…       ┆               ┆               ┆              ┆              ┆     ┆ b4807534…    │\n",
       "│ 64ca527054911 ┆ [335, 335, …  ┆ [26185,       ┆ 365.869995   ┆ 1            ┆ 30  ┆ 2e004e9cc8a9 │\n",
       "│ 70629589796a8 ┆ 62]           ┆ 26188, …      ┆              ┆              ┆     ┆ 9bb921505a79 │\n",
       "│ 11b4cb…       ┆               ┆ 67178]        ┆              ┆              ┆     ┆ 1f18005d…    │\n",
       "│ …             ┆ …             ┆ …             ┆ …            ┆ …            ┆ …   ┆ …            │\n",
       "│ 84a725fdaf9f2 ┆ [303, 303, …  ┆ [4917, 17299, ┆ 0.0          ┆ 0            ┆ 53  ┆ 39bf565d0f84 │\n",
       "│ 44d1032a52d44 ┆ 114]          ┆ … 82554]      ┆              ┆              ┆     ┆ 8522c758d0f7 │\n",
       "│ 309373…       ┆               ┆               ┆              ┆              ┆     ┆ b6d4da6e…    │\n",
       "│ 92e75ab280dca ┆ [132, 132, …  ┆ [49169,       ┆ 213.889999   ┆ 1            ┆ 44  ┆ c0b2c8425b04 │\n",
       "│ 1c9d86953c89c ┆ 107]          ┆ 49169, …      ┆              ┆              ┆     ┆ 5c874e2d5aaa │\n",
       "│ 34146d…       ┆               ┆ 80327]        ┆              ┆              ┆     ┆ c9a1f4bb…    │\n",
       "│ dde792dba7772 ┆ [338, 338, …  ┆ [613, 68368,  ┆ 17.98        ┆ 1            ┆ 20  ┆ 1c7322990209 │\n",
       "│ 9ec655a5594ae ┆ 6]            ┆ … 92577]      ┆              ┆              ┆     ┆ f2441b530e15 │\n",
       "│ d6d1d0…       ┆               ┆               ┆              ┆              ┆     ┆ bd933a81…    │\n",
       "│ 4c22cb4038b4c ┆ [297, 297, …  ┆ [7859, 45080, ┆ 0.0          ┆ 0            ┆ 24  ┆ 0e5493dc37a8 │\n",
       "│ 80ce210485004 ┆ 22]           ┆ … 89674]      ┆              ┆              ┆     ┆ 14e28518c4e9 │\n",
       "│ 54e944…       ┆               ┆               ┆              ┆              ┆     ┆ c01a9a40…    │\n",
       "│ 5673794d1ed33 ┆ [277, 277, …  ┆ [12611,       ┆ 239.889999   ┆ 1            ┆ 28  ┆ dd9ccbdd49a8 │\n",
       "│ 53632dc70dbf9 ┆ 83]           ┆ 56425, …      ┆              ┆              ┆     ┆ 8c5fa5ca39bc │\n",
       "│ f26c24…       ┆               ┆ 30555]        ┆              ┆              ┆     ┆ eb846797…    │\n",
       "└───────────────┴───────────────┴───────────────┴──────────────┴──────────────┴─────┴──────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:18:40.847784Z",
     "iopub.status.busy": "2025-03-06T11:18:40.847372Z",
     "iopub.status.idle": "2025-03-06T11:18:40.856188Z",
     "shell.execute_reply": "2025-03-06T11:18:40.855279Z",
     "shell.execute_reply.started": "2025-03-06T11:18:40.847755Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_vocabularies(train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    1) Ensures each df is a pandas DataFrame (for easy concatenation and indexing).\n",
    "    2) Builds dictionaries to map string IDs (customer_id, postal_code) to integer indices.\n",
    "    3) Finds max article ID, max day, and max age so we can define embedding sizes.\n",
    "    \"\"\"\n",
    "   \n",
    "    def to_pandas_if_polars(df):\n",
    "        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n",
    "\n",
    "    train_pd = to_pandas_if_polars(train_df)\n",
    "    val_pd   = to_pandas_if_polars(val_df)\n",
    "    test_pd  = to_pandas_if_polars(test_df)\n",
    "\n",
    "    # Combine all rows to build global vocabularies\n",
    "    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n",
    "\n",
    "    # Build user2idx for customer_id (string -> int)\n",
    "    unique_users = combined['customer_id'].unique()\n",
    "    user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "    num_customers = len(user2idx)\n",
    "\n",
    "    # Build postal2idx for postal_code (string -> int)\n",
    "    unique_postals = combined['postal_code'].unique()\n",
    "    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n",
    "    num_postal = len(postal2idx)\n",
    "\n",
    "    # Determine max article ID (assuming articles_ids_lst are numeric)\n",
    "    # We'll need to \"explode\" the list columns to find the maximum article ID\n",
    "    # If Polars list columns are already numeric, these become ints in Pandas\n",
    "    all_articles = []\n",
    "    for df_pd in [train_pd, val_pd, test_pd]:\n",
    "        for lst in df_pd['articles_ids_lst']:\n",
    "            all_articles.extend(lst)  # 'lst' should already be a Python list (or polars List)\n",
    "    max_article_id = max(all_articles)\n",
    "    num_articles = max_article_id + 1  # for embedding dimension\n",
    "\n",
    "    all_days = []\n",
    "    for df_pd in [train_pd, val_pd, test_pd]:\n",
    "        for lst in df_pd['days_before_lst']:\n",
    "            all_days.extend(lst)\n",
    "    max_day = max(all_days)\n",
    "\n",
    "    max_age = combined['age'].max()\n",
    "    num_age = max_age + 1  \n",
    "\n",
    "    return (\n",
    "        user2idx,\n",
    "        postal2idx,\n",
    "        num_customers,\n",
    "        num_postal,\n",
    "        num_articles,\n",
    "        max_day,\n",
    "        num_age\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:18:46.036623Z",
     "iopub.status.busy": "2025-03-06T11:18:46.036255Z",
     "iopub.status.idle": "2025-03-06T11:18:46.043206Z",
     "shell.execute_reply": "2025-03-06T11:18:46.042162Z",
     "shell.execute_reply.started": "2025-03-06T11:18:46.036597Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects columns:\n",
    "    customer_id (str), days_before_lst (list[i64]),\n",
    "    articles_ids_lst (list[i32]), regression_label (f32),\n",
    "    classification_label (i32), age (i64), postal_code (str).\n",
    "    \"\"\"\n",
    "    def __init__(self, df, user2idx, postal2idx):\n",
    "        \"\"\"\n",
    "        df can be either a Pandas or Polars DataFrame.\n",
    "        user2idx and postal2idx map string IDs to integer indices.\n",
    "        \"\"\"\n",
    "        if not hasattr(df, \"iloc\"):\n",
    "            df = df.to_pandas()\n",
    "        self.data = df\n",
    "\n",
    "        self.user2idx = user2idx\n",
    "        self.postal2idx = postal2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert string-based IDs to integer indices\n",
    "        user_id = self.user2idx[row['customer_id']]\n",
    "        postal_id = self.postal2idx[row['postal_code']]\n",
    "\n",
    "        # 'age' is i64; we can embed it or treat it as numeric. \n",
    "        # Here, we'll embed it, so we keep it as an integer index\n",
    "        age = int(row['age'])\n",
    "\n",
    "        articles = row['articles_ids_lst']\n",
    "        days = row['days_before_lst']\n",
    "\n",
    "        articles = torch.tensor(articles, dtype=torch.long)\n",
    "        days = torch.tensor(days, dtype=torch.long)\n",
    "\n",
    "        # Regression label is float\n",
    "        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n",
    "\n",
    "        return (\n",
    "            user_id,      # int\n",
    "            articles,     # tensor of shape (sequence_length,)\n",
    "            days,         # tensor of shape (sequence_length,)\n",
    "            age,          # int\n",
    "            postal_id,    # int\n",
    "            regression_label\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:18:52.508886Z",
     "iopub.status.busy": "2025-03-06T11:18:52.508579Z",
     "iopub.status.idle": "2025-03-06T11:18:52.524990Z",
     "shell.execute_reply": "2025-03-06T11:18:52.524069Z",
     "shell.execute_reply.started": "2025-03-06T11:18:52.508863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple positional embedding that learns a unique embedding per position\n",
    "    (0 to max_len-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, d_model)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        positions = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        return self.pe(positions)  # shape: (batch_size, seq_length, d_model)\n",
    "\n",
    "class BST(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_customers,\n",
    "        num_articles,\n",
    "        max_day,\n",
    "        num_age,\n",
    "        num_postal,\n",
    "        sequence_length,\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        user2idx,\n",
    "        postal2idx,\n",
    "        article_emb_dim=16,\n",
    "        day_emb_dim=8,\n",
    "        customer_emb_dim=16,\n",
    "        age_emb_dim=4,\n",
    "        postal_emb_dim=4,\n",
    "        transformer_nhead=2,\n",
    "        learning_rate=0.0005,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['train_df','val_df','test_df','user2idx','postal2idx'])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Keep data and dictionaries for setup()\n",
    "        self.train_df = train_df\n",
    "        self.val_df   = val_df\n",
    "        self.test_df  = test_df\n",
    "        self.user2idx = user2idx\n",
    "        self.postal2idx = postal2idx\n",
    "\n",
    "        # Embeddings\n",
    "        self.embeddings_customer = nn.Embedding(num_customers, customer_emb_dim)\n",
    "        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n",
    "        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n",
    "\n",
    "        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n",
    "        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n",
    "\n",
    "        self.seq_feature_dim = article_emb_dim + day_emb_dim\n",
    "        self.positional_embedding = PositionalEmbedding(sequence_length, self.seq_feature_dim)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.seq_feature_dim, \n",
    "            nhead=transformer_nhead, \n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # Flattened sequence dimension after transformer\n",
    "        transformer_output_dim = sequence_length * self.seq_feature_dim\n",
    "\n",
    "        # User features dimension\n",
    "        user_feature_dim = customer_emb_dim + age_emb_dim + postal_emb_dim\n",
    "\n",
    "        # Combine them\n",
    "        combined_dim = transformer_output_dim + user_feature_dim\n",
    "\n",
    "        print(\"Combined input dim: \",combined_dim)\n",
    "\n",
    "        # Fully-connected layers for regression\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def encode_input(self, batch):\n",
    "        user_id, articles, days, age, postal_id, regression_label = batch\n",
    "\n",
    "        # Embedding for the article and day sequences\n",
    "        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n",
    "        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n",
    "        sequence_features = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        pos_embeds = self.positional_embedding(sequence_features)  # (B, L, seq_feature_dim)\n",
    "        transformer_input = sequence_features + pos_embeds\n",
    "\n",
    "        # Transformer expects shape (L, B, d_model)\n",
    "        transformer_input = transformer_input.transpose(0, 1)  # (L, B, seq_feature_dim)\n",
    "        transformer_output = self.transformer_layer(transformer_input)\n",
    "        transformer_output = transformer_output.transpose(0, 1)  # (B, L, seq_feature_dim)\n",
    "\n",
    "        # Flatten the sequence dimension\n",
    "        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n",
    "\n",
    "        # Embeddings for user-level features\n",
    "        customer_embed = self.embeddings_customer(user_id)\n",
    "        age_embed = self.embeddings_age(age)\n",
    "        postal_embed = self.embeddings_postal(postal_id)\n",
    "        user_features = torch.cat([customer_embed, age_embed, postal_embed], dim=1)\n",
    "\n",
    "        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n",
    "        return combined_features, regression_label\n",
    "\n",
    "    def forward(self, batch):\n",
    "        features, target = self.encode_input(batch)\n",
    "        output = self.linear(features)\n",
    "        return output.squeeze(), target\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output, target = self(batch)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output, target = self(batch)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output, target = self(batch)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"\n",
    "        Create Dataset objects from the dataframes.\n",
    "        We pass user2idx and postal2idx so that each\n",
    "        string ID is mapped to an integer index.\n",
    "        \"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = CustomerDataset(self.train_df, self.user2idx, self.postal2idx)\n",
    "            self.val_dataset   = CustomerDataset(self.val_df,   self.user2idx, self.postal2idx)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset  = CustomerDataset(self.test_df,  self.user2idx, self.postal2idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=128, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:18:56.124025Z",
     "iopub.status.busy": "2025-03-06T11:18:56.123717Z",
     "iopub.status.idle": "2025-03-06T11:19:10.855971Z",
     "shell.execute_reply": "2025-03-06T11:19:10.854327Z",
     "shell.execute_reply.started": "2025-03-06T11:18:56.124002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce46422d59f44dc287c59541285820c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [2] at entry 0 and [7] at entry 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-44f432aa3e42>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m             \u001b[0mval_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on_sanity_check_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mprevious_dataloader_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0;31m# the dataloader has changed, notify the logger connector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# the iterator is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_ITERATOR_RETURN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Sequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# try the next iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [2] at entry 0 and [7] at entry 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(\n",
    "    user2idx,\n",
    "    postal2idx,\n",
    "    num_customers,\n",
    "    num_postal,\n",
    "    num_articles,\n",
    "    max_day,\n",
    "    num_age\n",
    ") = prepare_vocabularies(train_df, val_df, test_df)\n",
    "\n",
    "sequence_length = 8  \n",
    "\n",
    "model = BST(\n",
    "    num_customers=num_customers,\n",
    "    num_articles=num_articles,\n",
    "    max_day=max_day,\n",
    "    num_age=num_age,\n",
    "    num_postal=num_postal,\n",
    "    sequence_length=sequence_length,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    user2idx=user2idx,\n",
    "    postal2idx=postal2idx,\n",
    "    article_emb_dim=16,\n",
    "    day_emb_dim=8,\n",
    "    customer_emb_dim=16,\n",
    "    age_emb_dim=4,\n",
    "    postal_emb_dim=4,\n",
    "    transformer_nhead=2,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=5)\n",
    "trainer.fit(model)\n",
    "trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T11:26:19.388270Z",
     "iopub.status.busy": "2025-03-06T11:26:19.387938Z",
     "iopub.status.idle": "2025-03-06T11:30:06.472367Z",
     "shell.execute_reply": "2025-03-06T11:30:06.471537Z",
     "shell.execute_reply.started": "2025-03-06T11:26:19.388245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/dschoess/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name                 | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | embeddings_customer  | Embedding               | 18.7 M | train\n",
      "1 | embeddings_age       | Embedding               | 400    | train\n",
      "2 | embeddings_postal    | Embedding               | 1.3 M  | train\n",
      "3 | embeddings_article   | Embedding               | 1.6 M  | train\n",
      "4 | embeddings_day       | Embedding               | 2.9 K  | train\n",
      "5 | positional_embedding | PositionalEmbedding     | 192    | train\n",
      "6 | transformer_layer    | TransformerEncoderLayer | 102 K  | train\n",
      "7 | linear               | Sequential              | 242 K  | train\n",
      "8 | criterion            | MSELoss                 | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "22.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.0 M    Total params\n",
      "88.125    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dschoess/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'BST.val_dataloader.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 380\u001b[0m\n\u001b[1;32m    357\u001b[0m model \u001b[38;5;241m=\u001b[39m BST(\n\u001b[1;32m    358\u001b[0m   num_customers\u001b[38;5;241m=\u001b[39mnum_customers,\n\u001b[1;32m    359\u001b[0m     num_articles\u001b[38;5;241m=\u001b[39mnum_articles,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m\n\u001b[1;32m    376\u001b[0m )\n\u001b[1;32m    379\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 380\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:122\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_run_start()\n\u001b[1;32m    124\u001b[0m data_fetcher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:258\u001b[0m, in \u001b[0;36m_EvaluationLoop.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m combined_loader\u001b[38;5;241m.\u001b[39mlimits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_batches\n\u001b[1;32m    257\u001b[0m data_fetcher\u001b[38;5;241m.\u001b[39msetup(combined_loader)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# creates the iterator inside the fetcher\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# add the previous `fetched` value to properly track `is_last_batch` with no prefetching\u001b[39;00m\n\u001b[1;32m    261\u001b[0m data_fetcher\u001b[38;5;241m.\u001b[39mfetched \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mready\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:105\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_PrefetchDataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__iter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;66;03m# ignore pre-fetching, it's not necessary\u001b[39;00m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:52\u001b[0m, in \u001b[0;36m_DataFetcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_DataFetcher\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:351\u001b[0m, in \u001b[0;36mCombinedLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m _SUPPORTED_MODES[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterator\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    350\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m iterator\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:155\u001b[0m, in \u001b[0;36m_Sequential.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_current_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:173\u001b[0m, in \u001b[0;36m_Sequential._load_current_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_current_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# Load a single DataLoader, prevents multiple sets of workers from starting unnecessarily\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterables):\n\u001b[0;32m--> 173\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;66;03m# No more iterables to step through, return an empty list\u001b[39;00m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Applications/miniconda3/envs/bst/lib/python3.11/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'BST.val_dataloader.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def prepare_vocabularies(train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    1) Ensures each df is a pandas DataFrame (for easy indexing).\n",
    "    2) Builds dictionaries to map string IDs (customer_id, postal_code) to integer indices.\n",
    "    3) Finds max article ID, max day, and max age so we can define embedding sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_pandas_if_polars(df):\n",
    "        return df.to_pandas() if not hasattr(df, \"iloc\") else df\n",
    "\n",
    "    train_pd = to_pandas_if_polars(train_df)\n",
    "    val_pd   = to_pandas_if_polars(val_df)\n",
    "    test_pd  = to_pandas_if_polars(test_df)\n",
    "\n",
    "    # Combine for global vocabularies\n",
    "    combined = pd.concat([train_pd, val_pd, test_pd], ignore_index=True)\n",
    "\n",
    "    # Map string-based customer_id -> integer\n",
    "    unique_users = combined['customer_id'].unique()\n",
    "    user2idx = {u: i for i, u in enumerate(unique_users)}\n",
    "    num_customers = len(user2idx)\n",
    "\n",
    "    # Map string-based postal_code -> integer\n",
    "    unique_postals = combined['postal_code'].unique()\n",
    "    postal2idx = {p: i for i, p in enumerate(unique_postals)}\n",
    "    num_postal = len(postal2idx)\n",
    "\n",
    "    # Determine max article ID\n",
    "    all_articles = []\n",
    "    for df_pd in [train_pd, val_pd, test_pd]:\n",
    "        for lst in df_pd['articles_ids_lst']:\n",
    "            all_articles.extend(lst)  # 'lst' is a list of ints\n",
    "    max_article_id = max(all_articles)\n",
    "    num_articles = max_article_id + 1  # for embedding\n",
    "\n",
    "    # Determine max day\n",
    "    all_days = []\n",
    "    for df_pd in [train_pd, val_pd, test_pd]:\n",
    "        for lst in df_pd['days_before_lst']:\n",
    "            all_days.extend(lst)\n",
    "    max_day = max(all_days)\n",
    "\n",
    "    # Determine max age if we treat age as discrete\n",
    "    max_age = combined['age'].max()\n",
    "    num_age = max_age + 1\n",
    "\n",
    "    return user2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age\n",
    "\n",
    "class CustomerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects columns:\n",
    "    - customer_id (str)\n",
    "    - days_before_lst (list[int])\n",
    "    - articles_ids_lst (list[int])\n",
    "    - regression_label (float)\n",
    "    - classification_label (int)  (not used here)\n",
    "    - age (int)\n",
    "    - postal_code (str)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, user2idx: Dict[str,int], postal2idx: Dict[str,int]):\n",
    "        # Convert to Pandas if Polars\n",
    "        if not hasattr(df, \"iloc\"):\n",
    "            df = df.to_pandas()\n",
    "        self.data = df\n",
    "\n",
    "        self.user2idx = user2idx\n",
    "        self.postal2idx = postal2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Convert string-based IDs to integer indices\n",
    "        user_id = self.user2idx[row['customer_id']]\n",
    "        postal_id = self.postal2idx[row['postal_code']]\n",
    "\n",
    "        age = int(row['age'])  # embedding or numeric\n",
    "\n",
    "        # articles_ids_lst and days_before_lst are lists of ints\n",
    "        articles = torch.tensor(row['articles_ids_lst'], dtype=torch.long)\n",
    "        days = torch.tensor(row['days_before_lst'], dtype=torch.long)\n",
    "\n",
    "        regression_label = torch.tensor(float(row['regression_label']), dtype=torch.float)\n",
    "\n",
    "        return (\n",
    "            user_id,\n",
    "            articles,\n",
    "            days,\n",
    "            age,\n",
    "            postal_id,\n",
    "            regression_label\n",
    "        )\n",
    "\n",
    "\n",
    "# CUSTOM COLLATE FUNCTION FOR VARIABLE-LENGTH SEQUENCES\n",
    "\n",
    "\n",
    "def fixed_length_collate_fn(batch, sequence_length=8):\n",
    "    \"\"\"\n",
    "    Pads or truncates the 'articles' and 'days' sequences to 'sequence_length'.\n",
    "    Each item in the batch is a tuple:\n",
    "      (user_id, articles, days, age, postal_id, regression_label)\n",
    "    \"\"\"\n",
    "    user_ids      = []\n",
    "    article_seqs  = []\n",
    "    day_seqs      = []\n",
    "    ages          = []\n",
    "    postal_ids    = []\n",
    "    labels        = []\n",
    "\n",
    "    # 1) Unpack\n",
    "    for item in batch:\n",
    "        (user_id, articles, days, age, postal_id, label) = item\n",
    "        user_ids.append(user_id)\n",
    "        article_seqs.append(articles)\n",
    "        day_seqs.append(days)\n",
    "        ages.append(age)\n",
    "        postal_ids.append(postal_id)\n",
    "        labels.append(label)\n",
    "\n",
    "    # 2) Pad or truncate each sequence\n",
    "    def pad_or_trunc(seq, desired_length):\n",
    "        length = seq.size(0)\n",
    "        if length > desired_length:\n",
    "            return seq[:desired_length]\n",
    "        elif length < desired_length:\n",
    "            pad_size = desired_length - length\n",
    "            pad = torch.zeros(pad_size, dtype=seq.dtype)\n",
    "            return torch.cat([seq, pad], dim=0)\n",
    "        else:\n",
    "            return seq\n",
    "\n",
    "    for i in range(len(article_seqs)):\n",
    "        article_seqs[i] = pad_or_trunc(article_seqs[i], sequence_length)\n",
    "        day_seqs[i] = pad_or_trunc(day_seqs[i], sequence_length)\n",
    "\n",
    "    # 3) Stack everything\n",
    "    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "    article_seqs_tensor = torch.stack(article_seqs, dim=0)  # shape: (batch_size, sequence_length)\n",
    "    day_seqs_tensor = torch.stack(day_seqs, dim=0)         # shape: (batch_size, sequence_length)\n",
    "    ages_tensor = torch.tensor(ages, dtype=torch.long)\n",
    "    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.stack(labels, dim=0)  # shape: (batch_size,)\n",
    "\n",
    "    return (\n",
    "        user_ids_tensor,\n",
    "        article_seqs_tensor,\n",
    "        day_seqs_tensor,\n",
    "        ages_tensor,\n",
    "        postal_ids_tensor,\n",
    "        labels_tensor\n",
    "    )\n",
    "\n",
    "\n",
    "# BST MODEL\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple positional embedding that learns a unique embedding per position (0..max_len-1).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length, d_model)\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        positions = torch.arange(seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        return self.pe(positions)  # (batch_size, seq_length, d_model)\n",
    "\n",
    "class BST(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_customers,\n",
    "        num_articles,\n",
    "        max_day,\n",
    "        num_age,\n",
    "        num_postal,\n",
    "        sequence_length,\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        user2idx,\n",
    "        postal2idx,\n",
    "        article_emb_dim=16,\n",
    "        day_emb_dim=8,\n",
    "        customer_emb_dim=16,\n",
    "        age_emb_dim=4,\n",
    "        postal_emb_dim=4,\n",
    "        transformer_nhead=2,\n",
    "        learning_rate=0.0005\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['train_df','val_df','test_df','user2idx','postal2idx'])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # DataFrames + Mappings\n",
    "        self.train_df = train_df\n",
    "        self.val_df   = val_df\n",
    "        self.test_df  = test_df\n",
    "        self.user2idx = user2idx\n",
    "        self.postal2idx = postal2idx\n",
    "\n",
    "        # Embeddings\n",
    "        self.embeddings_customer = nn.Embedding(num_customers, customer_emb_dim)\n",
    "        self.embeddings_age = nn.Embedding(num_age, age_emb_dim)\n",
    "        self.embeddings_postal = nn.Embedding(num_postal, postal_emb_dim)\n",
    "\n",
    "        self.embeddings_article = nn.Embedding(num_articles, article_emb_dim)\n",
    "        self.embeddings_day = nn.Embedding(max_day + 1, day_emb_dim)\n",
    "\n",
    "        # Sequence dimension\n",
    "        self.seq_feature_dim = article_emb_dim + day_emb_dim\n",
    "        self.positional_embedding = PositionalEmbedding(sequence_length, self.seq_feature_dim)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.seq_feature_dim,\n",
    "            nhead=transformer_nhead,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # Flattened dimension after transformer\n",
    "        transformer_output_dim = sequence_length * self.seq_feature_dim\n",
    "\n",
    "        # User features dimension\n",
    "        user_feature_dim = customer_emb_dim + age_emb_dim + postal_emb_dim\n",
    "\n",
    "        # Combined dimension\n",
    "        combined_dim = transformer_output_dim + user_feature_dim\n",
    "\n",
    "        # Final regressor\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def encode_input(self, batch):\n",
    "        user_id, articles, days, age, postal_id, regression_label = batch\n",
    "\n",
    "        # Sequence embeddings\n",
    "        article_embeds = self.embeddings_article(articles)  # (B, L, article_emb_dim)\n",
    "        day_embeds = self.embeddings_day(days)              # (B, L, day_emb_dim)\n",
    "        sequence_features = torch.cat([article_embeds, day_embeds], dim=-1)  # (B, L, seq_feature_dim)\n",
    "\n",
    "        # Positional embeddings\n",
    "        pos_embeds = self.positional_embedding(sequence_features)\n",
    "        transformer_input = sequence_features + pos_embeds\n",
    "\n",
    "        # Transformer expects (L, B, d_model)\n",
    "        transformer_input = transformer_input.transpose(0, 1)  # (L, B, seq_feature_dim)\n",
    "        transformer_output = self.transformer_layer(transformer_input)\n",
    "        transformer_output = transformer_output.transpose(0, 1)  # (B, L, seq_feature_dim)\n",
    "\n",
    "        # Flatten\n",
    "        transformer_output_flat = transformer_output.reshape(transformer_output.size(0), -1)\n",
    "\n",
    "        # User features\n",
    "        customer_embed = self.embeddings_customer(user_id)\n",
    "        age_embed = self.embeddings_age(age)\n",
    "        postal_embed = self.embeddings_postal(postal_id)\n",
    "        user_features = torch.cat([customer_embed, age_embed, postal_embed], dim=1)\n",
    "\n",
    "        # Combine\n",
    "        combined_features = torch.cat([transformer_output_flat, user_features], dim=1)\n",
    "        return combined_features, regression_label\n",
    "\n",
    "    def forward(self, batch):\n",
    "        features, target = self.encode_input(batch)\n",
    "        output = self.linear(features)\n",
    "        return output.squeeze(), target\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output, target = self(batch)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output, target = self(batch)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output, target = self(batch)\n",
    "        loss = self.criterion(output, target)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = CustomerDataset(self.train_df, self.user2idx, self.postal2idx)\n",
    "            self.val_dataset   = CustomerDataset(self.val_df,   self.user2idx, self.postal2idx)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset  = CustomerDataset(self.test_df,  self.user2idx, self.postal2idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            collate_fn=lambda b: fixed_length_collate_fn(b, sequence_length=self.hparams.sequence_length)\n",
    "        )\n",
    "\n",
    "\n",
    "# TRAIN AND TEST\n",
    "\n",
    "\n",
    "user2idx, postal2idx, num_customers, num_postal, num_articles, max_day, num_age = prepare_vocabularies(\n",
    "     train_df, val_df, test_df\n",
    " )\n",
    "\n",
    "\n",
    "sequence_length = 8\n",
    "\n",
    "\n",
    "model = BST(\n",
    "  num_customers=num_customers,\n",
    "    num_articles=num_articles,\n",
    "    max_day=max_day,\n",
    "    num_age=num_age,\n",
    "    num_postal=num_postal,\n",
    "    sequence_length=sequence_length,\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    user2idx=user2idx,\n",
    "    postal2idx=postal2idx,\n",
    "    article_emb_dim=16,\n",
    "    day_emb_dim=8,\n",
    "    customer_emb_dim=16,\n",
    "    age_emb_dim=4,\n",
    "   postal_emb_dim=4,\n",
    "    transformer_nhead=2,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\", devices=\"auto\", max_epochs=1)\n",
    "trainer.fit(model)\n",
    "trainer.test(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# New padding function\n",
    "def fixed_length_collate_fn(batch: list[tuple[int, torch.Tensor, torch.Tensor, int, int, torch.Tensor]], \n",
    "                           sequence_length: int = 8, padding_value:int = 0) -> tuple[torch.Tensor, ...]:\n",
    "    \"\"\"\n",
    "    Efficiently pads sequences using PyTorch's pad_sequence and then truncates once.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of tuples where each tuple contains \n",
    "              (user_id, articles, days, age, postal_id, regression_label)\n",
    "        sequence_length: Desired length of the sequences\n",
    "        padding_value: Index to be used for padding\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of tensors: (user_ids, article_seqs, day_seqs, ages, postal_ids, labels)\n",
    "    \"\"\"\n",
    "    # Unpack all items at once using zip\n",
    "    user_ids, article_seqs, day_seqs, ages, postal_ids, labels = zip(*batch)\n",
    "    \n",
    "    # Use pad_sequence for efficient padding\n",
    "    article_seqs_tensor = pad_sequence(article_seqs, batch_first=True, padding_value=padding_value)\n",
    "    day_seqs_tensor = pad_sequence(day_seqs, batch_first=True, padding_value=padding_value)\n",
    "    \n",
    "    # Truncate padded tensors to sequence_length\n",
    "    article_seqs_tensor = article_seqs_tensor[:, :sequence_length]\n",
    "    day_seqs_tensor = day_seqs_tensor[:, :sequence_length]\n",
    "    \n",
    "    # Convert other elements to tensors\n",
    "    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "    ages_tensor = torch.tensor(ages, dtype=torch.long)\n",
    "    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.stack(labels, dim=0)\n",
    "    \n",
    "    return (\n",
    "        user_ids_tensor,\n",
    "        article_seqs_tensor,\n",
    "        day_seqs_tensor,\n",
    "        ages_tensor,\n",
    "        postal_ids_tensor,\n",
    "        labels_tensor\n",
    "    )\n",
    "\n",
    "def old_fixed_length_collate_fn(batch, sequence_length=8):\n",
    "    \"\"\"\n",
    "    Pads or truncates the 'articles' and 'days' sequences to 'sequence_length'.\n",
    "    Each item in the batch is a tuple:\n",
    "      (user_id, articles, days, age, postal_id, regression_label)\n",
    "    \"\"\"\n",
    "    user_ids      = []\n",
    "    article_seqs  = []\n",
    "    day_seqs      = []\n",
    "    ages          = []\n",
    "    postal_ids    = []\n",
    "    labels        = []\n",
    "\n",
    "    # 1) Unpack\n",
    "    for item in batch:\n",
    "        (user_id, articles, days, age, postal_id, label) = item\n",
    "        user_ids.append(user_id)\n",
    "        article_seqs.append(articles)\n",
    "        day_seqs.append(days)\n",
    "        ages.append(age)\n",
    "        postal_ids.append(postal_id)\n",
    "        labels.append(label)\n",
    "\n",
    "    # 2) Pad or truncate each sequence\n",
    "    def pad_or_trunc(seq, desired_length):\n",
    "        length = seq.size(0)\n",
    "        if length > desired_length:\n",
    "            return seq[:desired_length]\n",
    "        elif length < desired_length:\n",
    "            pad_size = desired_length - length\n",
    "            pad = torch.zeros(pad_size, dtype=seq.dtype)\n",
    "            return torch.cat([seq, pad], dim=0)\n",
    "        else:\n",
    "            return seq\n",
    "\n",
    "    for i in range(len(article_seqs)):\n",
    "        article_seqs[i] = pad_or_trunc(article_seqs[i], sequence_length)\n",
    "        day_seqs[i] = pad_or_trunc(day_seqs[i], sequence_length)\n",
    "\n",
    "    # 3) Stack everything\n",
    "    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "    article_seqs_tensor = torch.stack(article_seqs, dim=0)  # shape: (batch_size, sequence_length)\n",
    "    day_seqs_tensor = torch.stack(day_seqs, dim=0)         # shape: (batch_size, sequence_length)\n",
    "    ages_tensor = torch.tensor(ages, dtype=torch.long)\n",
    "    postal_ids_tensor = torch.tensor(postal_ids, dtype=torch.long)\n",
    "    labels_tensor = torch.stack(labels, dim=0)  # shape: (batch_size,)\n",
    "\n",
    "    return (\n",
    "        user_ids_tensor,\n",
    "        article_seqs_tensor,\n",
    "        day_seqs_tensor,\n",
    "        ages_tensor,\n",
    "        postal_ids_tensor,\n",
    "        labels_tensor\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6142016,
     "sourceId": 10523408,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "bst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
